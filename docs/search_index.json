[["rls.html", "Capítulo 7 Regresión lineal simple 7.1 El modelo de regresión lineal simple (RLS) 7.2 Estimación de los parámetros por minimos cuadrados de \\(\\beta_0, \\beta_1\\) 7.3 Estimación de \\(\\sigma^2\\) 7.4 Coeficiente de determinación 7.5 Propiedades de estimadores de mínimos cuadrados 7.6 Estimación de invervalos de confianza y prueba de hipótesis para \\(\\beta_1, \\beta_0\\) y \\(\\sigma^2\\) 7.7 Transformaciones para mejorar el ajuste del modelo 7.8 Análisis de residuales 7.9 Estimación e inferencia para la \\(\\widehat{\\mathbb{E}}(y\\mid x_0)\\) en RLS 7.10 Intervalo de predicción para una nueva \\(y_0\\) en \\(x_0\\)", " Capítulo 7 Regresión lineal simple En muchos procesos productivos se controlan las condiciones de operación de un sistema. Es fundamental determinar cómo una variable respuesta cambia dependiendo de dichas condiciones. Por ejemplo, en un proceso de embutido de cárnicos, la cantidad de producto empacado (en gramos) puede depender de la velocidad y la temperatura de operación de una máquina. En la practica, por supuesto es importante Entender la relación entre la cantidad de producto empacado, la velocidad y la temperatura de operación. Garantizar el cumplimiento de las especificaciones en la cantidad de producto empacado. Optmizar las condiciones de operación para reducir costos y minimizar desperdicios. El Modelo de Regresión Lineal se aplica ampliamente en ingeniería cuando se busca dar respuesta a los tres puntos anteriores. En general, los modelos de regresión lineal permiten estudiar la relación entre: Una variable respuesta \\(Y\\). Un conjunto de variables controlables del proceso, denotadas como \\(X_1, X_2, \\dots, X_k\\). Una vez entendida esta relación, es posible predecir valores futuros de la variable respuesta a partir de valores específicos y conocidos de las variables controlables. Nota: variable controlable Variable controlable: Son aquellas variables de proceso que pueden medirse fácilmente y mantenerse fijas durante la operación. Dependiendo del número de variables controlables, distinguimos: Modelo de Regresión Lineal Simple (RLS): Cuando se tiene una sola variable controlable, digamos \\(x\\). Modelo de Regresión Lineal Múltiple (RLM): Cuando existen más de una variable controlable, digamos \\(x_1, x_2, \\dots, x_k\\). Ejemplo en procesos de embutidos: \\[ x_1 = \\text{velocidad}, \\quad x_2 = \\text{temperatura} \\] 7.1 El modelo de regresión lineal simple (RLS) Consideremos un proceso donde se tienen registros de la Resistencia, en psi, y la Edad, en semanas, de varias soldaduras. Los datos pueden leerse en R como se muestra a continuación: Code ## Lectura de datos desde archivo local file &lt;- &quot;data/data_rls.txt&quot; df &lt;- read.table(file, header = TRUE) ## Primeras 6 filas head(df) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Con miras explorar si existe una relación entre la Resistencia y la Edad de la soldadura, existen dos estrategias. observación En RLS, es posible establecer si existe una posible relación entre \\(X\\) y \\(Y\\) es Construir un gráfico de dispersión Calculando el coeficiente de correlación lineal muestral Para construir un gráfico de dispersión procedemos de la siguiente manera: Code library(tidyverse) df %&gt;% ggplot(aes(x = Edad, y = Resistencia)) + geom_point() + theme_bw() Observe que, aparentemente, cuanto mayor sea la Edad de la soldadura menor será su Resistencia. Por lo tanto, podemos decir que la relación entre Edad y Resistencia es inversamente proporcional. Ahora, miremos la correlación, suponiendo que los datos tienen comportamiento Code ## coeficiente de correlación with(df, cor(Edad, Resistencia)) ## [1] -0.8827718 Por lo tanto, concluimos que la relación entre la Edad de la soldadura y su Resistencia es inversamente proporcional. 7.1.1 Modelo determinista y modelo probabilístico Para el modelo determinista: \\[ y_i = \\beta_0 + \\beta_1 x_i, \\] el valor observado de la Resistencia \\(Y_i\\) es una función lineal de la Edad \\(x_i\\). La generalización apropiada de esto a un modelo probabilístico supone que el valor esperado de la Resistencia \\(Y_i\\) es una función lineal de la Edad \\(x_i\\). Si denotamos por \\(E(Y_i \\,|\\, X = x_i)\\) a la esperanza de la variable aleatoria \\(Y_i\\), entonces bajo el supuesto de linealidad podemos escribir: \\[ E(Y_i \\,|\\, X = x_i) \\;=\\; \\beta_0 + \\beta_1 x_i. \\tag{7.1} \\] En la práctica, el valor observado de \\(Y_i\\) (Resistencia medida) se desviará inevitablemente de su valor esperado debido a la variabilidad y errores de medición. Si esta diferencia se representa mediante la variable aleatoria \\(\\varepsilon_i\\), con media cero por (7.1), se puede establecer: \\[ \\varepsilon_i \\;=\\; Y_i - E(Y_i \\,|\\, X = x_i) \\;=\\; Y_i - (\\beta_0 + \\beta_1 x_i), \\] o de manera equivalente: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i. \\tag{7.2} \\] La ecuación (7.2) es la llamada recta verdadera (o poblacional) de regresión, que describe la relación entre la Resistencia (psi) y la Edad (semanas) de las soldaduras, considerando tanto la tendencia promedio como la variabilidad aleatoria. Definición Supongamos que estamos interesados en conocer la relación entre la variable \\(Y\\) y una independente \\(X\\). Si la variable \\(X\\) toma los valores \\(x_i\\), entonces la ecuación de la recta poblacional de regresión expresa los correspondientes valores de \\(Y_i\\) como: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\] donde \\(\\beta_0\\) y \\(\\beta_1\\) son constantes, y \\(\\varepsilon_i\\), llamado término de error, es una variable aleatoria con media cero. 7.1.2 Supuestos básicos para el modelo de regresión lineal. Los supuestos para el modelo de regresión lineal son Supuestos para el modelo de regresión lineal simple Denotemos la recta verdadera de regresión por \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) y asumamos que se dispone de \\(n\\) pares de observaciones. Suelen realizarse, al respecto, los siguientes supuestos: Cada \\(x_i\\) es un número fijo (asignado, por ejemplo, por un investigador) o es la realización de una variable aleatoria \\(X_i\\) independiente del término de error \\(\\varepsilon_i\\). En el último caso, la inferencia se realiza condicionando al valor observado \\(x_i\\). Los términos de error \\(\\varepsilon_i\\) son variables aleatorias con media 0, es decir, \\(E(\\varepsilon_i) = 0\\), para todo \\(i = 1, \\ldots, n\\). Las variables aleatorias \\(\\varepsilon_i\\) tienen todas la misma varianza \\(\\sigma^2\\), es decir, \\(V(\\varepsilon_i) = \\sigma^2\\), para todo \\(i = 1, \\ldots, n\\). Las variables aleatorias \\(\\varepsilon_i\\) no se hallan correlacionadas, es decir, son independientes, luego \\(E(\\varepsilon_i \\varepsilon_j) = 0\\), para todo \\(i, j = 1, \\ldots, n\\), con \\(i \\neq j\\). Los errores \\(\\varepsilon_i\\) estan distribuidos normalmente. Teorema Denotando la recta verdadera de regresión por \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\), si se cumplen los supuestos del modelo de regresión, entonces, para un valor fijo \\(x_i\\) de \\(X\\), la esperanza y varianza condicionales de \\(Y_i\\), dado que \\(X = x_i\\), vienen dadas, respectivamente, por: \\(E(Y_i \\,|\\, X = x_i) = \\beta_0 + \\beta_1 x_i \\quad \\text{y} \\quad V(Y_i \\,|\\, X = x_i) = \\sigma^2.\\) Demostración del teorema Esperanza condicional. Usando linealidad de la esperanza y que \\(\\beta_0\\) y \\(\\beta_1\\, x_i\\) son constantes dado \\(X=x_i\\): \\[ E(Y_i \\mid X=x_i) = E(\\beta_0 + \\beta_1 x_i + \\varepsilon_i \\mid X=x_i) = \\beta_0 + \\beta_1 x_i + E(\\varepsilon_i \\mid X=x_i). \\] Por independencia (o, en su defecto, por el supuesto \\(E(\\varepsilon_i\\mid X)=0\\)): \\[ E(\\varepsilon_i \\mid X=x_i)=0. \\] Luego, \\[ \\boxed{\\,E(Y_i \\mid X=x_i)=\\beta_0+\\beta_1 x_i\\,}. \\] Varianza condicional. La varianza de una constante es cero y \\(\\beta_0+\\beta_1 x_i\\) es constante dado \\(X=x_i\\): \\[\\begin{align} \\operatorname{Var}(Y_i \\mid X=x_i) &amp;= \\operatorname{Var}(\\beta_0 + \\beta_1 x_i + \\varepsilon_i \\mid X=x_i) \\\\[6pt] &amp;= \\operatorname{Var}(\\beta_0 + \\beta_1 x_i \\mid X=x_i) + \\operatorname{Var}(\\varepsilon_i \\mid X=x_i) \\\\[6pt] &amp;= 0 + \\operatorname{Var}(\\varepsilon_i \\mid X=x_i). \\end{align}\\] Por independencia (o por homocedasticidad condicional): \\[ \\operatorname{Var}(\\varepsilon_i \\mid X=x_i) = \\operatorname{Var}(\\varepsilon_i)=\\sigma^2. \\] Por tanto, \\[ \\boxed{\\,\\operatorname{Var}(Y_i \\mid X=x_i)=\\sigma^2\\,}. \\] En general, el modelo de RLS surge ante la necesidad de predecir una variable respuesta \\(Y\\), generalmente continua, como función de una variable controlable \\(X\\), donde matematicamente el modelo puede expresarse como: \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\] \\[ \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2), \\] \\[ \\sigma^2 = \\text{constante}. \\] donde \\((\\beta_0, \\beta_1, \\sigma^2)\\) corresponden a los parámetros del modelo y \\(\\varepsilon_i\\) es el error aleatorio para la observación \\(i\\). Los términos \\((\\beta_0, \\beta_1)\\) se conocen como los coeficientes del modelo. \\(\\sigma^2\\) corresponde a la varianza de los errores. En la práctica, se tiene o se toma una muestra aleatoria de tamaño \\(n\\) proveniente de una población o de un proceso de producción/servicios. A partir de dicha muestra, se busca estimar los valores de \\((\\beta_0, \\beta_1, \\sigma^2)\\). 7.2 Estimación de los parámetros por minimos cuadrados de \\(\\beta_0, \\beta_1\\) Para estimar \\(\\beta_0\\) y \\(\\beta_1\\) se usa el método de mínimos cuadrados. Esto es, se estiman \\(\\beta_0\\) y \\(\\beta_1\\) tales que la suma de los cuadrados de las diferencias entre las observaciones \\(y_i\\) y la línea recta sea mínima. El modelo es: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i=1,2,\\ldots,n \\] El criterio de mínimos cuadrados es: \\[ S(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\varepsilon_i^2=\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Para obtener los estimadores, derivamos respecto a \\(\\beta_0\\) y \\(\\beta_1\\): \\[\\begin{align} \\frac{\\partial S}{\\partial \\beta_0}\\Big|_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;= -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[6pt] \\frac{\\partial S}{\\partial \\beta_1}\\Big|_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;= -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)x_i = 0 \\end{align}\\] Estas se simplifican a las llamadas ecuaciones normales: \\[\\begin{align} n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^n x_i &amp;= \\sum_{i=1}^n y_i \\\\[6pt] \\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 &amp;= \\sum_{i=1}^n y_i x_i \\end{align}\\] La solución de este sistema es: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\] \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n y_i x_i - \\frac{1}{n}\\left(\\sum_{i=1}^n y_i\\right)\\left(\\sum_{i=1}^n x_i\\right)} {\\sum_{i=1}^n x_i^2 - \\frac{1}{n}\\left(\\sum_{i=1}^n x_i\\right)^2} \\] donde: \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\qquad \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i \\] El modelo ajustado es: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] De manera más sencilla, definimos: \\[ S_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2 \\qquad \\text{y} \\qquad S_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] Entonces, una forma conveniente de escribir la pendiente es: \\[ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} \\] Ejercicio Queda de ejercicio verificar que en dicho punto crítico se alcanza un mínimo local. Sugerencia, realizar la segundas derivadas parciales. Ejercicio Demuestre que \\[\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) = \\sum_{i=1}^n x_i y_i - n \\, \\overline{x} \\, \\overline{y}\\] Demuestre que \\[\\sum_{i=1}^n (x_i - \\overline{x})^2 = \\sum_{i=1}^n x_i^2 - n \\overline{x}^2\\] Observacion En la práctica, \\(\\hat{\\beta}_1\\) se interpreta como el cambio unitario en el valor esperado de \\(Y\\), es decir, en \\(E[Y\\mid X]\\), por cada cambio unitario del factor controlable \\(X\\). Continuando con el ejemplo de los datos de soldadura, estimemos paso a paso \\(\\beta_0, \\beta_1\\): Code # 1. Lectura de datos df &lt;- read.table(file, header = TRUE) # 2. Variables Y &lt;- df$Resistencia X &lt;- df$Edad n &lt;- length(X) # 3. Medias x_bar &lt;- mean(X) y_bar &lt;- mean(Y) # 4. Cálculo de Sxx y Sxy Sxx &lt;- sum((X - x_bar)^2) Sxy &lt;- sum((X - x_bar) * (Y - y_bar)) # 5. Estimadores de mínimos cuadrados beta1 &lt;- Sxy / Sxx beta1 ## [1] -0.2437636 Code beta0 &lt;- y_bar - beta1 * x_bar beta0 ## [1] 15.74275 En R la función clave paara ajustar modelos de RLS es lm. Para mayor información, puede consultar la ayuda de la función escribiendo ?lm en la consola, o en la página oficial. En general, la sintaxis es Code # ajuste del modelo de RLS modelo &lt;- lm(Resistencia ~ Edad, data = df) modelo ## ## Call: ## lm(formula = Resistencia ~ Edad, data = df) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Los resultados indican que \\(\\hat{\\beta}_0 = 15.74\\) y \\(\\hat{\\beta}_1 = -0.244\\). Por lo tanto, el modelo ajustado puede escribirse como: \\[ \\widehat{\\text{Resistencia}}_i = 15.74 \\;-\\; 0.244 \\,\\text{Edad}_i, \\] \\[ \\varepsilon_i \\sim N(0, \\sigma^2), \\qquad \\sigma^2 = \\text{constante}. \\] Interpretación de \\(\\,\\hat{\\beta}_0\\,\\) y \\(\\,\\hat{\\beta}_1\\) Se espera que, por cada semana que transcurre, la resistencia de la soldadura disminuya 0.244 psi. Es decir, \\(\\hat{\\beta}_1 = -0.244\\). Una soldadura nueva \\((\\text{Edad} = 0)\\) tiene una resistencia promedio de 15.74 psi. Es decir, \\(\\hat{\\beta}_0 = 15.74\\). Ejercicio 1 Las cantidades de un compuesto químico “y”, que se disuelven en 100 gramos de agua a varias temperaturas “x”, se registran como sigue: x (°C) y (gramos) 0 8 0 6 0 8 15 12 15 10 15 14 30 25 30 21 30 24 45 31 45 33 45 28 60 44 60 39 60 42 75 48 75 51 75 44 Grafique un diagrama de dispersión de los datos. ¿Parecerá plausible un modelo de regresión lineal simple? Encuentre las estimaciones de mínimos cuadrados de la pendiente y la ordenada al origen del modelo de regresión lineal simple. Estime la cantidad de compuesto químico que se disolverá en 100 gramos de agua a 50°C. 7.3 Estimación de \\(\\sigma^2\\) Además de estimar \\(\\beta_0\\) y \\(\\beta_1\\), se requiere un estimado de \\(\\sigma^2\\) para probar hipótesis y formar intervalos de confianza pertinentes al modelo de regresión. Ahora, se llama residuos (errores) a las desviaciones verticales \\[\\varepsilon_i = y_i - \\hat{y}_i, \\quad i = 1, 2, \\ldots, n\\] de la recta estimada. Cuando no se puede usar información previa, el estimado de \\(\\sigma^2\\) se obtiene de la suma de cuadrados de los errores (SSE): \\[ SSE = S = \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\tag{7.3} \\] Si sustituimos \\(\\hat{y}_i = \\hat{\\beta_0}+\\hat{\\beta_1}x_i\\) en la ecuación (7.3) tenemos que \\[\\begin{align} SSE &amp;= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left( y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i \\right)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left[ (y_i - \\bar{y}) - \\hat{\\beta}_1 (x_i - \\bar{x}) \\right]^2 \\\\[6pt] &amp;= \\sum_{i=1}^n (y_i - \\bar{y})^2 - 2\\hat{\\beta}_1 \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) + \\hat{\\beta}_1^2 \\sum_{i=1}^n (x_i - \\bar{x})^2 \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1^2 S_{xx} \\end{align}\\] Como \\(\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}}\\) se sigue que \\[\\begin{align} SSE &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1^2 S_{xx} \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1 \\left(\\frac{S_{xy}}{S_{xx}}\\right) S_{xx} \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1 S_{xy} \\\\[6pt] &amp;= SST - \\hat{\\beta}_1 S_{xy}, \\end{align}\\] donde \\[\\begin{align} SST &amp;= \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2, \\\\[6pt] SSE &amp;= \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2. \\end{align}\\] La suma de cuadrados de residuales tiene \\(n - 2\\) grados de libertad, porque dos grados de libertad se asocian con los estimados \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que se usan para obtener \\(\\hat{y}_i\\). Por el teorema de MSA y MSE insesgado de \\(\\sigma^2\\), sabemos que el valor esperado de \\(SSE\\) es: \\[ E(SSE) = (n-2)\\sigma^2, \\] por lo que un estimador insesgado de \\(\\sigma^2\\) es: \\[ \\hat{\\sigma}^2 = \\frac{SSE}{n-2} = MSE \\] La cantidad \\(MSE\\) se llama cuadrado medio residual. La raíz cuadrada de \\(\\hat{\\sigma}^2\\) se llama, a veces, el error estándar de la regresión (RSE), y tiene las mismas unidades que la variable respuesta \\(y\\). Ya que \\(\\hat{\\sigma}^2\\) depende de la suma de cuadrados de residuales, cualquier violación de las hipótesis sobre los errores del modelo, o cualquier especificación equivocada de la forma del modelo, pueden dañar la utilidad de \\(\\hat{\\sigma}^2\\) como estimador de \\(\\sigma^2\\). Como \\(\\sigma^2\\) se calcula con los residuales del modelo de regresión, se dice que es un estimador de \\(\\sigma^2\\) dependiente del modelo. Finalmente, el modelo de Regresión Lineal Simple (RLS) estimado es \\[\\begin{align} \\hat{Y}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i, \\\\[6pt] \\varepsilon_i &amp;\\sim N(0, \\hat{\\sigma}^2), \\\\[6pt] \\hat{\\sigma}^2 &amp;= \\text{constante}. \\end{align}\\] Otra forma de expresarlo es \\[\\begin{align} \\hat{Y}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\[6pt] \\hat{\\mu}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i. \\end{align}\\] A partir del modelo ajustado es posible predecir el valor esperado de \\(Y\\) para valores conocidos de \\(X\\), es decir, si fijamos un valor \\(x_0\\) del factor \\(X\\), podemos calcular: \\[ E[Y \\mid X = x_0]. \\] Continuando con el ejemplo de soldaduras. Estimemos el \\(\\sigma\\). Haciendolo paso a paso Code # 1. Cargar datos datos &lt;- read.table(&quot;data/data_rls.txt&quot;, header = TRUE) x &lt;- datos$Edad y &lt;- datos$Resistencia # 2. Ajustar el modelo de regresión lineal simple modelo &lt;- lm(Resistencia ~ Edad, data = datos) # 3. Los coeficientes son b0 &lt;- modelo$coefficients[1] b1 &lt;- modelo$coefficients[2] # 4. Ajustados y residuos yhat &lt;- b0 + b1 * x e &lt;- y - yhat # 5. SSE por definición SSE &lt;- sum(e^2) # Otra forma de btener los residuos residuos &lt;- resid(modelo) SSE_otro &lt;- sum(residuos^2) # 6. Calcular MSE n &lt;- nrow(datos) # número de observaciones gl &lt;- n - 2 # grados de libertad MSE &lt;- SSE / gl MSE ## [1] 0.3491253 Code # 7. Calcular RSE RSE &lt;- sqrt(MSE) RSE ## [1] 0.5908683 Cuando ajustamos el modelo con la función modelo &lt;- lm(Resistencia ~ Edad, data = datos) para obtener el RSE de la siguiente forma Code ## estimación de sigma summary(modelo)$sigma ## [1] 0.5908683 A partir de este resultado podemos concluir que \\(\\hat{\\sigma}=0.591\\). Por lo tanto, \\(MSE=0.591^2=0.349\\). Finalmente, el modelo ajustado puede expresarse como \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2) \\\\[6pt] \\hat{\\mu}_i &amp;= 15.743 - 0.244 \\, \\text{Edad}_i \\\\[6pt] \\hat{\\sigma}^2 &amp;= 0.591^2 = 0.349 \\end{align}\\] 7.4 Coeficiente de determinación Definición: coeficiente de determinación El Coeficiente de Determinación (\\(R^2\\)) es una medida estadística que indica la proporción de la variabilidad total de la variable dependiente \\(Y\\) que es explicada por el modelo de regresión. Se define como: \\[ R^2 = 1 - \\dfrac{SSE}{SST} \\] Su valor está comprendido entre 0 y 1: \\(R^2 \\approx 1\\): el modelo explica gran parte de la variabilidad de los datos. \\(R^2 \\approx 0\\): el modelo explica muy poca variabilidad. El coeficiente de determinación puede escribirse de un modo diferente al introducir una tercera suma de cuadrados, la suma de cuadrados de regresión. \\[\\begin{align} SSR &amp;= \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 = \\sum (\\hat{y}_i - \\bar{y})^2 = \\sum \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i - \\bar{y} \\right)^2 \\\\[6pt] &amp;= \\sum \\left( -\\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i \\right)^2 = \\hat{\\beta}_1^2 \\sum (x_i - \\bar{x})^2 = \\hat{\\beta}_1 S_{xy} = SST - SSE \\end{align}\\] Así, \\[\\begin{align} R^2 = 1 - \\frac{SSE}{SST} = \\frac{SST - SSE}{SST} = \\frac{SSR}{SST} = \\frac{\\hat{\\beta}_1 S_{xy}}{S_{yy}} \\end{align}\\] Continuando con el ejemplo de los datos de soldadura, calculemos paso a paso el coeficiente de determinación Code # 1. Lectura de datos file &lt;- &quot;data/data_rls.txt&quot; df &lt;- read.table(file, header = TRUE) # 2. Variables Y &lt;- df$Resistencia X &lt;- df$Edad n &lt;- length(X) # 3. Medias x_bar &lt;- mean(X) y_bar &lt;- mean(Y) # 4. Cálculo de Sxx y Sxy Sxx &lt;- sum((X - x_bar)^2) Sxy &lt;- sum((X - x_bar) * (Y - y_bar)) # 5. Estimadores de mínimos cuadrados beta1 &lt;- Sxy / Sxx # 6. Estimador beta0 beta0 &lt;- y_bar - beta1 * x_bar # 7. Valores ajustados Y_hat &lt;- beta0 + beta1 * X # 8. Sumas de cuadrados SST &lt;- sum((Y - y_bar)^2) SSE &lt;- sum((Y - Y_hat)^2) SSR &lt;- SST - SSE # también se puede calcular como sum((Y_hat - y_bar)^2) # 9. Coeficiente de determinación R2 &lt;- SSR / SST R2 ## [1] 0.7792861 Otra forma de calcular el coefiente de determinación es Code ## coeficiente de determinación rho_est &lt;- with(df, cor(Resistencia, Edad)) rho_est^2 ## [1] 0.7792861 Ahora, usando la función lm Code # ajuste del modelo de RLS modelo &lt;- lm(Resistencia ~ Edad, data = df) # obtención del R^2 a partir de un objeto lm summary(modelo)$r.squared ## [1] 0.7792861 Ejercicio 1 Todos los meses, durante cierto período, se midieron la temperatura promedio en °C (x) y el número de libras de vapor (y) consumidas por cierta planta química. La recta de mínimos cuadrados calculada de los datos resultantes es: \\(\\hat{y} = 245.82 + 1.13x\\) Pronostique el número de libras del vapor consumido en un mes donde la temperatura promedio es 65 °C. Si dos meses difieren en sus temperaturas promedio por 5 °C, ¿cuánto predice que será diferente el número de libras del vapor consumido? Ejercicio 2 En un estudio de la relación entre la dureza de Brinell (x) y la tensión de compresión en ksi (y) de elementos de cobre extruidos en frío, la recta de mínimos cuadrados fue: \\(\\hat{y} = -196.32 + 2.42x\\) Pronostique la fuerza de tensión de un elemento cuya dureza de Brinell es 102.7. Si dos muestras difieren en su dureza de Brinell en 3, ¿cuánto predice que serán diferentes sus tensiones de compresión? Ejercicio 3 Una recta de mínimos cuadrados está ajustando a un conjunto de puntos. Si la suma total de los cuadrados es \\(\\sum (y_i - \\bar{y})^2 = 9615\\), y la suma de los cuadrados de los errores es \\(\\sum (y_i - \\hat{y}_i)^2 = 1450\\), calcule el coeficiente de determinación \\(R^2\\). Ejercicio 4 Una recta de mínimos cuadrados está ajustando un conjunto de puntos. Si la suma total de cuadrados es \\(\\sum (y_i - \\bar{y})^2 = 181.2\\), y la suma de los cuadrados de los errores es \\(\\sum (y_i - \\hat{y}_i)^2 = 33.9\\), calcule el coeficiente de determinación \\(R^2\\). Ejercicio 5 Con los datos de las estaturas de Galton, la recta de mínimos cuadrados para pronosticar la longitud del antebrazo (y) de la estatura (x) es: \\(\\hat{y} = -0.2967 + 0.2738x\\) Pronostique la longitud del antebrazo de un hombre cuya estatura es 70 pulgadas. ¿Qué estatura debe tener un hombre con el propósito de que se pronostique que su longitud de antebrazo sea de 19 pulgadas? Todos los hombres en cierto grupo tienen 72 pulgadas de estatura. Explique por qué la pendiente de la recta de mínimos cuadrados es menos de 1 si ambos, las estaturas y los antebrazos se miden en las mismas unidades (pulgadas). Ejercicio 6 Los adelantos tecnológicos han hecho posible fabricar botes inflables. Estos botes de goma inflables, que pueden enrollarse formando un paquete no mayor que una bolsa de golf, tienen tamaño suficiente para dos pasajeros con su equipo de excursionismo. La revista Canoe &amp; Kayac probó los botes de nueve fabricantes para ver su funcionamiento en un recorrido de tres días. Uno de los criterios de evaluación fue su capacidad para equipaje que se evaluó utilizando una escala de 4 puntos, siendo 1 la puntuación más baja y 4 la puntuación más alta. Los datos siguientes muestran la evaluación que obtuvieron respecto a capacidad para equipaje y los precios de los botes (Canoe Kayak, marzo 2003). Bote Capacidad para equipaje Precio ($) S14 4 1595 Orinoco 4 1399 Outside Pro 4 1890 Explorer 380X 3 795 River XK2 2.5 600 Sea Tiger 4 1995 Maverik II 3 1205 Starlite 100 2 583 Fat Pack Cat 3 1048 Trace el diagrama de dispersión de estos datos empleando la capacidad para equipaje como variable independiente. ¿Qué indica el diagrama de dispersión del inciso a) respecto a la relación entre capacidad para equipaje y precio? A través de los puntos de los datos trace una línea recta para aproximar la relación lineal entre capacidad para equipaje y precio. Utilice el método de mínimos cuadrados para obtener la ecuación de regresión estimada. Dé una interpretación de la pendiente de la ecuación de regresión estimada. Diga cuál será el precio de un bote que tenga 3 en la evaluación de su capacidad para equipaje. Ejercicio 7 Wageweb realiza estudios sobre datos salariales y presenta resúmenes de éstos en su sitio de la Red. Basándose en datos salariales desde el 1 de octubre de 2002, Wageweb publicó que el salario anual promedio de los vicepresidentes de ventas era $142 111 con una gratificación anual promedio de $15 432 (Wageweb.com, 13 de marzo de 2003). Suponga que los datos siguientes sean una muestra de salarios y bonos anuales de 10 vicepresidentes de ventas. Los datos se dan en miles de dólares. Vicepresidente Salario Gratificación 1 135 12 2 115 14 3 147 16 4 167 22 5 165 24 6 176 29 7 98 7 8 136 17 9 163 21 10 119 11 Trace un diagrama de dispersión con estos datos tomando como variable independiente los salarios. ¿Qué indica el diagrama de dispersión del inciso a) acerca de la relación entre salario y gratificación? Use el método de mínimos cuadrados para obtener la ecuación de regresión estimada. Dé una interpretación de la ecuación de regresión estimada. ¿Cuál será la gratificación de un vicepresidente que tenga un salario anual de $120 000? Ejercicio 8 Se midió el peso inercial (en toneladas) y el ahorro de combustible (en milla/galón) para una muestra de siete camiones de diésel. En la tabla siguiente se presentan los resultados. (De “In—Use Emissions from Heavy—Duty Diesel Vehicles,” J. Yanowitz, tesis de doctorado, Escuela de Minas de Colorado, 2001.) Peso Millaje 8.00 7.69 24.50 4.97 27.00 4.56 14.50 6.49 28.50 4.34 12.75 6.24 21.25 4.45 Construya un diagrama de puntos del millaje (y) contra el peso (x). Compruebe que un modelo lineal es adecuado. Calcule la recta de mínimos cuadrados para pronosticar el millaje a partir del peso. Si los dos camiones son diferentes en peso por cinco toneladas, ¿cuánto predeciría que son diferentes sus millajes? Pronostique el millaje para camiones con un peso de 15 toneladas. ¿Cuáles son las unidades de la pendiente estimada \\(\\hat{\\beta}_1\\)? ¿Cuáles son las unidades del intercepto estimado \\(\\hat{\\beta}_0\\)? Ejercicio 9 El procesamiento de carbón natural implica el “lavado” durante el cual se elimina ceniza de carbón (no orgánico, material no combustible). El artículo “Quantifying Sampling Precision for Coal Ash Using Gy’s Discrete Model of the Fundamental Error” (Journal of Coal Quality, 1989, 33-39) proporciona los datos relacionados con los porcentajes de ceniza con la densidad de una partícula de carbón. Se midió el promedio de porcentaje de ceniza para cinco densidades de partículas de carbón. En la tabla siguiente se presentan los datos: Densidad (g/cm³) Porcentaje de ceniza 1.25 1.93 1.325 4.63 1.375 8.95 1.45 15.05 1.55 23.31 Construya un diagrama de dispersión del porcentaje de ceniza (y) contra la densidad (x). Verifique que es adecuado un modelo lineal. Calcule la recta de mínimos cuadrados para pronosticar porcentaje de ceniza a partir de la densidad. Si las dos partículas de carbón difieren en densidad por 0.1 g/cm³, ¿cuánto predeciría que será diferente el porcentaje de ceniza? Pronostique el porcentaje de ceniza para partículas con 1.40 g/cm³. Calcule los valores ajustados. Calcule los residuos. ¿Qué punto tiene el residuo con magnitud mayor? Calcule la correlación entre la densidad y el porcentaje de ceniza. Haga la suma de cuadrados de regresión, la suma de cuadrados del error y la suma total de cuadrados. Divida la suma de cuadrados de regresión entre la suma total de cuadrados. ¿Cuál es la relación entre esta cantidad y el coeficiente de correlación? Ejercicio 10 Un ingeniero quiere pronosticar el valor de y cuando x = 4.5 utilizando el siguiente conjunto de datos: x y z = ln y x y z = ln y 1 0.2 -1.61 6 2.3 0.83 2 0.3 -1.20 7 2.9 1.06 3 0.5 -0.69 8 4.5 1.50 4 0.5 -0.69 9 8.7 2.16 5 1.3 0.26 10 12.0 2.48 Construya un diagrama de dispersión de los puntos (x, y). ¿La recta de mínimos cuadrados se debe utilizar para pronosticar el valor de y cuando x = 4.5? Si es así, calcule la recta de mínimos cuadrados y el valor pronosticado. Si no, explique. Construya un diagrama de dispersión de los puntos (x, z), donde z = ln y. Utilice la recta de mínimos cuadrados para pronosticar el valor de z cuando x = 4.5. ¿Éste es un método adecuado de pronóstico? Explique por qué sí o no. Sea ẑ el valor pronosticado de z calculado en el inciso d). Sea ŷ = eẑ. Explique por qué ŷ es un pronosticador razonable del valor de y cuando x = 4.5. 7.5 Propiedades de estimadores de mínimos cuadrados Teorema Sean \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) los estimadores de mínimos cuadrados de los parámetros en un modelo de regresión lineal simple \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\\\[6pt] \\varepsilon_i &amp;\\sim N(0, \\sigma^2). \\end{align}\\] Entonces Insesgado: \\[ E(\\hat{\\beta}_1) = \\beta_1, \\quad E(\\hat{\\beta}_0) = \\beta_0 \\] Varianzas: \\[ \\hat{\\sigma}^2_{\\hat{\\beta}_1}=Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}}, \\quad \\hat{\\sigma}^2_{\\hat{\\beta}_0}= Var(\\hat{\\beta}_0) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\] donde \\(S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\). Distribuciones: \\[ \\hat{\\beta}_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{S_{xx}}\\right), \\quad \\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right) \\] Demostración del teorema Inicialmente probemos que \\(E(\\hat{\\beta}_1) = \\beta_1\\). En efecto \\[ \\hat{\\beta}_1=\\frac{S_{xy}}{S_{xx}}, \\qquad S_{xy}=\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y), \\qquad S_{xx}=\\sum_{i=1}^n (x_i-\\bar x)^2&gt;0. \\] Ahora, \\[\\begin{align} \\sum_{i=1}^n (x_i - \\bar{x}) &amp;= \\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar{x} = \\sum_{i=1}^n x_i - n\\bar{x} \\\\[6pt] &amp;= \\sum_{i=1}^n x_i - n\\left(\\tfrac{1}{n}\\sum_{i=1}^n x_i\\right)= \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0. \\end{align}\\] Como \\(\\sum_{i=1}^n (x_i-\\bar x)=0\\) se tiene que \\[\\begin{align} S_{xy} &amp;= \\sum_{i=1}^n (x_i-\\bar x)y_i - \\bar y \\sum_{i=1}^n (x_i-\\bar x) \\\\ &amp;= \\sum_{i=1}^n (x_i-\\bar x)\\, y_i, \\end{align}\\] de modo que \\[ \\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i-\\bar x)\\, y_i}{S_{xx}}. \\] Sustituyendo el modelo \\(y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i\\) tenemos que \\[\\begin{align} \\sum_{i=1}^n (x_i-\\bar x) y_i &amp;= \\sum_{i=1}^n (x_i-\\bar x)(\\beta_0+\\beta_1 x_i+\\varepsilon_i) \\\\ &amp;= \\beta_0 \\sum_{i=1}^n (x_i-\\bar x) + \\beta_1 \\sum_{i=1}^n (x_i-\\bar x)x_i + \\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i \\\\ &amp;= 0 + \\beta_1 \\sum_{i=1}^n (x_i-\\bar x)x_i + \\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i\\\\ &amp;= \\beta_1 \\sum_{i=1}^n (x_i-\\bar x)x_i + \\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i. \\end{align}\\] Por otro lado, \\[\\begin{align} \\sum_{i=1}^n (x_i - \\bar{x})x_i &amp;= \\sum_{i=1}^n \\big[(x_i - \\bar{x})(x_i - \\bar{x} + \\bar{x})\\big] \\\\[6pt] &amp;= \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\bar{x}\\sum_{i=1}^n (x_i - \\bar{x}) \\\\[6pt] &amp;= \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\bar{x}\\cdot 0 \\\\[6pt] &amp;= \\sum_{i=1}^n (x_i - \\bar{x})^2 \\\\[6pt] &amp;= S_{xx}. \\end{align}\\] Usando la identidad \\(\\sum_{i=1}^n (x_i-\\bar x)x_i = S_{xx}\\) se sigue que \\[ \\sum_{i=1}^n (x_i-\\bar x) y_i = \\beta_1 S_{xx} + \\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i. \\] Por tanto \\[ \\hat{\\beta}_1 = \\frac{\\beta_1 S_{xx} + \\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i}{S_{xx}} = \\beta_1 + \\frac{\\sum_{i=1}^n (x_i-\\bar x)\\varepsilon_i}{S_{xx}}. \\] Como \\(\\hat{\\beta}_1\\) se construye con los \\(x_i\\) y los \\(\\varepsilon_i\\), al condicionar en \\(X\\) quitamos la incertidumbre de los \\(x_i\\) y nos quedamos solo con la parte aleatoria de los errores, por lo que \\[\\begin{align} E[\\hat{\\beta}_1 \\mid X] &amp;= \\beta_1 + \\frac{\\sum_{i=1}^{n} (x_i-\\bar x) E[\\varepsilon_i]}{S_{xx}} = \\beta_1 + \\frac{0}{S_{xx}} = \\beta_1. \\end{align}\\] Como la igualdad es cierta para cualquier conjunto de valores \\(X\\), se concluye que \\[ \\boxed{\\,E(\\hat{\\beta}_1)=\\beta_1\\,} \\] Ahora, probemos que \\(E(\\hat{\\beta}_0)=\\beta_0\\). En efecto, sabemos que \\(\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\\), por lo que \\[\\begin{align} E[\\hat{\\beta}_0 \\mid X] &amp;= E[\\overline{y} \\mid X] - E[\\hat{\\beta}_1 \\mid X]\\overline{x} \\\\[6pt] &amp;= E[\\big(\\beta_0 + \\beta_1 \\overline{x}\\big) \\mid X] - E[\\hat{\\beta}_1 \\mid X]\\overline{x} \\\\[6pt] &amp;= \\big(\\beta_0 + \\beta_1 \\overline{x}\\big) - \\beta_1 \\overline{x} \\\\[6pt] &amp;= \\beta_0. \\end{align}\\] Y como esta igualdad vale para cualquier conjunto de valores \\(X\\) \\[ \\boxed{E(\\hat{\\beta}_0) = \\beta_0} \\] Por otro lado, probemos que \\(V(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}}\\). En efecto; anteriormente probamos que \\[\\begin{align} \\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})\\varepsilon_i}{S_{xx}}, \\end{align}\\] Como \\(Var(\\beta_1)=0\\), ya que \\(\\beta_1\\) es constante entonces \\[\\begin{align} Var(\\hat{\\beta}_1\\mid X) &amp;= Var\\!\\left(\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})\\varepsilon_i}{S_{xx}}\\mid X\\right). \\end{align}\\] Sabemos que los errores son independientes, \\(E(\\varepsilon_i)=0\\) y \\(Var(\\varepsilon_i)=\\sigma\\) por lo que \\[\\begin{align} Var(\\hat{\\beta}_1\\mid X) &amp;= \\frac{1}{S_{xx}^2} \\; Var\\!\\left(\\sum_{i=1}^{n} (x_i - \\bar{x})\\varepsilon_i\\mid X\\right)= \\frac{1}{S_{xx}^2}Var\\!\\left(\\sum_{i=1}^{n} (x_i - \\bar{x})\\varepsilon_i\\mid X\\right) \\\\[6pt] &amp;= \\frac{1}{S_{xx}^2}\\sum_{i=1}^{n} (x_i - \\bar{x})^2 Var(\\varepsilon_i) = \\frac{1}{S_{xx}^2}\\sigma^2 \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\[6pt] &amp;= \\frac{1}{S_{xx}^2}\\sigma^2 S_{xx}=\\frac{\\sigma^2}{S_{xx}} \\end{align}\\] En consecuencia \\[ \\boxed{Var(\\hat{\\beta}_1)= \\frac{\\sigma^2}{S_{xx}}} \\] Probemos que \\(V(\\hat{\\beta}_0) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\). En efecto \\[\\begin{align} Var(\\hat{\\beta}_0\\mid X) &amp;= Var(\\bar{y} - \\hat{\\beta}_1 \\bar{y}\\mid X) \\\\[6pt] &amp;= Var(\\bar{y}\\mid X) + \\bar{x}^2 Var(\\hat{\\beta}_1\\mid X) - 2\\bar{x}\\,Cov(\\bar{y},\\hat{\\beta}_1\\mid X). \\end{align}\\] Pruebe \\(Cov(\\bar{y},\\hat{\\beta}_1\\mid X) = 0\\), entonces \\[\\begin{align} Var(\\hat{\\beta}_0\\mid X) &amp;= Var(\\bar{y}\\mid X) + \\bar{x}^2 Var(\\hat{\\beta}_1\\mid X)\\\\[6pt] &amp;= \\frac{\\sigma^2}{n} + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\[6pt] &amp;=\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}\\right) \\end{align}\\] Por lo tanto \\[ \\boxed{Var(\\hat{\\beta}_0)= \\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}\\right)}\\quad\\square \\] 7.6 Estimación de invervalos de confianza y prueba de hipótesis para \\(\\beta_1, \\beta_0\\) y \\(\\sigma^2\\) Al utilizar un modelo de Regresión Lineal Simple (RLS) queremos estar seguros de que el modelo ajustado es considerablemente mejor que no tener dicho modelo. Esto equivale a determinar si la contribución del factor controlable \\(X\\) para predecir \\(E[Y \\mid X = x_0]\\) es sustancial. Formalmente, esto se traduce en realizar una prueba de significancia global para el modelo. Observación La prueba de significancia global puede realizarse de dos maneras en RLS: 1. A través de un procedimiento de pruebas de hipótesis basado en la distribución \\(t\\) de Student. 2. Utilizando los resultados de la Tabla ANOVA. 7.6.1 Intervalo de confianza para \\(\\beta_1\\) En un modelo de regresión lineal simple, queremos estimar el verdadero valor de la pendiente \\(\\beta_1\\). Para ello construimos un intervalo de confianza (IC) basado en la distribución muestral del estimador \\(\\hat{\\beta}_1\\) y en las propiedades de los estimadores de mínimos cuadrados El estadístico de prueba de la variable estandarizada es: \\[ T = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\widehat{\\sigma}_{\\hat{\\beta}_1}} \\sim t_{(n-2)} \\] donde \\(t_{(n-2)}\\) es la distribución t de Student con \\(n-2\\) grados de libertad. cuya expresión de probabilidad definida de un intervalo de confianza es \\[ P\\left(-t_{\\alpha/2,\\,n-2} &lt; \\frac{\\hat{\\beta}_1 - \\beta_1}{\\widehat{\\sigma}_{\\hat{\\beta}_1}} &lt; t_{\\alpha/2,\\,n-2}\\right) = 1-\\alpha \\] Reordenando la desigualdad y despejando \\(\\beta_1\\), obtenemos el intervalo para la pendiente \\[ \\hat{\\beta}_1 - t_{\\alpha/2,\\,n-2}\\,\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} \\;&lt;\\; \\beta_1 \\;&lt;\\; \\hat{\\beta}_1 + t_{\\alpha/2,\\,n-2}\\,\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} \\tag{6.1} \\] donde \\(\\hat{\\beta}_1\\): estimador de la pendiente. \\(t_{\\alpha/2,\\,n-2}\\): valor crítico de la \\(t\\) de Student. \\(\\hat{\\sigma}^2\\): estimador de la varianza residual. \\(S_{xx} = \\sum (x_i - \\bar{x})^2\\): suma de cuadrados de la variable explicativa. Con este intervalo, afirmamos que con una confianza \\((1-\\alpha)100\\%\\), la verdadera pendiente \\(\\beta_1\\) se encuentra dentro de los límites calculados. Observación Cuando \\(n-2&gt;30\\), podemos aproximar la distribución \\(t_{n-2}\\) a la distribución normal estándar. En R, utilizaríamos la pnorm() en lugar de pt(). Para más detalles, escriba ?pnorm en la consola del R. 7.6.2 Prueba de hipótesis para \\(\\beta_1\\) El par de hipótesis más común en la regresión lineal simple se plantea sobre la pendiente \\(\\beta_1\\): \\[ H_0: \\beta_1 = 0 \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq 0 \\] La interpretación de la hipótesis nula nos queda - Si \\(H_0\\) es verdadera, entonces \\(E(Y \\mid x) = \\beta_0\\), es decir, la variable explicativa \\(x\\) no aporta información sobre la variable dependiente \\(Y\\). - La prueba se conoce como prueba de utilidad del modelo: verifica si la pendiente es significativamente distinta de cero. Si \\(H_0\\) es rechazada (para \\(\\alpha\\) pequeña), confirmamos la utilidad del modelo. Esto suele ocurrir cuando el coeficiente de determinación \\(R^2\\) es grande. El procedimiento de prueba es Planteamiento de hipótesis \\[ H_0 : \\beta_1 = 0 \\quad \\text{vs} \\quad H_1 : \\beta_1 \\neq 0 \\] Cálculo del estadístico de prueba \\[ t_0 = \\frac{\\hat{\\beta}_1}{\\sqrt{\\dfrac{\\hat{\\sigma}^2}{S_{xx}}}} \\] Cálculo del valor p \\[ P = 2P(T &gt; |t_0|), \\quad T \\sim t_{(n-2)} \\] de alli la decisión Si \\(P &lt; \\alpha\\), rechazamos \\(H_0\\) → la variable explicativa \\(x\\) aporta información significativa y el modelo es útil. Si \\(P \\geq \\alpha\\), no rechazamos \\(H_0\\) → no hay evidencia suficiente para afirmar que \\(\\beta_1 \\neq 0\\). 7.6.3 Intervalo de confianza para \\(\\beta_0\\) En la regresión lineal simple, el estimador del intercepto es: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{y} \\] Al igual que con la pendiente, el estadístico de prueba de la variable estandarizada es: \\[ T = \\frac{\\hat{\\beta}_0 - \\beta_0}{\\widehat{\\sigma}_{\\hat{\\beta}_0}} \\sim t_{(n-2)} \\] El error estándar de \\(\\hat{\\beta}_0\\) es: \\[ \\widehat{\\sigma}_{\\hat{\\beta}_0} = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)} \\] donde \\(n\\): número de observaciones. \\(\\bar{x}\\): media de la variable explicativa. \\(S_{xx} = \\sum (x_i - \\bar{x})^2\\). \\(\\hat{\\sigma}^2\\): estimador de la varianza residual. A partir de la expresión de probabilidad, se obtiene que el intervalo de confianza \\((1-\\alpha)100\\%\\) para \\(\\beta_0\\) es: \\[ \\hat{\\beta}_0 - t_{\\alpha/2,\\,n-2}\\,\\sqrt{\\hat{\\sigma}^2 \\left( \\tfrac{1}{n} + \\tfrac{\\bar{x}^2}{S_{xx}} \\right)} \\;&lt;\\; \\beta_0 \\;&lt;\\; \\hat{\\beta}_0 + t_{\\alpha/2,\\,n-2}\\,\\sqrt{\\hat{\\sigma}^2 \\left( \\tfrac{1}{n} + \\tfrac{\\bar{x}^2}{S_{xx}} \\right)} \\] donde \\(\\hat{\\beta}_0\\): estimador del intercepto. \\(t_{\\alpha/2,\\,n-2}\\): valor crítico de la distribución t de Student. \\(\\widehat{\\sigma}_{\\hat{\\beta}_0}\\): error estándar del intercepto. Por lo que el intervalo nos da un rango en el cual se encuentra con probabilidad \\((1-\\alpha)\\) el verdadero valor del intercepto \\(\\beta_0\\), es decir, el valor esperado de \\(Y\\) cuando \\(X = 0\\). 7.6.4 Prueba de hipótesis para \\(\\beta_0\\) El contraste de hipótesis más común sobre el intercepto \\(\\beta_0\\) es \\[ H_0 : \\beta_0 = 0 \\quad \\text{vs} \\quad H_1 : \\beta_0 \\neq 0 \\] La interpretación de la hipótesis nula nos queda Si \\(H_0\\) es verdadera, el modelo pasaría por el origen. En este caso, el valor esperado de \\(Y\\) cuando \\(X=0\\) sería exactamente cero. Rechazar \\(H_0\\) sugiere que existe un desplazamiento inicial (intercepto distinto de cero) que debe ser tenido en cuenta en el modelo. El procedimiento de prueba se realiza así Planteamiento de hipótesis \\[ H_0 : \\beta_0 = 0 \\quad \\text{vs} \\quad H_1 : \\beta_0 \\neq 0 \\] Cálculo del estadístico de prueba \\[ t_0 = \\frac{\\hat{\\beta}_0}{\\widehat{\\sigma}_{\\hat{\\beta}_0}} \\] donde \\[ \\widehat{\\sigma}_{\\hat{\\beta}_0} = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)} \\] Cálculo del valor p \\[ P = 2P(T &gt; |t_0|), \\quad T \\sim t_{(n-2)} \\] La decisión nos queda Si \\(P &lt; \\alpha\\), rechazamos \\(H_0\\): el intercepto es significativamente distinto de cero. Si \\(P \\geq \\alpha\\), no rechazamos \\(H_0\\): no hay evidencia suficiente para afirmar que \\(\\beta_0 \\neq 0\\). 7.6.5 Intervalo de confianza para \\(\\sigma^2\\) En la regresión lineal simple, la varianza del error \\(\\sigma^2\\) se estima mediante \\[ \\hat{\\sigma}^2 = \\frac{SSE}{n-2} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-2} \\] donde \\(SSE\\) es la suma de cuadrados de los residuos. Hallemos el intervalo de confianza La distribución muestral que se tiene es \\[ \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)} \\] es decir, la cantidad \\((n-2)\\hat{\\sigma}^2/\\sigma^2\\) sigue una distribución \\(\\chi^2\\) con \\(n-2\\) grados de libertad. La expresión de probabilidad definida en un intervalo de confianza se deduce de \\[ P\\left( \\chi^2_{\\alpha/2,\\,n-2} &lt; \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} &lt; \\chi^2_{1-\\alpha/2,\\,n-2} \\right) = 1-\\alpha \\] Reordenando la desigualdad y despejando \\(\\sigma^2\\), el intervalo de confianza \\((1-\\alpha)100\\%\\) para \\(\\sigma^2\\) es \\[ \\frac{(n-2)\\hat{\\sigma}^2}{\\chi^2_{1-\\alpha/2,\\,n-2}} \\;&lt;\\; \\sigma^2 \\;&lt;\\; \\frac{(n-2)\\hat{\\sigma}^2}{\\chi^2_{\\alpha/2,\\,n-2}} \\] donde \\(\\hat{\\sigma}^2\\): estimador de la varianza residual. \\(\\chi^2_{\\alpha/2,\\,n-2}\\) y \\(\\chi^2_{1-\\alpha/2,\\,n-2}\\): valores críticos de la distribución \\(\\chi^2\\) con \\(n-2\\) grados de libertad. Este intervalo proporciona un rango plausible para la varianza verdadera de los errores del modelo. 7.6.6 Prueba de hipótesis para \\(\\sigma^2\\) En regresión lineal simple, la varianza de los errores \\(\\sigma^2\\) se puede contrastar mediante la hipótesis: \\[H_0: \\sigma^2 = \\sigma_0^2 \\quad \\text{vs.} \\quad H_1: \\sigma^2 \\neq \\sigma_0^2\\] 7.6.6.1 Estadístico de prueba \\[ T \\;=\\; \\frac{SSE}{\\sigma_0^2} \\;=\\; \\frac{(n-2)\\hat\\sigma^2}{\\sigma_0^2} \\;\\sim\\; \\chi^2_{(n-2)} \\] donde \\(SSE = \\sum_{i=1}^n (y_i - \\hat y_i)^2\\) es la suma de cuadrados de los errores. \\(\\hat\\sigma^2 = SSE/(n-2)\\) es el estimador de la varianza residual. \\(n-2\\) son los grados de libertad del modelo. 7.6.6.2 Regla de decisión Prueba bilateral \\[\\text{Rechazar } H_0 \\text{ si } T &lt; \\chi^2_{\\alpha/2,\\;n-2} \\quad \\text{o} \\quad T &gt; \\chi^2_{1-\\alpha/2,\\;n-2}.\\] Prueba unilateral superior \\[ \\text{Rechazar } H_0 \\text{ si } T &gt; \\chi^2_{1-\\alpha,\\;n-2}. \\] Prueba unilateral inferior \\[ \\text{Rechazar } H_0 \\text{ si } T &lt; \\chi^2_{\\alpha,\\;n-2}. \\] Continuando con el ejemplo de los datos de soldadura, hallemos los intervalos de confianza para \\(\\sigma, \\beta_1,\\beta_0\\) y veamos si son significativos o no. Code # Lectura de datos file &lt;- &quot;data/data_rls.txt&quot; df &lt;- read.table(file, header = TRUE) # Primeras 6 filas head(df) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Construyamos el modelo de regresión lineal Code # ajuste del modelo de RLS modelo &lt;- lm(Resistencia ~ Edad, data = df) modelo ## ## Call: ## lm(formula = Resistencia ~ Edad, data = df) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Estimemos el error estandar residual \\(\\hat{\\sigma}\\): Code # estimación de sigma summary(modelo)$sigma ## [1] 0.5908683 Ahora, hallemos el intervalo de confianza para \\(\\sigma^2\\): Code # intervalo de confianza para sigma^2 alpha &lt;- 0.05 n &lt;- nrow(df) sigma &lt;- summary(modelo)$sigma gl &lt;- n-2 lim_inf &lt;- gl*sigma^2/qchisq(1-alpha/2, gl) lim_sup &lt;- gl*sigma^2/qchisq(alpha/2, gl) c(lim_inf = lim_inf, sigma2_hat =sigma^2, lim_sup = lim_sup) ## lim_inf sigma2_hat lim_sup ## 0.2688068 0.3491253 0.4719150 Finalmente, con una confianza del \\(95\\%\\), \\(\\sigma^2 \\in (0.269,\\;0.472)\\). Ahora, como el valor \\(0\\) no se encuentra en el intervalo de confianza, podemos afirmar que $ ^2 &gt; 0$ a nivel poblacional. Para obtener un intervalo de confianza del \\(95\\%\\) para \\(\\sigma\\), basta calcular la raíz cuadrada de estos límites. Por lo tanto, \\(\\sigma \\in (0.519,\\;0.687)\\). Ejercicio Verifiquemos el contraste de la hipótesis: \\[H_0:\\ \\sigma^2 = 0.25 \\quad\\text{vs.}\\quad H_1:\\ \\sigma^2 \\neq 0.25\\] con nivel de significancia \\(\\alpha = 0.05\\). El estadístico de prueba está dado por: \\[ T = \\frac{(n-2)\\hat\\sigma^2}{0.25} \\;\\;\\sim\\;\\; \\chi^2_{(n-2)} \\] donde \\(\\hat\\sigma^2 = 0.3491\\) es la varianza residual estimada y \\(0.25\\) es el valor hipotético de referencia de la varianza poblacional. Ahora, hallemos el intervalo de confianza para \\(\\beta_0\\) Code n &lt;- nrow(df) x &lt;- df$Edad y &lt;- df$Resistencia x_bar &lt;- mean(x) y_bar &lt;- mean(y) # Sxx Sxx &lt;- sum((x - x_bar)^2) # Pendiente e intercepto b1 &lt;- coef(modelo)[2] b0 &lt;- coef(modelo)[1] # Sigma estimada sigma_hat &lt;- summary(modelo)$sigma # Varianza de b0 var_b0 &lt;- sigma_hat^2 * (1/n + (x_bar^2)/Sxx) se_b0 &lt;- sqrt(var_b0) # Valor crítico t alpha &lt;- 0.05 t_crit &lt;- qt(1-alpha/2, df = n - 2) # Intervalo de confianza inferior &lt;- b0 - t_crit * se_b0 superior &lt;- b0 + t_crit * se_b0 c( LI = inferior , beta0 = b0, LS = superior) ## LI.(Intercept) beta0.(Intercept) LS.(Intercept) ## 15.38816 15.74275 16.09734 Ahora, verifiquemos la prueba de hipótesis \\(H_0:\\beta_0=0\\, vs\\, H_1:\\beta_0\\neq 0\\) Code # intercepto b0 &lt;- coef(modelo)[1] # se(b0) = sigma_hat * sqrt( 1/n + x̄^2 / Sxx ) var_b0 &lt;- sigma_hat^2 * (1/n + (x_bar^2)/Sxx) se_b0 &lt;- sqrt(var_b0) # Hipótesis: H0: beta0 = 0 vs H1: beta0 != 0 t_calc &lt;- (b0 - 0) / se_b0 gl &lt;- n-2 p_valor &lt;- 2 * (1 - pt(abs(t_calc), gl)) Code # Convertir a data.frame para tabla resultados &lt;- data.frame( beta0_hat = b0, se_beta0 = se_b0, estadistico_t = t_calc, gl = gl, p_value = p_valor ) # Mostrar tabla con kable knitr::kable(resultados, digits = 4, caption = &quot;Resultados de la prueba de hipótesis para β₀&quot;) Tabla 7.1: Resultados de la prueba de hipótesis para β₀ beta0_hat se_beta0 estadistico_t gl p_value (Intercept) 15.7427 0.1787 88.105 98 0 Ahora, hallemos el intervalo de confianza para \\(\\beta_1\\) Code # Pendiente b1 &lt;- coef(modelo)[2] # Varianza y error estándar de b1 var_b1 &lt;- sigma_hat^2 / Sxx se_b1 &lt;- sqrt(var_b1) # Valor crítico t (ya lo tenías calculado) t_calc &lt;- qt(1-alpha/2, df = n - 2) # Intervalo de confianza para beta_1 inferior &lt;- b1 - t_calc* se_b1 superior &lt;- b1 + t_calc * se_b1 c(LI = inferior, beta1 = b1, LS = superior) ## LI.Edad beta1.Edad LS.Edad ## -0.2697692 -0.2437636 -0.2177580 Ahora, verifiquemos la prueba de hipótesis \\(H_0:\\beta_1=0\\, vs\\, H_1:\\beta_1\\neq 0\\) Code # Pendiente b1 &lt;- coef(modelo)[2] # Varianza y error estándar de beta1 var_b1 &lt;- sigma_hat^2 / Sxx se_b1 &lt;- sqrt(var_b1) # Hipótesis: H0: beta1 = 0 vs H1: beta1 != 0 t_calc &lt;- (b1 - 0) / se_b1 gl &lt;- n - 2 p_valor&lt;- 2 * (1 - pt(abs(t_calc), gl)) Code # Organizar resultados en data.frame resultados_b1 &lt;- data.frame( beta1_hat = b1, se_beta1 = se_b1, estadistico_t = t_calc, gl = gl, p_value = p_valor ) # Mostrar tabla knitr::kable(resultados_b1, digits = 4, caption = &quot;Prueba de hipótesis para β₁ (pendiente)&quot;) Tabla 7.2: Prueba de hipótesis para β₁ (pendiente) beta1_hat se_beta1 estadistico_t gl p_value Edad -0.2438 0.0131 -18.6014 98 0 Para probar si los calculos paso a paso estan correcto usamos las siguientes instrucciones: Code # estadísticos de prueba para beta_0 y beta_1 coefficients(summary(modelo)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.7427499 0.17868164 88.10502 3.989520e-95 ## Edad -0.2437636 0.01310455 -18.60144 6.394519e-34 De acuerdo con los resultados de la regresión lineal simple, se observa que Para el intercepto (\\(\\beta_0\\)): con una confianza del \\(95\\%\\), podemos decir el intercepto de la regresión lineal es significativo (\\(t_{(98)}=88.11, p-valor &lt; 0.001\\)) Para la pendiente (\\(\\beta_1\\)): con una confianza del \\(95\\%\\), podemos afirmar que la pendiente asociada a la variable Edad es significativa (\\(t_{(98)} = -18.60,\\; p-valor &lt; 0.001\\)), lo cual indica que existe una relación negativa entre Edad y Resistencia. En conclusión, ambos coeficientes resultan significativos al nivel del \\(5\\%\\) (e incluso a niveles mucho más estrictos), lo que confirma que el modelo de regresión lineal simple es adecuado para explicar la relación entre Edad y Resistencia. Para el mismo nivel de significancia, los intervalos de confianza pueden calcularse como en R a través de la función confint.default: Code # intervalos de confianza via confint confint.default(modelo) ## 2.5 % 97.5 % ## (Intercept) 15.392540 16.0929595 ## Edad -0.269448 -0.2180791 Para el intercepto (\\(\\beta_0\\)): con una confianza del \\(95\\%\\), el intervalo de confianza es \\((15.39,\\;16.09)\\). Como este intervalo no contiene el valor \\(0\\), podemos concluir que el intercepto es significativo en el modelo. Para la pendiente (\\(\\beta_1\\)): con una confianza del \\(95\\%\\), el intervalo de confianza es \\((-0.269,\\;-0.218)\\). Como el intervalo no incluye el valor \\(0\\), concluimos que la pendiente es estadísticamente significativa. Además, el signo negativo indica que existe una relación inversa entre la variable Edad y la variable Resistencia: a mayor Edad, menor Resistencia. En conclusión, tanto el intercepto como la pendiente resultan significativos al \\(95\\%\\) de confianza, lo que respalda la validez del modelo de regresión lineal simple para explicar la relación entre Edad y Resistencia. 7.6.7 ANOVA en regresión lineal El objetivo del ANOVA en el contexto de la regresión lineal es evaluar si el modelo, en su conjunto, explica una proporción significativa de la variabilidad de la variable respuesta \\(Y\\). 7.6.7.1 Descomposición de la variabilidad La suma total de cuadrados (SST) puede descomponerse en: \\[SST = SSR + SSE\\] \\(SST\\): variabilidad total de \\(Y\\). \\(SSR\\): variabilidad explicada por el modelo (regresión). \\(SSE\\): variabilidad no explicada (error o residuos). Los grados de libertad se calculan de la siguiente manera \\(gl_{T} = n-1\\) \\(gl_{R} = k\\) (número de predictores; en RLS simple \\(k=1\\)) \\(gl_{E} = n-k-1\\) 7.6.7.2 Estadístico \\(F\\) El estadístico de prueba se construye como: \\[F = \\frac{MSR}{MSE} = \\frac{SSR / gl_R}{SSE / gl_E} \\sim F_{(gl_R,\\;gl_E)} \\quad \\text{bajo } H_0\\] donde: \\(MSR\\): cuadrado medio de la regresión. \\(MSE\\): cuadrado medio del error. 7.6.8 Hipótesis global \\[ H_0:\\ \\beta_1=0 \\quad\\text{vs.}\\quad H_1:\\ \\beta_1 \\neq 0 \\] Rechazar \\(H_0\\) indica que el modelo explica una parte significativa de la variabilidad de \\(Y\\). Interpretación práctica de \\(F_{calculado}\\) \\(F_{calc} \\ll 1\\): los factores incontrolables explican mucho más que el factor controlable \\(X\\). Conviene explorar otro predictor. \\(F_{calc} \\approx 1\\): la variabilidad de \\(Y\\) explicada por el factor controlable y por los factores incontrolables es similar. El predictor actual apenas aporta al modelo. \\(F_{calc} \\gg 1\\): la mayor parte de la variabilidad de \\(Y\\) se explica por el factor controlable \\(X\\) más que por los factores incontrolables. Este es el caso ideal. Continuando con el ejemplo de soldadura Code # Ajustar modelo de regresión modelo &lt;- lm(Resistencia ~ Edad, data = df) # ANOVA del modelo anova(modelo) ## Analysis of Variance Table ## ## Response: Resistencia ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Edad 1 120.802 120.802 346.01 &lt; 2.2e-16 *** ## Residuals 98 34.214 0.349 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A partir del ajuste del modelo de Regresión Lineal Simple, se obtuvo el siguiente estadístico: Dado que el \\(p-valor &lt; 0.001\\) es muchísimo menor que el nivel de significancia usual (\\(\\alpha = 0.05\\)), se rechaza la hipótesis nula y se concluye que el modelo ajustado es globalmente significativo. En otras palabras, la variable Edad contribuye de manera estadísticamente significativa a explicar la variabilidad de la variable Resistencia. Para obtener todo lo desarrollado paso a paso de cada una de las estimaciones podemos realizar el siguiente paso: Code summary(modelo) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6852 -0.3543 0.0209 0.4225 1.8004 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.7427 0.1787 88.11 &lt;2e-16 *** ## Edad -0.2438 0.0131 -18.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5909 on 98 degrees of freedom ## Multiple R-squared: 0.7793, Adjusted R-squared: 0.777 ## F-statistic: 346 on 1 and 98 DF, p-value: &lt; 2.2e-16 7.7 Transformaciones para mejorar el ajuste del modelo En este capítulo se explora cómo las transformaciones matemáticas aplicadas a la variable respuesta (Y) o a las covariables (X) pueden ser utilizadas para mejorar el ajuste de un modelo lineal. Cuando los datos no presentan una relación lineal directa, estas transformaciones permiten acercar el comportamiento observado a uno que sí pueda ser representado por un modelo lineal clásico, facilitando así su análisis e interpretación. 7.7.1 ¿Por qué transformar variables? En muchos casos, el modelo teórico que relaciona Y con X no es estrictamente lineal. Sin embargo, mediante transformaciones adecuadas, es posible linealizar la relación, de modo que el modelo ajustado en la nueva escala cumpla con los supuestos de regresión lineal (como homocedasticidad, normalidad de errores, y linealidad). Estas transformaciones son particularmente útiles cuando: Se detecta curvatura en los residuos. La dispersión de los errores aumenta con el valor de X o Y. El modelo lineal muestra falta de ajuste significativa. Se busca estabilizar la varianza o mejorar la normalidad de los residuos. 7.7.2 Tipos comunes de transformaciones A continuación se presentan algunos tipos comunes de transformaciones para X o Y que permiten linealizar relaciones no lineales: Forma original del modelo no lineal Transformación sugerida Modelo lineal equivalente \\(Y = a \\cdot X^b\\) log(Y) ~ log(X) Log-log (modelo potencial) \\(Y = a \\cdot e^{bX}\\) log(Y) ~ X Log-lineal (modelo exponencial) \\(Y = \\frac{1}{a + bX}\\) 1/Y ~ X Inverso de Y \\(Y = a + b \\cdot \\log(X)\\) Y ~ log(X) Lineal en log(X) \\(Y^2 = a + bX\\) sqrt(Y) ~ X o Y^2 ~ X Transformación raíz o cuadrática \\(Y = a + b \\cdot \\sqrt{X}\\) Y ~ sqrt(X) Lineal en raíz cuadrada de X La elección de la transformación debe basarse tanto en criterios teóricos como empíricos (exploración visual, pruebas de falta de ajuste, etc.). 7.7.3 Evaluación posterior a la transformación Una vez aplicado el cambio de escala, es indispensable: Volver a graficar la relación entre las variables transformadas. Revisar los residuos del nuevo modelo. Evaluar si la transformación mejoró la linealidad y redujo la falta de ajuste. Si es necesario hacer predicciones, aplicar una retrotransformación para volver a la escala original de la variable respuesta. 7.7.3.1 Transformación de la variable explicativa En este caso, usarás el conjunto de datos taiwan_real_estate, y tomarás como variable explicativa la distancia a la estación de metro más cercana (dist_to_mrt_m), aplicando una transformación de raíz cuadrada. El objetivo es observar si la relación con el precio por metro cuadrado (price_twd_msq) mejora. Carguemos los datos Code library(fst) file_path &lt;- file.path(&quot;data/taiwan_real_estate.fst&quot;) taiwan_real_estate &lt;- read_fst(file_path) # convertirlo en un factor taiwan_real_estate$house_age_years &lt;- as.factor(taiwan_real_estate$house_age_years) # muestra las 3 primeras filas head(taiwan_real_estate, 3) ## dist_to_mrt_m n_convenience house_age_years price_twd_msq ## 1 84.87882 10 30 to 45 11.46747 ## 2 306.59470 9 15 to 30 12.76853 ## 3 561.98450 5 0 to 15 14.31165 Visualicemos la relación original Code taiwan_real_estate %&gt;% ggplot(aes(dist_to_mrt_m, price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Relacion original: distancia vs. precio&quot;)+ theme_bw() Ajustando el modelo de línea recta tenemos Code modelo1 &lt;- lm(price_twd_msq~ dist_to_mrt_m, data=taiwan_real_estate) El resumen del modelo1 es Code summary(modelo1) ## ## Call: ## lm(formula = price_twd_msq ~ dist_to_mrt_m, data = taiwan_real_estate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.7097 -1.8177 -0.3617 1.4616 22.2338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.8733516 0.1974616 70.26 &lt;2e-16 *** ## dist_to_mrt_m -0.0021973 0.0001188 -18.50 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 412 degrees of freedom ## Multiple R-squared: 0.4538, Adjusted R-squared: 0.4524 ## F-statistic: 342.2 on 1 and 412 DF, p-value: &lt; 2.2e-16 Del resultado anterior se observa que La distancia a la estación de metro más cercana (dist_to_mrt_m) es significativa por que el \\(p-valor &lt; 0.0001\\). Esto indica que existe evidencia estadística suficiente para afirmar que la distancia al metro influye en el precio por metro cuadrado. El coeficiente asociado a dist_to_mrt_m es \\(-0.0021973\\), lo que indica que por cada metro adicional que se aleja una propiedad de la estación de metro, el precio por metro cuadrado disminuye en promedio 0.0022 millones de \\(TWD/m^2\\) (o sea, aproximadamente 2.197 \\(TWD/m^2\\)). En otras palabras, a mayor distancia al metro, menor precio, lo cual tiene sentido desde un punto de vista urbano y económico. El coeficiente de determinación \\(R^2=0.4538\\) indica que el modelo explica aproximadamente el \\(45.38\\%\\) de la variabilidad en el precio por metro cuadrado. El modelo es estadísticamente significativo en su conjunto, ya que el \\(p-valor\\) del estadístico \\(F\\) es \\(&lt;0.001\\). El error estándar residual (3.046) sugiere una dispersión moderada de los residuos alrededor de la recta de regresión. Ahora, si deseamos ajustar el modelo para mejorar el coeficiente de determinación, es necesario transformar la variable independiente, es decir, vamos a entrenar el modelo con \\(X* = \\sqrt{X}\\). Transformemos la variable explicativa (raíz cuadrada) Code taiwan_real_estate %&gt;% ggplot(aes(sqrt(dist_to_mrt_m), price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Transformacion: raiz cuadrada de la distancia vs. precio&quot;)+ theme_bw() Observa si los puntos siguen mejor la línea de regresión después de la transformación. Si es así, el modelo lineal será más adecuado Ajustemos el modelo Code modelo2 &lt;- lm(price_twd_msq~ sqrt(dist_to_mrt_m), data=taiwan_real_estate) Code summary(modelo2) ## ## Call: ## lm(formula = price_twd_msq ~ sqrt(dist_to_mrt_m), data = taiwan_real_estate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.7843 -1.5410 -0.2594 1.2294 21.7483 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.709799 0.277741 60.16 &lt;2e-16 *** ## sqrt(dist_to_mrt_m) -0.182843 0.008436 -21.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.818 on 412 degrees of freedom ## Multiple R-squared: 0.5327, Adjusted R-squared: 0.5316 ## F-statistic: 469.7 on 1 and 412 DF, p-value: &lt; 2.2e-16 De la tabla de coeficientes del modelo se observa que: La variable sqrt(dist_to_mrt_m) es estadísticamente significativa, ya que su \\(p-valor &lt; 0.0001\\). Por cada unidad que aumenta la raíz cuadrada de la distancia a la estación de MRT (sqrt(dist_to_mrt_m)), el valor de la variable dependiente disminuye en 0.182843 unidades, manteniendo constante el resto de variables. El modelo presenta un \\(R^2\\) de 0.5327, lo cual indica que aproximadamente el \\(53.27\\%\\) de la variabilidad en la variable de respuesta puede ser explicada por sqrt(dist_to_mrt_m). El modelo es altamente significativo en su conjunto, como lo demuestra el estadístico \\(F = 469.7\\) y el \\(p-valor &lt; 0.001\\). El error estándar residual (2.818) sugiere una dispersión moderada de los residuos alrededor de la recta de regresión. 7.7.3.2 Transformación de la variable de respuesta También es posible transformar la variable dependiente (respuesta), por ejemplo cuando sus valores están muy sesgados o varían demasiado. Pero al hacerlo, se requiere luego una “retrotransformación” para volver a la escala original e interpretar correctamente. En este caso, usarás el conjunto ad_conversion, con: n_impressions: número de veces que se mostró el anuncio. n_clicks: número de clics recibidos. Se transformará ambas variables con una potencia 0.25 (raíz cuarta) para estabilizar la relación. Carguemos los datos Code library(fst) file_path &lt;- file.path(&quot;data/ad_conversion.fst&quot;) ad_conversion &lt;- read_fst(file_path) # muestra las 3 primeras filas head(ad_conversion, 3) ## spent_usd n_impressions n_clicks ## 1 1.43 7350 1 ## 2 1.82 17861 2 ## 3 1.25 4259 1 Visualicemos la relación original Code ad_conversion %&gt;% ggplot(aes(n_impressions, n_clicks)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE)+ theme_bw() Ajustando el modelo de línea recta tenemos Code modelo1 &lt;- lm(n_clicks ~ n_impressions,data = ad_conversion) El resumen del modelo1 es Code summary(modelo1) ## ## Call: ## lm(formula = n_clicks ~ n_impressions, data = ad_conversion) ## ## Residuals: ## Min 1Q Median 3Q Max ## -186.099 -5.392 -1.422 2.070 119.876 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.683e+00 7.888e-01 2.133 0.0331 * ## n_impressions 1.718e-04 1.960e-06 87.654 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.91 on 934 degrees of freedom ## Multiple R-squared: 0.8916, Adjusted R-squared: 0.8915 ## F-statistic: 7683 on 1 and 934 DF, p-value: &lt; 2.2e-16 De la tabla anterior se observa que: La variable n_impressions es altamente significativa, ya que su \\(p-valor &lt; 0.001\\). Esto indica una fuerte evidencia estadística de que esta variable tiene un efecto sobre la variable dependiente. Por cada impresión adicional, se espera que la variable dependiente (probablemente ingresos, clics o visitas, dependiendo del contexto) aumente en 0.0001718 unidades, manteniendo todo lo demás constante. El \\(R^2\\) es 0.8916, lo cual indica que el modelo explica el 89.16% de la variabilidad de la variable respuesta. Esto representa un ajuste excelente del modelo a los datos. El modelo completo es significativo, ya que el \\(p-valor\\) del estadístico \\(F\\) también es menor a 0.001. El error estándar residual (19.91) sugiere una dispersión alta de los residuos alrededor de la recta de regresión. Transformemos la variable explicativa (raíz cuadrada) Code ad_conversion %&gt;% ggplot(aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE)+ theme_bw() Observa si los puntos siguen mejor la línea de regresión después de la transformación. Si es así, el modelo lineal será más adecuado Ajustemos el modelo Code modelo2 &lt;- lm( I(n_clicks ^ 0.25) ~ I(n_impressions ^ 0.25), data = ad_conversion ) Code summary(modelo2) ## ## Call: ## lm(formula = I(n_clicks^0.25) ~ I(n_impressions^0.25), data = ad_conversion) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57061 -0.13229 0.00582 0.14494 0.46888 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0717479 0.0172019 4.171 3.32e-05 *** ## I(n_impressions^0.25) 0.1115330 0.0008844 126.108 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1969 on 934 degrees of freedom ## Multiple R-squared: 0.9445, Adjusted R-squared: 0.9445 ## F-statistic: 1.59e+04 on 1 and 934 DF, p-value: &lt; 2.2e-16 La variable I(n_impressions^0.25) es altamente significativa (p &lt; 0.001), lo que indica una relación robusta con la variable dependiente. Por cada unidad que aumenta n_impressions^0.25, la respuesta se incrementa en 0.1115 unidades, manteniendo constante el intercepto. El modelo presenta un ajuste excelente, con un \\(R^2\\) de 0.9445, lo que indica que explica el \\(94.45\\%\\) de la variabilidad observada. El modelo es estadísticamente significativo en su conjunto (\\(F = 15900, p-valor &lt; 0.001\\)). El error estándar residual (0.1969) refleja una baja dispersión de los residuos respecto a la línea de ajuste. 7.8 Análisis de residuales En esta sección se presentan diversas herramientas y técnicas que permiten realizar un diagnóstico integral del modelo ajustado, facilitando la evaluación de su validez, precisión y supuestos xestadísticos. 7.8.1 Validación de supuestos En la formulación matemática del modelo, se estableció que \\[ \\epsilon \\sim N(0,\\sigma^2), \\quad \\sigma^2 = \\text{constante} \\] y \\(\\epsilon_i\\), para \\(i=1,\\dots,n\\) son independientes. Por lo tanto, cuando se ajusta un modelo de RLS, debemos validar los siguientes supuestos sobre el error: Independencia Normalidad Media cero Varianza \\(\\sigma^2\\) constante Observación Sólo cuando se han verificado todos los supuestos asociados a los errores, es posible proceder a realizar la predicción de valores futuros de la variable respuesta utilizando el modelo de Regresión Lineal Simple (RLS) previamente ajustado. 7.8.2 Independencia de los errores Para analizar mediante una visualización esta situación es necesario contar con el historial de los errores, es decir, conocer el orden en que se realizaron las observaciones. Con esta información se puede elaborar un diagrama de dispersión de los residuales en función del tiempo, similar al que se muestra a continuación. Figura 7.1: Diagrama de dispersión entre los residuos y el tiempo En la siguiente figura se muestran los distintos patrones que pueden observarse en el gráfico de los residuales \\(\\epsilon_i\\) en función del tiempo. Para que se cumpla el supuesto de independencia, se espera que los puntos se dispongan como una nube aleatoria, sin presentar ningún patrón definido. Figura 7.2: Análisis de independencia Para validar el supuesto de independencia de los errores se recomiendan dos aproximaciones: la prueba formal de independencia de Durbin-Watson y la prueba gráfica basada en la Función de Autocorrelación (ACF en inglés). Pruebas para la independencia de los errores La prueba de Durbin-Watson se encuentra disponible en la función durbinWatsonTest del paquete car, y permite evaluar la presencia de autocorrelación en los residuales del modelo. La Función de Autocorrelación (ACF) puede calcularse mediante la función acf. Para obtener más detalles sobre su uso, escriba ?acf en la consola de R. Continuando con el ejemplo de soldadura Code ## Lectura de datos desde archivo local file &lt;- &quot;data/data_rls.txt&quot; df &lt;- read.table(file, header = TRUE) ## Primeras 6 filas head(df) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Hagamos el modelo de regresión lineal simple: Code modelo &lt;- lm(Resistencia ~ Edad, data = df) modelo ## ## Call: ## lm(formula = Resistencia ~ Edad, data = df) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Hagamos el gráfico de independencia Code library(tidyverse) # Crear un data frame con residuales y orden df_resid &lt;- data.frame( Orden = 1:length(residuals(modelo)), Residuales = residuals(modelo) ) # Gráfico con ggplot2 ggplot(df_resid, aes(x = Orden, y = Residuales)) + geom_point(color = &quot;red&quot;, size = 3) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + labs( title = &quot;Diagrama de dispersión de los residuales vs tiempo&quot;, x = &quot;Orden (Tiempo)&quot;, y = &quot;Residuales&quot; ) + theme_bw(base_size = 14) Apliquemos la prueba de Durbin-Watson haciendo: Code ## prueba de Durbin-Watson require(car) car:::durbinWatsonTest(modelo) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.02653343 2.050609 0.824 ## Alternative hypothesis: rho != 0 Basados en el valor \\(p\\), no es posible rechazar la hipótesis nula \\(H_0\\), ya que el valor \\(p\\) obtenido en la prueba de Durbin-Watson es superior al nivel de significancia \\(\\alpha = 0.05\\). Por lo tanto, se concluye que los residuales del modelo ajustado son independientes, cumpliéndose así el supuesto de independencia de los errores. Para graficar la ACF podemos hacer: Code # Cálculo de los residuales del modelo ajustado r &lt;- residuals(modelo) # Gráfico de la función de autocorrelación (ACF) acf( r, main = &quot;Función de Autocorrelación (ACF) de los residuales&quot;, ylab = &quot;Autocorrelación&quot;, xlab = &quot;Rezago (Lag)&quot;, col = &quot;black&quot;, las = 1 ) Para concluir que los residuales del modelo ajustado son independientes, la gráfica de la Función de Autocorrelación (ACF) debe mostrar que ninguna de las barras verticales —que representan las correlaciones entre errores— supera los límites de confianza (bandas azules) para valores de \\(\\text{Lag} &gt; 0\\). En este caso, dado que todas las correlaciones se encuentran dentro de dichos límites, podemos afirmar que no existe autocorrelación significativa entre los residuales, por lo tanto, se cumple el supuesto de independencia de los errores. 7.8.3 Normalidad de los errores La verificación del supuesto de normalidad de los errores implica realizar la siguiente prueba de hipótesis: \\[ H_0 : \\text{Los errores siguen una distribución normal.} \\] \\[ H_A : \\text{Los errores no siguen una distribución normal.} \\] Este supuesto puede evaluarse mediante pruebas estadísticas formales o de manera gráfica, utilizando histogramas, gráficos de densidad o el gráfico cuantil-cuantil (Q-Q plot), que permite comparar la distribución de los residuales con una distribución normal teórica. Existen diversas pruebas para comprobar la normalidad, varias de ellas implementadas en R, lo que facilita su aplicación y análisis. Pruebas de normalidad para los errores Entre las principales pruebas de normalidad disponibles en R, se encuentran las siguientes: Prueba de Shapiro-Wilk, implementada en la función shapiro.test(). Prueba de Anderson-Darling, disponible en la función ad.test() del paquete nortest. Prueba de Cramér–von Mises, implementada mediante la función cvm.test() del paquete nortest. Prueba de Lilliefors (Kolmogorov–Smirnov), disponible en la función lillie.test() del paquete nortest. Prueba de Pearson, basada en la distribución \\(\\chi^2\\), implementada en la función pearson.test() del paquete nortest. Prueba de Shapiro–Francia, implementada en la función sf.test() del paquete nortest. Cada una de estas pruebas evalúa la hipótesis de que los errores provienen de una distribución normal, siendo la prueba de Shapiro–Wilk la más recomendada para muestras pequeñas y medianas. El gráfico cuantil-cuantil (Q-Q plot) compara los cuantiles teóricos de una distribución Normal con los cuantiles muestrales de los errores estimados del modelo. Este gráfico permite evaluar visualmente si los residuales siguen aproximadamente una distribución normal: cuanto más cerca estén los puntos de la línea diagonal, mayor será el grado de normalidad. En R existen diversas implementaciones para generar este tipo de gráfico, entre ellas: - La función qqnorm() del paquete base, que genera el gráfico Q-Q. - La función qqline(), que añade la línea de referencia al gráfico. - La función qqPlot() del paquete car, que ofrece una versión más completa y personalizada del gráfico. Como ejemplo, para aplicar la prueba de normalidad de Shapiro–Wilk sobre los residuales de un modelo ajustado, puede utilizarse el siguiente código, considerando que el objeto modelo contiene los resultados del ajuste: Code # Cálculo de los residuales r &lt;- residuals(modelo) # Prueba de normalidad de Shapiro-Wilk shapiro.test(r) ## ## Shapiro-Wilk normality test ## ## data: r ## W = 0.98872, p-value = 0.5629 Code # Gráfico Q-Q base qqnorm(r, main = &quot;Gráfico Q-Q de los residuales&quot;) qqline(r, col = &quot;blue&quot;, lwd = 2) Estos resultados indican que el valor \\(p\\) de la prueba de normalidad es \\(p = 0.563\\). Dado que \\(p &gt; 0.05\\), no se rechaza la hipótesis nula \\(H_0\\) y se concluye que los residuales del modelo ajustado pueden considerarse provenientes de una distribución Normal. En R, el gráfico cuantil–cuantil (Q–Q plot) puede generarse de dos formas: - Con funciones base: qqnorm() y qqline(). - Con la función qqPlot() del paquete car. Code # Q-Q plot usando el paquete car library(car, quietly = TRUE) car:::qqPlot(r, las = 1) ## [1] 77 18 Aunque la elaboración de este gráfico es mucho más compleja, su interpretación es simple: diremos que los residuales del modelo ajustado cumplen con el supuesto de Normalidad si todos los puntos en encuentran dentro de las bandas de color azul. En este caso, las observaciones 18 y 77 parecen ser extremas. 7.8.4 Media cero de los errores La validación del supuesto de media cero de los errores se plantea como la siguiente prueba de hipótesis: \\[ H_0:\\ \\mu_{\\varepsilon}=0 \\qquad\\text{vs}\\qquad H_A:\\ \\mu_{\\varepsilon}\\neq 0, \\] donde \\(\\mu_{\\varepsilon}\\) es la media poblacional del término de error. Nota importante Cuando el modelo de Regresión Lineal por Mínimos Cuadrados Ordinarios (MCO) incluye intercepto, el procedimiento de estimación garantiza que la media muestral de los residuales es exactamente cero: \\[ \\bar e = \\frac{1}{n}\\sum_{i=1}^{n} e_i = 0. \\] Por ello, no es necesario validar este supuesto en ese caso; está implícitamente satisfecho. Si el modelo no tiene intercepto o se desea corroborarlo de forma explícita, puede realizarse una prueba \\(t\\) de una muestra sobre los residuales: Code # Residuales del modelo ajustado (objeto &#39;modelo&#39;) r &lt;- residuals(modelo) # Comprobación numérica de la media muestral (debe ser ~ 0 con intercepto) mean(r) ## [1] -4.617007e-18 Code # Prueba t de una muestra: H0: media(r) = 0 t.test(r, mu = 0) ## ## One Sample t-test ## ## data: r ## t = -7.8537e-17, df = 99, p-value = 1 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.1166475 0.1166475 ## sample estimates: ## mean of x ## -4.617007e-18 El resultado de la prueba arroja un valor \\(p = 1.00\\); por lo tanto, no se rechaza la hipótesis nula \\(H_0: \\mu_{\\varepsilon}=0\\). En consecuencia, no existe evidencia estadística para afirmar que la media poblacional de los residuales difiera de cero. A un nivel de significancia \\(\\alpha = 0.05\\) (95% de confianza), concluimos que la media de los errores puede considerarse igual a cero. 7.8.5 Varianza constante de los errores En la figura se comparan dos escenarios: varianza constante \\(\\sigma^{2}\\) (homocedasticidad) y varianza no constante \\(\\sigma^{2}\\) (heterocedasticidad). Figura 7.3: Análisis de homocedasticidad La homocedasticidad implica que la dispersión de los residuales es similar a lo largo de todo el rango de valores ajustados; en cambio, la heterocedasticidad muestra un patrón de apertura o cierre (“embudo”) donde la variabilidad cambia con el nivel de la predicción. Para los modelos de regresión lineal, se asume homocedasticidad: este supuesto garantiza estimaciones correctas de los errores estándar y la validez de las pruebas de inferencia (\\(t\\) y \\(F\\)). Para verificar la varianza constante de los errores se recomienda construir el gráfico de \\(\\varepsilon_i\\) (errores) versus \\(\\hat{\\mu}_i\\) (valores ajustados). Si la varianza es constante (homocedasticidad), los puntos se dispersan de forma similar alrededor de cero en todo el rango de \\(\\hat{\\mu}_i\\); si aparece un patrón de “embudo” u otra estructura, sugiere heterocedasticidad. A continuación, se muestra un diagrama de este tipo (similar al ilustrado en la figura). Figura 7.4: Análisis de dispersión En la figura siguiente se ilustran patrones típicos que pueden aparecer en el gráfico de errores \\(\\varepsilon_i\\) frente a los valores ajustados \\(\\hat{\\mu}_i\\). Para que se cumpla el supuesto de homocedasticidad (varianza constante de los errores), los puntos deben formar una nube aleatoria alrededor de cero, sin estructura ni tendencia visibles. Cualquier patrón sistemático (p. ej., forma de embudo, abanico, bandas o curvaturas) constituye evidencia de heterocedasticidad, indicando que la varianza de los errores cambia con el nivel de \\(\\hat{\\mu}_i\\). Figura 7.5: Patrones de la homocedasticidad Debemos validar el supuesto de varianza constante de los errores. Formalmente, se plantea la siguiente prueba de hipótesis: \\[ H_0:\\ \\sigma^2 \\text{ es constante} \\qquad\\text{vs}\\qquad H_1:\\ \\sigma^2 \\text{ no es constante}. \\] Prueba de homocedasticidad La verificación puede realizarse mediante pruebas estadísticas o métodos gráficos: Prueba de Breusch–Pagan En car: ncvTest(modelo) En lmtest: bptest(modelo) Métodos gráficos Dispersión residuales vs valores ajustados \\((\\hat{y})\\) Residuales vs cada predictor \\((x)\\) Residuales vs orden/tiempo de medición En el caso ideal, los puntos forman una nube aleatoria sin patrón . veamos la grafica Code # Datos para el diagnóstico diag_df &lt;- data.frame( Ajustados = fitted(modelo), Residuales = resid(modelo), ResStd = rstandard(modelo) # opcional: residuales estandarizados ) # Homocedasticidad: Residuales vs Ajustados (ggplot) ggplot(diag_df, aes(x = Ajustados, y = Residuales)) + geom_point(size = 3, alpha = 0.85) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, span = 0.9) + labs( title = &quot;Chequeo de homocedasticidad: Residuales vs Valores ajustados&quot;, x = &quot;Valores ajustados (ŷ)&quot;, y = &quot;Residuales (εᵢ)&quot; ) + theme_bw(base_size = 14) Sin entrar en los detalles formales, la prueba de ** ** busca determinar, estadísticamente, si la varianza del modelo de RLS ajustado es constante o no. En R podemos realizar esta prueba con a función bptest del paquete lmtest: Code # prueba de Breusch-Pagan para varianza constante require(lmtest) lmtest:::bptest(modelo) ## ## studentized Breusch-Pagan test ## ## data: modelo ## BP = 9.7572, df = 1, p-value = 0.001786 Como el \\(p-valor\\) de la prueba es mayor que \\(0.05\\), concluimos que la varianza del modelo no es costante. Code # prueba de Breusch-Pagan con car ncvTest(modelo) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 12.55073, Df = 1, p = 0.00039605 7.9 Estimación e inferencia para la \\(\\widehat{\\mathbb{E}}(y\\mid x_0)\\) en RLS Sea el modelo de Regresión Lineal Simple: \\[ y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i,\\qquad \\varepsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2),\\ i=1,\\ldots,n . \\] Para un valor dado \\(x_0\\) (dentro del rango observado de \\(x\\)), el estimador insesgado de la respuesta media \\(\\mathbb{E}(y\\mid x_0)\\) es \\[ \\widehat{\\mathbb{E}}(y\\mid x_0);= \\hat{\\mu}_{y/x_0}\\;=\\;\\hat y(x_0)\\;=\\;\\hat\\beta_0+\\hat\\beta_1\\,x_0 . \\] Como \\(\\hat y(x_0)\\) es una combinación lineal de las \\(y_i\\), su varianza es \\[ \\begin{align} \\operatorname{Var}\\!(\\hat{\\mu}_{y\\mid x_0}) &amp;= \\operatorname{Var}\\!\\left(\\hat{\\beta}_0+\\hat{\\beta}_1 x_0\\right) \\\\[4pt] &amp;= \\operatorname{Var}\\!\\left(\\bar{y}+\\hat{\\beta}_1(x_0-\\bar{x})\\right) \\\\[4pt] &amp;= \\frac{\\sigma^2}{n} \\;+\\; \\sigma^2\\,\\frac{(x_0-\\bar{x})^2}{S_{xx}}, \\quad cov(\\overline{y},\\hat{\\beta}_1)=0 \\\\[6pt] &amp;= \\sigma^2\\!\\left[\\,\\frac{1}{n} \\;+\\; \\frac{(x_0-\\bar{x})^2}{S_{xx}}\\,\\right]. \\end{align} \\] Luego el estadístico pivote y su distribución \\[ T\\;=\\;\\frac{\\hat y(x_0)-\\mathbb{E}(y\\mid x_0)} {\\sigma\\;\\sqrt{\\dfrac{1}{n}+\\dfrac{(x_0-\\bar x)^2}{S_{xx}}}} \\;\\sim\\; t_{\\,n-2}. \\] Para un IC del \\(100(1-\\alpha)\\%\\) para la media en \\(x_0\\) \\[ \\boxed{\\; \\hat y(x_0)\\ \\pm\\ t_{\\,1-\\alpha/2;\\,n-2}\\; \\sigma\\,\\sqrt{\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{S_{xx}}} \\;} \\] Ancho del IC. El ancho es mínimo cuando \\(x_0=\\bar{x}\\) y crece a medida que \\(|x_0-\\bar x|\\) aumenta, porque hay menos información en los extremos del rango de \\(x\\). 7.10 Intervalo de predicción para una nueva \\(y_0\\) en \\(x_0\\) \\[ \\hat y(x_0)\\ \\pm\\ t_{\\,1-\\alpha/2;\\,n-2}\\; \\sigma\\,\\sqrt{1+\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{S_{xx}}}. \\] Nota importante Intervalos de confianza (IC): Determinan el rango donde se espera que se ubique el valor medio de \\(Y\\) para un valor específico de \\(X\\). Formalmente, estiman \\(\\mathbb{E}(Y \\mid X = x_0)\\), es decir, \\[ \\hat{\\mu}_{\\,\\mid X=x_0} \\approx \\mathbb{E}(Y \\mid X=x_0). \\] Intervalos de predicción (IP): Determinan el rango donde es probable que caiga el próximo valor individual de \\(Y\\) cuando \\(X = x_0\\). Aunque los residuales del modelo ajustado para los datos de soldadura no cumple con el supuesto de varianza constante, se mostrará cómo predecir la Resistencia de una soldadura con Edad=20, es decir, \\(x_0=20\\). Con el objeto modelo, podemos construir el intervalo de confianza del \\(95\\%\\) haciendo Code # intervalo de confianza del 95% para E[Resistencia|Edad = 20] predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 10.86748 10.64805 11.08691 Este resultado implica que, si medimos la Resistencia de varias soldaduras con \\(Edad=20\\), se espera que la Resistencia promedio sea \\(10.87\\) psi, y que, a nivel poblacional, dicho promedio se encuentre en el intervalo \\((10.65,11.09)\\) con una confianza del \\(95\\%\\). Finalmente, el intervalo de predicción cuando \\(Edad=20\\) será: Code # intervalo de confianza del 95% para Resistencia|Edad = 20 predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 10.86748 9.674564 12.06039 Por lo tanto, se espera que en una próxima soldadura de \\(Edad=20\\) el valor de Resistencia sea \\(10.87\\) psi. A nivel poblacional, dicha Resistencia estará en el intervalo \\((9.67,12.06)\\) el \\(95\\%\\) de las veces. Los siguientes ejercicios son obtenidos del libro de (Navidi 2021) Ejercicio 1 El procesamiento de carbón sin tratar implica el “lavado”, en donde se elimina la ceniza de carbón (no orgánico, material incombustible). El artículo “Quantifying Sampling Precision for Coal Ash Using Gy’s Discrete Model of the Fundamental Error” (Journal of Coal Quality, 1989:33-39) proporciona los datos relacionados con los porcentajes de ceniza con el volumen de una partícula de carbón. Se midieron los porcentajes promedio de ceniza para seis volúmenes de partículas de carbón. Los datos son los siguientes: Volumen (cm3) Porcentaje de ceniza 0.01 3.32 0.06 4.05 0.58 5.69 2.24 7.06 15.55 8.17 276.02 9.36 Calcule la recta de mínimos cuadrados para predecir el porcentaje de ceniza (y) a partir del volumen (x). Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Explique. Calcule la recta de mínimos cuadrados para predecir el porcentaje de ceniza a partir del logaritmo natural del volumen. Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Explique. Calcule la recta de mínimos cuadrados para predecir el porcentaje de ceniza a partir de la raíz cuadrada del volumen. Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Explique. Utilizando el modelo más adecuado, pronostique el porcentaje de ceniza para partículas con un volumen de 50 m3. Utilizando el modelo más adecuado, construya un intervalo de confianza de 95% para la media de un porcentaje de ceniza para partículas con un volumen de 50 m3. Ejercicio 2 Para determinar el efecto de la temperatura sobre la producción de cierto proceso químico, el proceso se opera 24 veces en diferentes temperaturas. La temperatura (en °C) y la producción (expresada como un porcentaje de un máximo teórico) para cada operación se muestran a continuación. Los resultados se presentan en el orden en que se operaron, de los primeros a los últimos: Orden Temp (°C) Producción (%) 1 30 49.2 2 32 55.3 3 35 53.4 4 34 59.9 5 31 51.4 6 27 52.1 7 33 60.2 8 34 60.5 9 25 59.3 10 38 64.5 11 39 68.2 12 30 56.0 13 30 58.3 14 39 70.8 15 40 71.6 16 44 73.0 17 34 65.9 18 43 75.2 19 34 69.5 20 41 78.6 21 37 72.0 22 42 80.2 23 41 76.3 24 28 69.5 Calcule la recta de mínimos cuadrados para predecir la producción (y) de temperatura (x). Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Explique. Grafique los residuos contra el orden en el que se hicieron las observaciones. ¿Hay una tendencia en los residuos con el tiempo? ¿El modelo lineal parece adecuado? Explique. Verifique si cumple o no los supuestos de los errores Ejercicio 3 En una roca detonada con explosivos, la velocidad máxima de la partícula (VMP) depende de la distancia de la explosión y de la cantidad de carga. El artículo “Prediction of Particle Velocity Caused by Blasting for an Infrastructure Excavation Covering Granite Bedrock” (A. Kahírman, Mineral Resources Engineering, 2001:205–218) sugiere que predecir la VMP (y) a partir de la distancia escalada (x), la cual se define como la distancia dividida entre la raíz cuadrada de la carga. Los resultados para 15 explosiones se presentan en la tabla siguiente: VMP (mm/s) Distancia escalada (m/kg0.5) 1.4 47.33 15.7 9.6 2.54 15.8 1.14 24.3 0.889 23.0 1.65 16.3 1.4 39.8 1.02 29.94 4.57 10.9 6.6 8.63 1.02 28.64 3.94 18.21 1.4 33.0 1.4 34.0 Grafique la VMP contra la distancia escalada. ¿La relación parece ser lineal? Calcule la recta de mínimos cuadrados para el modelo \\(\\ln(VMP) = \\beta_0 + \\beta_1 \\ln(\\text{distancia escalada}) + \\epsilon\\). Grafique los residuos contra los valores ajustados. ¿Este modelo lineal parece adecuado? Utilice la recta de mínimos cuadrados calculada en el inciso (b) para predecir la VMP cuando la distancia escalada es 20. Determine un intervalo de predicción de 95%. Verifique si cumple o no los supuestos de los errores. Ejercicio 4 La buena previsión y el control de las actividades de preconstrucción conducen al uso más eficiente del tiempo y recursos en proyectos de construcción de autopistas. Los datos acerca de los costos de construcción (en miles de dólares) y las horas-persona de trabajo requeridas para varios proyectos se presentan a continuación y provienen del artículo “Forecasting Engineering Manpower Requirements for Highway Preconstruction Activities” (K. Persad, J. O’Connor, y K. Varghese, Journal of Management Engineering, 1995:41–47). Cada valor representa un promedio de algunos proyectos; se eliminaron dos datos atípicos. Horas-persona (x) Costo (y) (miles de dólares) 939 251 5 796 4 690 289 124 283 294 138 138 2 698 1 385 663 345 1 069 355 6 945 5 253 4 159 1 177 1 266 802 1 481 945 4 716 2 327 Calcule la recta de mínimos cuadrados para predecir y a partir de x. Grafique los residuos contra los valores ajustados. ¿El modelo parece adecuado? Calcule la recta de mínimos cuadrados para predecir ln y a partir de ln x. Grafique los residuos contra los valores ajustados. ¿El modelo parece adecuado? Usando el modelo más adecuado, construya un intervalo de predicción de 95% para el costo de un proyecto que requiere mil horas-persona de trabajo. Ejercicio 5 El artículo “Oxidation State and Activities of Chromium Oxides in CaO–SiO2–CrOx Slag System” (Y. Xiao, L. Holappa y M. Reuter, Metallurgical and Materials Transactions B, 2002:595–603) presenta la cantidad x (en porcentaje molar) y el coeficiente de actividad y de CrO1.5 para varios elementos. Los datos, extraídos de una tabla más grande, se muestran a continuación: x (% mol) y 10.20 2.6 5.03 19.9 8.84 0.8 6.62 5.3 2.89 20.3 2.31 39.4 7.13 5.8 3.40 29.4 5.57 2.2 7.23 5.5 2.12 33.1 1.67 44.2 5.33 13.1 16.70 0.6 9.75 2.2 2.74 16.9 2.58 35.5 1.50 48.0 Calcule la recta de mínimos cuadrados para predecir y a partir de x. Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Calcule la recta de mínimos cuadrados para predecir y a partir de 1/x. Grafique los residuos contra los valores ajustados. Usando la mejor línea ajustada, obtenga un intervalo de confianza del 95% para la media de y cuando x = 5.0. Ejercicio 6 Se utiliza un molino de viento para generar corriente continua. Se reúnen datos en 45 días diferentes para determinar la relación entre la velocidad del viento (en mi/h, x) y la corriente (en kA, y). Los datos se presentan a continuación: Día Velocidad del viento (x) Corriente (y) 1 4.2 1.9 2 1.4 0.7 3 6.6 2.2 4 4.7 2.0 5 2.6 1.1 6 5.8 2.6 7 1.8 0.3 8 5.8 2.3 9 7.3 2.6 10 7.1 2.7 11 6.4 2.4 12 4.6 2.2 13 1.6 1.1 14 2.3 1.5 15 4.2 1.5 16 3.7 2.1 17 5.9 2.2 18 6.0 2.6 19 10.7 3.2 20 5.3 2.3 21 5.1 1.9 22 4.9 2.3 23 8.3 3.1 24 7.1 2.3 25 9.2 2.9 26 4.4 1.8 27 8.0 2.6 28 10.5 3.0 29 5.1 2.1 30 5.8 2.5 31 2.6 1.4 32 7.7 2.8 33 6.1 2.4 34 5.5 2.2 35 4.7 2.3 36 4.0 2.0 37 2.3 1.2 38 11.9 3.0 39 8.6 2.5 40 5.6 2.1 41 4.2 1.7 42 6.2 2.3 43 7.7 2.6 44 6.6 2.9 45 6.9 2.6 Calcule la recta de mínimos cuadrados para predecir y a partir de x. Realice una gráfica de residuos contra valores ajustados. Calcule la recta de mínimos cuadrados para predecir y a partir de ln x. Realice una gráfica de residuos contra valores ajustados. Calcule la recta de mínimos cuadrados para predecir ln y a partir de x. Realice una gráfica de residuos contra valores ajustados. Calcule la recta de mínimos cuadrados para predecir √y a partir de x. Realice una gráfica de residuos contra valores ajustados. ¿Cuál de los cuatro modelos (de a a d) se ajusta mejor? Explique. Para el modelo que ajusta mejor, grafique los residuos contra el orden en el cual se realizaron las observaciones. ¿Los residuos parecen variar con el tiempo? Utilizando el mejor modelo, pronostique la corriente cuando la velocidad del viento sea 5.0 mi/h. Utilizando el mejor modelo, determine un intervalo de predicción de \\(95\\%\\) para la corriente en un día específico cuando la velocidad del viento es 5.0 mi/h. Para el mejor modelo, verifique si cumple o no los supuestos de los errores. Ejercicio 7 Dos detectores de radón se colocan en ubicaciones diferentes en el sótano de una casa. Cada uno proporcionó mediciones horarias de la concentración de radón, en unidades de pCi/L. Los datos se presentan como R1 y R2 en la tabla siguiente. R1 R2 1.2 1.2 1.3 1.5 1.3 1.6 1.3 1.7 1.5 1.7 1.5 1.7 1.6 1.8 2.0 1.8 2.0 1.9 2.4 1.9 2.9 1.9 3.0 2.0 2.0 4.0 2.0 4.0 2.1 4.3 2.1 4.3 2.2 4.4 2.2 4.4 2.3 4.7 2.3 4.7 2.4 4.8 2.4 4.8 2.4 4.9 2.4 5.4 2.6 5.5 2.7 5.8 2.7 5.9 2.8 6.0 2.9 6.0 3.0 6.1 3.1 6.2 3.2 6.5 3.2 6.6 3.5 6.9 3.5 7.0 3.5 7.0 Calcule la recta de mínimos cuadrados para predecir la concentración de radón en la posición 2 a partir de la concentración en la posición 1. Grafique los residuos contra los valores ajustados. ¿El modelo lineal parece adecuado? Divida los datos en dos grupos: los puntos donde R1 &lt; 4 en un grupo y donde R1 ≥ 4 en el otro. Calcule la recta de mínimos cuadrados y la gráfica de residuos para cada grupo. ¿La recta describe bien algún grupo? ¿Cuál? Explique por qué podría ser una buena idea ajustar un modelo lineal a una parte de estos datos y un modelo no lineal a la otra. Ejercicio 8 El artículo “The Equilibrium Partitioning of Titanium Between Ti3+ and Ti4+ Valency States in CaO–SiO2–TiO2 Slags” (G. Tranell, O. Ostrovski y S. Jahanshahi, Metallurgical and Materials Transactions B, 2002:61–66) analiza la relación entre el cociente redox Ti3+/Ti4+ (y) y la presión parcial de oxígeno pO2 (x) en CaO–SiO2–TiO2. Se realizaron mediciones independientes del cociente redox en cinco presiones parciales distintas. Presión parcial de O2 (x) Cociente redox (y) 10-7 0.011 10-7 0.017 10-7 0.034 10-7 0.039 10-8 0.018 10-8 0.011 10-8 0.026 10-8 0.050 10-8 0.034 10-8 0.068 10-8 0.061 10-9 0.027 10-9 0.038 10-9 0.076 10-9 0.088 10-10 0.047 10-10 0.069 10-10 0.123 10-10 0.116 10-10 0.122 10-12 0.160 10-12 0.220 10-12 0.399 10-12 0.469 La teoría sugiere que y es proporcional a \\(x^{\\beta}\\) para algún \\(\\beta\\). Exprese esta relación como un modelo lineal y especifique las variables transformadas. Calcule la recta de mínimos cuadrados para ese modelo lineal. Grafique los residuos contra los valores ajustados. ¿El modelo lineal es válido? Teóricamente, bajo las condiciones del experimento, y debería ser proporcional a \\(x^{-1/4}\\). ¿Los datos son consistentes con esta teoría? Explique. Ejercicio 9 El artículo “The Selection of Yeast Strains for the Production of Premium Quality South African Brandy Base Products” (C. Steger y M. Lambrechts, Journal of Industrial Microbiology and Biotechnology, 2000:431–440) presenta información detallada sobre la composición compuesta volátil de la base de vino realizada para cada una de 16 variedades de levadura seleccionadas. A continuación se muestran las concentraciones de ésteres totales y ácidos volátiles totales (en mg/L) en cada uno de los vinos: Ésteres (x) Ácidos (y) 284.34 445.70 215.34 332.59 139.38 356.98 658.38 192.59 173.01 265.43 188.72 166.73 197.81 291.72 105.14 412.42 229.55 210.58 144.39 254.82 238.33 215.83 295.24 442.57 312.95 203.62 172.79 342.21 256.02 152.38 170.41 391.30 Construya un diagrama de dispersión de la concentración de ácido (y) contra la concentración de ésteres (x). Indique el dato atípico. Calcule los coeficientes de la recta de mínimos cuadrados para predecir el nivel de ácido a partir del nivel de ésteres, junto con sus desviaciones estándar estimadas. Calcule el valor P de la prueba de hipótesis nula \\(H_0: \\beta_1 = 0\\). Elimine el dato atípico y calcule nuevamente los coeficientes de la recta de mínimos cuadrados, junto con sus desviaciones estándar estimadas. Calcule el valor P de la prueba de hipótesis nula \\(H_0: \\beta_1 = 0\\) para los datos con el dato atípico eliminado. ¿Un modelo lineal parece útil para predecir la concentración de ácido a partir de la concentración de ésteres? Explique. Ejercicio 10 El artículo “Mathematical Modeling of the Argon–Oxygen Decarburization Refining Process of Stainless Steel: Part II. Application of the Model to Industrial Practice” (J. Wei y D. Zhu, Metallurgical and Materials Transactions B, 2001:212–217) presenta el contenido de carbono (en % de masa) y la temperatura del baño (en K) para 32 calores de acero inoxidable austenítico. Los datos se muestran en la siguiente tabla: % de carbono (x) Temperatura (y) % de carbono (x) Temperatura (y) % de carbono (x) Temperatura (y) 19 1975 17 1984 18 1962 23 1947 19 1963 17 1963 22 1954 21 1965 21 1972 16 1992 22 1963 18 1986 17 1975 22 1960 22 1954 18 1971 19 1960 21 1990 12 2046 19 1953 15 1989 24 1945 Calcule la recta de mínimos cuadrados para predecir la temperatura del baño (y) a partir del contenido de carbono (x). Identifique los datos atípicos. Calcule las dos rectas de mínimos cuadrados resultantes de eliminar cada dato atípico por separado y la recta de mínimos cuadrados que se obtiene al eliminar ambos. ¿Las rectas de mínimos cuadrados calculadas en los incisos (a) y (b) son similares? Si es así, reporte la recta que ajustó a todo el conjunto de datos, junto con los intervalos de confianza de 95% para la pendiente y el intercepto. Si no, informe el rango de pendientes, sin un intervalo de confianza. Ejercicio 11 La ley de Beer–Lambert relaciona la absorción \\(A\\) de una solución con la concentración \\(C\\) de una especie en solución por \\(A = M\\,L\\,C\\), donde \\(L\\) es la longitud de la trayectoria y \\(M\\) el coeficiente de absorción molar. Suponga que \\(L = 1\\ \\text{cm}\\). Se realizan mediciones de \\(A\\) a diferentes concentraciones. Los datos se presentan en la tabla siguiente: Concentración \\(C\\) \\(\\left(\\text{mol}/\\text{cm}^3\\right)\\) Absorción \\(A\\) 1.00 0.99 1.20 1.13 1.50 1.52 1.70 1.73 2.00 1.96 Sea \\(A = \\beta_0 + \\beta_1 C\\) la ecuación de la recta de mínimos cuadrados para predecir la absorción \\(A\\) a partir de la concentración \\(C\\). Calcule los valores de \\(\\hat\\beta_0\\) y \\(\\hat\\beta_1\\). ¿Qué valor le asigna la ley de Beer–Lambert a \\(\\beta_0\\)? ¿Cuál es la cantidad física que estima \\(\\beta_1\\)? Pruebe \\(H_0:\\ \\beta_0=0\\). ¿Este resultado es consistente con la ley de Beer–Lambert? Ejercicio 12 El artículo “Effect of Temperature on the Marine Immersion Corrosion of Carbon Steels” (R. Melchers, Corrosion, 2002:768–781) presenta mediciones de pérdida por corrosión (en mm) que soportan elementos de acero y de cobre sumergidos en agua de mar en 14 ubicaciones diferentes. Para cada ubicación se registró la media de la pérdida por corrosión (\\(y\\), en mm) junto con la media de la temperatura del agua (\\(x\\), en °C). Los resultados, después de un año de inmersión, se presentan en la siguiente tabla: Corrosión (mm) Media de la temperatura (°C) Corrosión (mm) Media de la temperatura (°C) 0.2655 23.5 0.2200 26.5 0.1680 18.5 0.0845 15.0 0.1130 23.5 0.1860 18.0 0.1060 14.0 0.1075 17.5 0.2390 17.5 0.1295 11.0 0.1410 20.0 0.0900 11.0 0.3505 26.0 0.2515 13.5 Calcule la recta de mínimos cuadrados para predecir la pérdida por corrosión (\\(y\\)) a partir de la media de la temperatura (\\(x\\)). Determine un intervalo de confianza de 95% para la pendiente \\(\\beta_1\\). Determine un intervalo de confianza de 95% para la media de la pérdida de corrosión a una media de temperatura de \\(20^\\circ\\text{C}\\). Determine un intervalo de predicción de 90% para la pérdida por corrosión de un elemento inmerso a una media de temperatura de \\(20^\\circ\\text{C}\\). Ejercicio 13 La supervisión de la producción de una reacción química dada a diferentes temperaturas del recipiente de reacción produce los resultados que se muestran en la siguiente tabla: Temperatura (°C) Producción (%) Temperatura (°C) Producción (%) 150 77.4 250 88.9 150 76.7 250 89.2 150 78.2 300 94.7 200 84.1 300 94.8 200 84.5 300 94.7 200 83.7 300 95.9 Determine los estimadores de mínimos cuadrados para \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma^2\\) para el modelo lineal simple \\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\). ¿Puede concluir que \\(\\beta_0 \\neq 0\\)? ¿Puede concluir que \\(\\beta_1 \\neq 0\\)? Realice una gráfica de residuos. ¿El modelo lineal parece adecuado? Determine un intervalo de confianza de 95% para la pendiente \\(\\beta_1\\). Determine un intervalo de confianza de 95% para la media de la producción a \\(225^\\circ \\text{C}\\). Determine un intervalo de predicción de 95% para una producción a \\(225^\\circ \\text{C}\\). Referencias Navidi, William. 2021. Estadística Para Ingenieros y Científicos. 7ma ed. Ciudad de México, México: McGraw-Hill. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
