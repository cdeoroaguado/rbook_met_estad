[["index.html", "Notas de métodos estadísticos Asignatura", " Notas de métodos estadísticos Carlos de Oro Aguado 2025-08-04 Asignatura La asignatura de Métodos Estadísticos tiene como propósito desarrollar en el estudiante competencias analíticas rigurosas para la comprensión, modelación e interpretación de datos cuantitativos, con base en herramientas estadísticas fundamentales orientadas a la ciencia de datos. A lo largo del curso se emplean enfoques descriptivos, inferenciales y multivariados, complementados con técnicas robustas no paramétricas, con énfasis en aplicaciones reales y toma de decisiones basadas en evidencia. Durante el primer corte, el enfoque se centra en el análisis exploratorio de datos y diagnóstico inferencial, abordando la caracterización de variables mediante medidas de tendencia central, dispersión y forma. Se introducen técnicas de análisis de correlación (coeficientes de Pearson, Spearman y Kendall) como base para evaluar relaciones lineales o no lineales entre variables. Además, se presenta la Regresión Lineal Simple (RLS), incluyendo la identificación de términos básicos, formulación del modelo, interpretación de coeficientes y evaluación de la bondad de ajuste. Se discuten los supuestos del modelo (normalidad, homocedasticidad, independencia y linealidad), y se introducen herramientas para el análisis gráfico de residuos. En el segundo corte, se profundiza en la modelación con RLS, enfocándose en sus aplicaciones prácticas, validación estadística e inferencia sobre los parámetros. Se abordan temas como la construcción de intervalos de confianza, contrastes de hipótesis para los coeficientes de regresión, análisis de significancia del modelo y predicciones puntuales e intervalares. Se refuerzan conceptos clave de validación, tales como el análisis de residuos, influencia y multicolinealidad. Asimismo, se inicia la transición hacia modelos más complejos, preparando el terreno para el análisis multivariado. El tercer corte está dedicado a la Regresión Lineal Múltiple (RLM), donde se modela la respuesta de una variable dependiente a partir de múltiples variables explicativas. Se analizan sus fundamentos matemáticos, estimación por mínimos cuadrados, interpretación de coeficientes parciales y evaluación de supuestos del modelo. Se incluye inferencia sobre parámetros, prueba global de significancia, análisis de varianza (ANOVA), selección de variables y medidas de ajuste como \\(R^2\\) ajustado y el criterio de información de Akaike (AIC). Finalmente, se ofrece una introducción a otros modelos de regresión (como regresión polinómica o con transformaciones) y a los principios básicos del análisis multivariado, orientado a estudiar estructuras de dependencia entre múltiples variables simultáneamente. En conjunto, este curso constituye un pilar fundamental para el desarrollo de habilidades analíticas aplicadas a la ciencia de datos, y prepara al estudiante para abordar con solvencia asignaturas posteriores como inferencia estadística, teoría de la probabilidad, aprendizaje estadístico y análisis multivariante avanzado. "],["intro.html", "Capítulo 1 Introducción 1.1 Introducción a R y RStudio 1.2 Introducción a Git y GitHub 1.3 Introducción a Tidyverse 1.4 Pasos para publicar un libro bookdown en GitHub Pages", " Capítulo 1 Introducción 1.1 Introducción a R y RStudio El primer paso para comenzar a trabajar con R, un lenguaje de programación especializado en estadística, ciencia de datos y visualización, es instalarlo en tu computadora. R es compatible con los principales sistemas operativos, incluyendo Windows, macOS y Linux. 1.1.1 ¿Qué es R? R es un lenguaje de programación y un entorno de software libre dedicado al análisis estadístico y la generación de gráficos. Su potencia radica en una gran variedad de paquetes estadísticos, su comunidad activa y su capacidad de integración con otras herramientas como Python, SQL, y plataformas de visualización como Power BI o Tableau. A continuación, se indican los enlaces oficiales para descargar (paso a paso) R: Descargar R Página oficial del proyecto R (https://cran.r-project.org): Figura 1.1: Software R En esta página puedes seleccionar tu sistema operativo: Para Windows: https://cran.r-project.org/bin/windows/base/ Para macOS: https://cran.r-project.org/bin/macosx/ Para Linux: https://cran.r-project.org/bin/linux/ Se realizará el proceso para la instalación de R para el sistema operativo Windows Figura 1.2: Paso a paso de la instalación de R (izquierda a derecha) 1.1.2 ¿Qué es RStudio? RStudio es un entorno de desarrollo integrado (IDE) que proporciona una interfaz gráfica intuitiva y muy funcional para trabajar con R. Entre sus características destacan: Editor de scripts con resaltado de sintaxis. Consola interactiva para ejecutar comandos. Panel de visualización de datos y objetos en memoria. Gráficos integrados. Soporte para proyectos y versiones de R. Integración con Git, Markdown, Quarto y Shiny. Aunque se puede usar R sin RStudio, la mayoría de los usuarios prefieren trabajar dentro de este entorno por su productividad, organización y facilidad de uso. A continuación, se indican los enlaces oficiales para descargar (paso a paso) RStudio: Descargar RStudio Página oficial de RStudio (ahora llamado Posit, https://posit.co/download/rstudio-desktop/): Figura 1.3: Desarrollo de entorno integrado (IDE) RStudio Selecciona la versión gratuita de RStudio Desktop y descarga el instalador correspondiente a tu sistema operativo. Se realizará el proceso para la instalación de RStudio para el sistema operativo Windows: Figura 1.4: Paso a paso de la instalación de RStudio Una vez finalizada la instalación, puedes iniciar RStudio desde el acceso directo en tu escritorio o buscándolo en el menú de inicio. Figura 1.5: Visualización de RStudio Al abrir RStudio por primera vez, se presenta un entorno dividido en cuatro paneles: Script o editor de código (arriba a la izquierda): donde se escriben los scripts .R o .Rmd. Consola (abajo a la izquierda): donde se ejecutan los comandos directamente. Entorno / Historial (arriba a la derecha): muestra los objetos cargados y el historial de comandos. Archivos, gráficos, paquetes, ayuda y visor (abajo a la derecha): herramientas auxiliares para explorar y trabajar eficientemente. Puedes verificar que R y RStudio están funcionando correctamente ejecutando una operación simple en la consola, como: Code 2 + 3 1.2 Introducción a Git y GitHub 1.2.1 ¿Qué es Git? Git es un sistema de control de versiones distribuido que permite gestionar y registrar los cambios realizados en archivos de un proyecto a lo largo del tiempo. Fue creado por Linus Torvalds y se ha convertido en el estándar para el desarrollo de software y proyectos colaborativos. El enlace es https://git-scm.com/. Figura 1.6: Sotfware Git Ventajas principales de Git Permite llevar un historial detallado de versiones. Facilita la colaboración en proyectos con múltiples personas. Permite trabajar en ramas (branches) para desarrollar funcionalidades de forma aislada. No depende de internet para el trabajo local. 1.2.1.1 Pasos para instalar Git Daremos una guía para la instalación de Git usando Windows (paso a paso): Instalar Git:  https://git-scm.com/downloads Ahora, debes realizar lo siguiente: Figura 1.7: Instalación de Git A continuación, comprobemos la instalación de Git. Figura 1.8: Verificación de Git 1.2.2 ¿Qué es GitHub? GitHub es una plataforma en línea que permite alojar repositorios de Git en la nube. Es ideal para compartir proyectos, colaborar en equipo y automatizar flujos de trabajo. Este es el enlace https://github.com. Figura 1.9: Plataforma GitHub Funciones principales de GitHub Crear y administrar repositorios públicos o privados. Gestionar cambios mediante pull requests. Seguir errores o tareas usando issues. Crear documentación, páginas web y wikis para los proyectos. Automatizar procesos con GitHub Actions. 1.2.2.1 Pasos para instalar GitHub A continuación, se presenta una guía paso a paso para la instalación de GitHub en Windows: Crear una cuenta en GitHub:  https://github.com Ahora, sigue las imagenes: Figura 1.10: Registro en GitHub Despues haces el proceso de verificación Completa el captcha de seguridad. GitHub puede pedirte que verifiques tu correo electrónico. Revisa tu bandeja de entrada y haz clic en el enlace de confirmación. Ingresas al enlace https://github.com y despues: Figura 1.11: Credenciales de verificación en GitHub Al ingresar, este sería el inicio Figura 1.12: Dashboard de GitHub A continuación, vamos a descargar GitHub Desktop para Windows; para ello debes usar el enlace https://desktop.github.com/download/ Figura 1.13: Credenciales de verificación en GitHub Diferencias claves entre Git y GitHub Git GitHub Herramienta local Plataforma en la nube Administra versiones Aloja y comparte repositorios No requiere internet Requiere conexión para sincronizar Se usa desde terminal o IDE Se accede por navegador o API 1.2.2.2 ¿Cómo se relacionan? Git administra tu proyecto localmente, guardando versiones y cambios. GitHub actúa como repositorio remoto, permitiendo subir (push) o descargar (pull) cambios desde y hacia otros colaboradores. Juntos permiten trabajar de forma segura, organizada y colaborativa desde distintos lugares. 1.3 Introducción a Tidyverse El Tidyverse es un conjunto de paquetes integrados para el lenguaje de programación R, diseñados con el objetivo de facilitar el análisis de datos de manera estructurada, legible y eficiente. Su filosofía se basa en el concepto de “datos ordenados” (tidy data), donde cada variable es una columna, cada observación una fila, y cada tipo de unidad observacional forma una tabla. Estos paquetes comparten principios de diseño comunes y una gramática coherente, lo que permite a los usuarios aprender un conjunto de reglas aplicables en todo el ecosistema, aumentando así la productividad y la claridad del código. 1.3.1 Representación visual del ecosistema Tidyverse Figura 1.14: Paquetes de tidyverse 1.3.2 Principales paquetes del Tidyverse A continuación se describen los paquetes más representativos que conforman el núcleo del Tidyverse:  ggplot2 Permite la creación de visualizaciones estadísticas sofisticadas mediante la Gramática de los Gráficos (Grammar of Graphics). Con ggplot2 puedes combinar capas de datos, geometrías, escalas y temas para construir gráficos informativos y estéticamente agradables. Es ampliamente utilizado en análisis exploratorio y comunicación de resultados.  dplyr Facilita la manipulación de datos mediante verbos intuitivos como: filter() para filtrar filas según condiciones lógicas, select() para elegir columnas, mutate() para crear o transformar variables, summarise() para resumir valores, group_by() para operaciones agrupadas. Trabaja perfectamente con tibble y se puede aplicar a bases de datos con dbplyr.  tidyr Su propósito es convertir datos desorganizados en un formato “ordenado”. Algunas de sus funciones clave incluyen: pivot_longer() y pivot_wider() para cambiar el formato de los datos. separate() para dividir una columna en varias. unite() para fusionar columnas en una sola. Estas funciones son fundamentales para preparar datos antes del análisis.  readr Ofrece funciones rápidas y confiables para importar archivos .csv, .tsv, .fwf, entre otros. Es más eficiente que read.table() y permite: Importar con read_csv(), read_delim() o read_fwf(). Detectar problemas de formato con problems(). Controlar tipos de datos con argumentos explícitos.  stringr Simplifica el trabajo con cadenas de texto (caracteres). Proporciona una sintaxis coherente para tareas comunes como: Búsqueda de patrones (str_detect, str_subset), Manipulación (str_replace, str_sub, str_trim), Conteo (str_count), Extracción (str_extract, str_match). Ideal para limpiar y procesar textos en análisis de encuestas, redes sociales o nombres de variables.  forcats Dedicado al tratamiento de factores en R, un tipo especial de variable categórica. forcats ofrece funciones para: Reordenar niveles según frecuencia (fct_infreq()), Fusionar niveles (fct_collapse()), Recodificar (fct_recode()), Ordenar según otras variables (fct_reorder()). Útil en análisis estadísticos donde el tratamiento adecuado de las categorías es clave.  lubridate Permite el manejo sencillo de fechas y horas. Resuelve uno de los aspectos más complejos en análisis temporal con funciones como: ymd(), dmy(), mdy() para convertir cadenas en fechas, hour(), minute(), second() para extraer componentes, interval(), duration() y period() para trabajar con intervalos de tiempo.  haven Permite importar y exportar archivos desde software estadístico como: SPSS (.sav, .zsav), Stata (.dta), SAS (.sas7bdat, .xpt). Esencial para interoperabilidad con datos institucionales y académicos que provienen de otras plataformas.  readxl Importa archivos de Excel (.xls, .xlsx) sin necesidad de tener Excel instalado. Las funciones clave incluyen: read_excel() para leer hojas completas, excel_sheets() para listar las hojas de un archivo. Ideal para usuarios que reciben datos administrativos, contables o estadísticos en formato Excel.  purrr Introduce herramientas de programación funcional en R, permitiendo aplicar funciones a listas o vectores con más control que lapply() o sapply(). Permite: Iterar con map(), map_df(), map_dbl(), etc. Trabajar con errores mediante safely(), possibly(). Manipular listas de manera estructurada. Ideal para automatizar tareas repetitivas.  tibble Es una versión mejorada de los data frames tradicionales. Entre sus ventajas: Muestra solo las primeras 10 filas y columnas visibles. No modifica automáticamente los nombres de columnas. Trabaja mejor con los flujos de trabajo del Tidyverse. 1.3.3 Instalación del Tidyverse en R Para instalar todos los paquetes del núcleo del Tidyverse, simplemente se ejecuta en la consola de R: Code install.packages(&quot;tidyverse&quot;) 1.4 Pasos para publicar un libro bookdown en GitHub Pages Para publicar un libro bookdown en GitHub Pages debes tener lo siguiente: Pre-requisitos Tener una cuenta en GitHub. Haber creado un repositorio público (ej. metodos_estadisticos). Tener Git instalado y configurado. Tener R y RStudio instalado y configurado Instalar el siguiente paquete Code install.packages(&quot;bookdown&quot;) Tener un proyecto bookdown funcional en tu computadora. Abre el archivo _bookdown.yml y asegúrate de incluir: book_filename: &quot;index&quot; output_dir: &quot;docs&quot; Esto hace que el libro se renderice en la carpeta docs, que es donde GitHub Pages busca el sitio por defecto. En RStudio o consola de R, corre: Code bookdown::render_book(&quot;index.Rmd&quot;) Abre la terminal en la carpeta del libro y ejecuta: Code git init git remote add origin https://github.com/cdeoroaguado/metodos_estadistico.git git add docs git commit -m &quot;primer despliegue del libro&quot; git push -u origin main Activa GitHub Pages Ve al repositorio en GitHub. Haz clic en Settings &gt; Pages. En “Source”, selecciona: Branch: main o master Folder: /docs Guarda los cambios. Después de unos segundos, tu libro estará disponible en: Enlace del texto Dale click en este enlace:  https://cdeoroaguado.github.io/metodos_estadistico/ 1.4.1 Video paso a paso de la publicación del libro en GitHub Pages Uno de los pasos más importantes al desarrollar un libro con bookdown es su publicación en línea, permitiendo el acceso abierto y permanente al contenido. Para ello, GitHub Pages se convierte en una herramienta ideal por su facilidad de uso y compatibilidad con proyectos de R. A continuación, se presenta un video tutorial donde se explican paso a paso los procedimientos necesarios para publicar correctamente un libro elaborado en bookdown a través de un repositorio en GitHub: Cabe resaltar que este libro de muestra estamos utilizando el paquete bookdown (Xie 2025), el cual fue construido sobre R Markdown y knitr (Xie 2015) Referencias Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2025. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "],["reglas.html", "Capítulo 2 Reglas de curso y diagnostico 2.1 Reglas de las clases 2.2 Política de evaluación y normas de integridad académica 2.3 Fechas de taller y examenes 2.4 Programación del curso por unidad 2.5 Referencias 2.6 Terminos básicos de la estadística 2.7 Diagnostico de estadística básica", " Capítulo 2 Reglas de curso y diagnostico Para asegurar que nuestro espacio de aprendizaje sea un ambiente productivo, respetuoso y justo para cada uno de ustedes, es fundamental que sigamos algunas pautas de convivencia y evaluación. Estas reglas están diseñadas para fomentar la concentración, facilitar la participación y garantizar la integridad académica en todo momento. 2.1 Reglas de las clases Durante la clase Respeto durante explicaciones. Evitar el uso de celulares cuando no sea requerido. Evite conversaciones que puedan irrumpir con el desarrollo de las clases. Si tiene alguna duda, pregunte todo lo que necesite. Durante los exámenes o evaluaciones No se admite ningún tipo de pregunta relacionada con las temáticas vistas. Solo se responderán preguntas para aclaraciones relacionadas con redacción o instrucciones. No se permiten dispositivos electrónicos. 2.2 Política de evaluación y normas de integridad académica Normas de evaluación y sanciones Todo fraude o intento de fraude, en cualquier tipo de evaluación, acarreará al estudiante una calificación de 0.0, sin perjuicio de las acciones disciplinarias a que hubiere lugar Se entiende por FRAUDE ACADÉMICO, cualquier comportamiento o práctica ilícita, empleada para obtener una nota o alcanzar un objetivo en el desarrollo de una actividad académica, que vaya en contra de las normas, reglamentos y procesos pedagógicos que la Institución establece y que atenta contra la integridad intelectual y moral del estudiante. Las evaluaciones parciales o finales no presentadas serán calificadas con cero (0.0). Esta calificación solo podrá ser modificada por la calificación obtenida en una evaluación supletoria. Fuente: Reglamento Estudiantil - Universidad del Norte Inasistencia En toda clase se tomará asistencia. Si las faltas exceden el 25% del total de las clases, el estudiante perderá el derecho del examen final, el cual será calificado en 0,0. Fuente: Reglamento Estudiantil - Universidad del Norte Inasistencias Cuando un estudiante no pueda presentar exámenes o compromisos académicos en la fecha programada por motivos de fuerza mayor, podrá solicitar una evaluación supletoria al profesor de la asignatura, quien tendrá la facultad de autorizar la realización de la misma. La evaluación supletoria se realizará dentro de los diez (10) días hábiles siguientes a la fecha inicialmente programada y es indispensable que el estudiante realice el trámite correspondiente en el sistema de información académico donde será aprobado y fijada la fecha de su realización. Fuente: Reglamento Estudiantil - Universidad del Norte 2.3 Fechas de taller y examenes Estas son las fecha de los taller y los examenes: Fechas importantes Tabla 2.1: Fechas de cortes y distribución de porcentajes Corte Porcentaje Evaluación Semana Primer Corte 25% 5% Actividades Hasta la semana 5 20% Parcial Semana 5 Segundo Corte 25% 5% Actividades Hasta la semana 9 20% Parcial Semana 9 Tercer Corte 25% 5% Actividades Hasta la semana 13 20% Parcial Semana 13 Cuarto Corte 25% 5% Actividades Hasta la semana 16 20% Parcial Fecha de registro Otras fechas para tener en cuenta: \\(6-12\\) de octubre. Semana de receso Fecha limite de retiro: 5 de noviembre 2.4 Programación del curso por unidad Este el contenido del curso de métodos estadísticos: Contenido por corte I corte. Diagnósticos Inferencial Análisis de correlación y RLS: introducción Términos básicos II corte. Análisis de correlación y RLS Modelación y aplicaciones Supuestos e inferencia sobre los parámetros III corte. RLM: Matemáticas y aplicaciones Supuestos e inferencia sobre los parámetros Temas adicionales Otros modelos de regresión Introducción al análisis multivariado 2.5 Referencias Estas son algunas referencias para la clase de métodos Referencias bibliográficas Tabla 2.2: Referencias bibliográficas del curso Referencia Bibliográfica Tipo de referencia Guía De Referencia Idioma Existe en Biblioteca Probabilidad Y Estadística Para Ingeniería Y Ciencias, 2015, CENGAGE Learning. Libro impreso x Español Sí Estadística descriptiva y distribuciones de probabilidad, Ediciones Uninorte 2015. ISBN: 9587419154. Libro impreso x Español Sí Estadística inferencial, Ediciones Uninorte 2020. ISBN: 9587419154. Libro impreso x Español Sí Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning, 2017, STHDA. Libro impreso x Inglés Sí Practical Guide To Principal Component Methods in R: PCA, M(CA), FAMD, MFA, HCPC, factoextra, 2017, STHDA. Libro impreso x Inglés Sí Hands-On Time Series Analysis with R: Perform time series analysis and forecasting using R, 2019, Packt Publishing Ltd. Libro impreso x Inglés Sí 2.6 Terminos básicos de la estadística Definición: Unidad experimental. Es el objeto o entidad sobre la cual se realiza una observación o medición dentro de un estudio estadístico. Ejemplo En un estudio clínico, cada paciente corresponde a una unidad experimental. Definición: Población, muestra, censo. La población es el conjunto completo de unidades experimentales de interés La muestra es un subconjunto de unidades experimentales o de una población. Debe ser representativa y aleatoria. El censo es la enumeración completa de las unidades experimentales de una población. Ejemplo La colección de promedios por carreras de la Universidad del Norte (PPCUN) puede servir como una población estadística, y cualquier subcolección, digamos los PPCUN de la carrera de medicina, puede servir como una muestra de esa población. Definición: Variable aleatoria, dato y observación La variable es la característica que se desea medir u observar. El dato: es el valor específico que toma una variable para una unidad experimental. La observación es el conjunto de datos registrados para una unidad experimental. Ejemplo Una universidad está realizando un estudio para conocer el perfil de sus estudiantes de primer semestre. Para ello, se recolecta información de cada estudiante, como su edad, programa académico y puntaje obtenido en las pruebas de admisión. Para el estudiante Laura Torres, se registraron los siguientes valores: Edad: 18 años Programa académico: Psicología Puntaje de admisión: 92 Solución Variable: Edad, programa académico, puntaje de admisión. Dato: 18 años es el dato para la variable edad, Psicología para programa académico”, 92 para “Puntaje de admisión”. Observación: Edad = 18, Programa = Psicología, Puntaje = 92 Definición: Parametro, estadístico. Un parámetro es cualquier caractéristica numérica de una población. Un estadístico es cualquier caractéristica numérica de una muestra. Ejemplo Una universidad quiere conocer el promedio de edad de todos sus estudiantes de primer semestre. Como sería costoso y demorado consultar a toda la población, se toma una muestra aleatoria de 100 estudiantes y se calcula su promedio de edad. Identifique el paramétro y el estadístico. Solución Parametro: El promedio real de edad de todos los estudiantes de primer semestre de la universidad. Estadístico: El promedio de edad calculado con los 100 estudiantes seleccionados en la muestra. Definición: Estadística descriptiva e inferencial. Estadística descriptiva: Parte de la estadística que organiza, resume y presenta los datos de forma útil. Estadística inferencial: Parte de la estadística que permite inferir propiedades poblacionales a partir de una muestra, sin necesidad de censar toda la población. 2.7 Diagnostico de estadística básica Se realizará un diagnoticos de conceptos básicos Ejercicio En una muestra de pacientes, el número de varones dividido entre el total de pacientes es: ¿Cuál de las siguientes medidas define mejor la tendencia central de los datos: 5, 4, 42, 4, 6? Estadística inferencial es la rama de la estadística que se encarga de: Describir la información obtenida del estudio de una muestra para hacer inferencias sobre la población Describir un conjunto de datos recopilados en una muestra Diseñar científicamente un estudio con el objetivo de tomar decisiones respecto a un problema Estudiar el conjunto de individuos que poseen la característica sujeta a estudio La distribución normal:: Es simétrica Es una distribución de probabilidad de variable discreta Es asimétrica La mediana no coincide con la moda Es bimodal El intervalo [media muestral ± 1,96 EEM (error estándar de la media)]:: No dice gran cosa Comprende un 95% de las veces a la media poblacional Comprende un 99% de las veces a la media poblacional Da una seguridad del 68% Da una seguridad del 5% El intervalo [media muestral ± 1,96 EEM (error estándar de la media)]:: No dice gran cosa Comprende un 95% de las veces a la media poblacional Comprende un 99% de las veces a la media poblacional Da una seguridad del 68% Da una seguridad del 5% Un estimador es: Un parámetro que se utiliza para estimar los estadísticos Un estadístico que se utiliza para estimar los parámetros de la muestra Un estadístico que se utiliza para estimar parámetros poblacionales Un parámetro que se utiliza para estimar algunos estadísticos Son todas falsas Los grados de libertad de una tabla de contingencia (independencia) \\(2\\times 2\\) son: 1 2 3 4 5 La estimación de medias se hace con: La distribución normal El error estándar La distribución \\(t\\) de Student La distribución binomial Ciertas i, ii y iii La estimación de porcentajes se hace con (Señalar lo falso): La distribución normal La distribución t de Student La distribución binomial El porcentaje muestral El error estándar del porcentaje Se desea comparar la talla media entre hombres y mujeres. ¿Cuál será la prueba estadística más apropiada? F de Snedecor Chi-cuadrado t de Student Coeficiente de correlación de Pearson Ninguna de las anteriores Si el contraste \\(H_0: \\mu = 50\\) , \\(H_1: \\mu &lt; 50\\) no se rechaza con un \\(p-valor = 0,12\\), el contraste \\(H_0: \\mu \\geq 30\\) , \\(H_1: \\mu &lt; 30\\): Se rechaza \\(H_0\\) Contraste unilateral cola a la izquierda \\(p-valor = 0.12\\) \\(p-valor = 0.88\\) Ciertas 2 y 4 Si el contraste \\(H_0: \\mu = 100\\) , \\(H_1: μ &gt; 100\\) se acepta con un \\(p-valor = 0,10\\), el contraste \\(H_0: \\mu \\leq 100\\) , \\(H_1: \\mu &gt; 100\\) tiene: \\(p-valor = 0.10\\) Contraste unilateral cola a la derecha \\(p-valor = 0.05\\) \\(p-valor = 0.90\\) Ciertas 2 y 3 "],["eda.html", "Capítulo 3 Análisis exploratorio de datos (EDA) 3.1 ¿Cuándo debo utilizarlo? 3.2 Representación de datos según su naturaleza 3.3 Presentación y análisis de la información en estudios descriptivos 3.4 Análisis explotario con variable respuesta numérica 3.5 Análisis explotario con variable respuesta categórica 3.6 Análisis de estadística paramétrica y no paramétrica 3.7 ANOVA (análisis de varianza para comparar múltiples medias)", " Capítulo 3 Análisis exploratorio de datos (EDA) Figura 3.1: Análisis exploratorio de datos (EDA) El Análisis Exploratorio de Datos (Exploratory Data Analysis, EDA por sus siglas en inglés) es una etapa fundamental del proceso estadístico que consiste en utilizar gráficos, visualizaciones y resúmenes numéricos para examinar un conjunto de datos. Su propósito principal es explorar, descubrir patrones, identificar anomalías y formular posibles hipótesis, sin realizar inferencias estadísticas formales. El EDA busca comprender la estructura de los datos y generar ideas, más que confirmar hipótesis previamente establecidas. 3.1 ¿Cuándo debo utilizarlo? El Análisis Exploratorio de Datos (EDA) es una herramienta poderosa para examinar, comprender y preparar un conjunto de datos. Aunque el análisis esté orientado a hipótesis específicas, el EDA es útil en etapas previas como la limpieza de datos, la detección de errores, el análisis de subgrupos o simplemente para obtener una mejor comprensión de la estructura y comportamiento de los datos. Uno de los pasos iniciales más importantes del EDA es la representación gráfica de los datos, que permite identificar patrones, tendencias y valores atípicos con mayor claridad. 3.1.1 Tipos de análisis exploratorio: No gráfico: Utiliza estadísticas descriptivas para resumir las variables numéricamente. Gráfico: Representa visualmente los datos mediante histogramas, diagramas de caja, gráficos de dispersión, entre otros. Univariado: Analiza una sola variable a la vez, observando su distribución, tendencia central y dispersión. Multivariado: Examina dos o más variables simultáneamente, identificando relaciones o asociaciones entre ellas. Cada una de estas divisiones puede, a su vez, clasificarse según el tipo de variable involucrada: categórica o numérica. 3.2 Representación de datos según su naturaleza La forma en que se representan los datos depende fundamentalmente de su naturaleza, es decir, del tipo de variable que se está analizando. Esta clasificación influye directamente en la selección de métodos gráficos y estadísticos adecuados para el análisis. Tabla 3.1: Fechas de cortes y distribución de porcentajes Naturaleza.de.la.variable Escala.de.Medidas Frecuencias Medidas.de.Localización Medidas.de.Dispersión Medidas.de.Distribución Gráficos Cualitativa Nominal Sí Moda No No Sectores, Barras Ordinal Sí Moda No No Sectores, Barras (sin orden) Cuantitativa Intervalo Agrupadas Media, Mediana y Moda Sí Sí Histograma, Tallo y hojas, Cajas y Bigotes, Dispersión Razón Sí Sí 3.3 Presentación y análisis de la información en estudios descriptivos En los estudios descriptivos, la presentación y análisis de la información constituyen fases clave para comprender las características fundamentales de los datos recolectados. El objetivo principal es resumir y organizar la información de forma clara y accesible, sin realizar inferencias ni establecer relaciones causales. Tabla 3.2: Relación entre tipo de tabla y tipo de gráfico Tipo de Tabla Tipo de Gráfico De Frecuencia (Variable Cualitativa) Barras simples Pastel De Frecuencia (Variable Cuantitativa) Histograma De Asociacion (Dos Variables Cualitativas) Barras compuestas Barras superpuestas De Asociacion (Una Variable Cualitativa y una Cuantitativa Discreta) Barras: Compuestas Superpuestas De Asociacion (Una Variable Cualitativa y una Cuantitativa Continua) Poligono de Frecuencia Box plot (diagrama de cajas y bigotes) De Asociacion (Dos Variables Cuantitativas) Diagrama de Puntos 3.4 Análisis explotario con variable respuesta numérica Para el desarrollo de las visualización trabajaremos con datos sobre los almacenes Walmart, que es un cadena de grande almacenes de Ewa. Figura 3.2: Tienda Walmart El conjunto de datos completos lo puede encontrar este link. Trabajaremos un subconjunto de datos contiene las ventas semanales en dolares, cada tienda tiene un número de identificación y un tipo de tienda específico, las ventas estan separadas por ID de departamento. Junto con las ventas hay variables como si fue de vacaciones o no, la temperatura media durante la semana en esa localidad, el tiempo medio del combustible en dolares por litro esa semana y la tasa de desempleo de esa semana. 3.4.1 Contexto de los datos de Walmart Aquí tienes una explicación de las variables en el conjunto de datos de ventas proporcionado: Unnamed: Columna de índice que parece haber sido incluida al guardar el archivo. No es una variable significativa. store: Identificador del número de la tienda. type: Tipo de tienda, representado por una letra (por ejemplo, “A”, “B”, etc.). n department: Identificador del número de departamento dentro de la tienda. date: Fecha en la que se registró la venta. weekly_sales: Ventas semanales en USD registradas en esa tienda y departamento específicos (Target). is_holiday: Variable booleana que indica si la fecha corresponde a un día festivo o no. Los valores son “True” o “False”. temperature_c: Temperatura en grados Celsius en la fecha registrada. fuel_price_usd_per_l: Precio del combustible en dólares estadounidenses por litro en la fecha registrada. unemployment: Tasa de desempleo en la fecha registrada. El objetivo a resolver es predecir las ventas semanales, pero iniciaremos el análisis exploratorio de los datos de Walmart 3.4.2 Extracción, transformación y carga (ETL) Carguemos el conjunto de datos: Code url &lt;- &quot;https://raw.githubusercontent.com/cdeoroaguado/Datos/main/datamanip/sales.csv&quot; datos &lt;- read.csv(url) Verifiquemos que leímos bien los datos viendo el encabezado y la cola de los datos: Code head(datos,n=5) ## X store type department date weekly_sales is_holiday temperature_c fuel_price_usd_per_l unemployment ## 1 0 1 A 1 2010-02-05 24924.50 False 5.727778 0.6794508 8.106 ## 2 1 1 A 1 2010-03-05 21827.90 False 8.055556 0.6934520 8.106 ## 3 2 1 A 1 2010-04-02 57258.43 False 16.816667 0.7182841 7.808 ## 4 3 1 A 1 2010-05-07 17413.94 False 22.527778 0.7489281 7.808 ## 5 4 1 A 1 2010-06-04 17558.09 False 27.050000 0.7145857 7.808 Las dimensiones, los nombres de las columnas y la estructura de la base de datos se obtienen con los códigos: Code dim(datos) ## [1] 10774 10 Code colnames(datos) ## [1] &quot;X&quot; &quot;store&quot; &quot;type&quot; &quot;department&quot; ## [5] &quot;date&quot; &quot;weekly_sales&quot; &quot;is_holiday&quot; &quot;temperature_c&quot; ## [9] &quot;fuel_price_usd_per_l&quot; &quot;unemployment&quot; Code str(datos) ## &#39;data.frame&#39;: 10774 obs. of 10 variables: ## $ X : int 0 1 2 3 4 5 6 7 8 9 ... ## $ store : int 1 1 1 1 1 1 1 1 1 1 ... ## $ type : chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ department : int 1 1 1 1 1 1 1 1 1 1 ... ## $ date : chr &quot;2010-02-05&quot; &quot;2010-03-05&quot; &quot;2010-04-02&quot; &quot;2010-05-07&quot; ... ## $ weekly_sales : num 24925 21828 57258 17414 17558 ... ## $ is_holiday : chr &quot;False&quot; &quot;False&quot; &quot;False&quot; &quot;False&quot; ... ## $ temperature_c : num 5.73 8.06 16.82 22.53 27.05 ... ## $ fuel_price_usd_per_l: num 0.679 0.693 0.718 0.749 0.715 ... ## $ unemployment : num 8.11 8.11 7.81 7.81 7.81 ... Eliminemos la variable X y date, ya que no estan en nuestra investigación: Code library(tidyverse) datos %&gt;% select(-X,-date) -&gt; df Code head(df, n=5) ## store type department weekly_sales is_holiday temperature_c fuel_price_usd_per_l unemployment ## 1 1 A 1 24924.50 False 5.727778 0.6794508 8.106 ## 2 1 A 1 21827.90 False 8.055556 0.6934520 8.106 ## 3 1 A 1 57258.43 False 16.816667 0.7182841 7.808 ## 4 1 A 1 17413.94 False 22.527778 0.7489281 7.808 ## 5 1 A 1 17558.09 False 27.050000 0.7145857 7.808 Identifiquemos los valores NA por columna: Code df %&gt;% summarise(across(everything(), ~ sum(is.na(.)), .names = &quot;NA_{.col}&quot;)) ## NA_store NA_type NA_department NA_weekly_sales NA_is_holiday NA_temperature_c NA_fuel_price_usd_per_l ## 1 0 0 0 0 0 0 0 ## NA_unemployment ## 1 0 Otra forma de hacerlo es: Code library(Amelia) missmap(df) 3.4.3 Análisis de la variable weekly_sales (Target) Consideremos el resumen de la weekly_sales: Code df %&gt;% summarise( n = length(weekly_sales), media = mean(weekly_sales), ds = sd(weekly_sales), mediana = median(weekly_sales), minimo = min(weekly_sales), maximo = max(weekly_sales), Q1 = quantile(weekly_sales, 0.25), Q3 = quantile(weekly_sales, 0.75), IQR = IQR(weekly_sales) ) ## n media ds mediana minimo maximo Q1 Q3 IQR ## 1 10774 23843.95 30220.39 12049.06 -1098 293966 3867.115 32349.85 28482.73 La variable weekly_sales fue analizada a partir de 10.774 observaciones. Se obtuvo un promedio de ventas semanales de aproximadamente \\(23.843,95\\) usd (DS = \\(30.220,39\\) usd), donde El \\(50\\%\\) de las ventas semanales se encuentran por debajo de \\(12.049,06\\) usd. El mínimo registrado fue de \\(-1.098\\) usd, lo que indica que en ciertas semanas se presentaron saldos negativos de ventas, posiblemente debido a devoluciones de productos, ajustes contables o cancelaciones, situaciones comunes en grandes cadenas minoristas como Walmart. Por su parte, el máximo alcanzó los \\(293.966\\) usd, lo cual refleja una alta heterogeneidad entre tiendas o departamentos. Veamos un histograma Code df %&gt;% ggplot(aes(x = weekly_sales)) + geom_histogram(aes(y = after_stat(density)), binwidth = 5000, fill = &quot;#2c7fb8&quot;, color = &quot;white&quot;, alpha = 0.6) + geom_density(color = &quot;darkblue&quot;, linewidth = 1.2) + labs( title = &quot;Distribución de Ventas Semanales&quot;, x = &quot;Ventas Semanales (USD)&quot;, y = &quot;Densidad&quot; ) + theme_bw() Veamos un diagrama de cajas y bigotes Code df %&gt;% ggplot(aes(x = &quot;&quot;, y = weekly_sales)) + geom_boxplot(fill = &quot;#a6cee3&quot;, color = &quot;#1f78b4&quot;, outlier.color = &quot;red&quot;) + stat_summary( fun = mean, geom = &quot;point&quot;, shape = 20, size = 3, color = &quot;black&quot; ) + labs( title = &quot;Diagrama de cajas y bigotes de ventas semanales&quot;, x = &quot;&quot;, y = &quot;Ventas Semanales (USD)&quot; ) + theme_bw() La distribución de weekly_sales es altamente asimétrica, con fuerte concentración de valores bajos y una cantidad significativa de valores extremos altos. Esto indica que aunque la mayoría de las tiendas tienen ventas semanales moderadas, existen algunas con ventas excepcionalmente altas que influyen notablemente en los estadísticos como la media y la desviación estándar. 3.4.4 Análisis de las variables características (independientes) Analizaremos el iniciamente las variables numéricas Code df %&gt;% summarise( n = length(temperature_c), media = mean(temperature_c), ds = sd(temperature_c), mediana = median(temperature_c), minimo = min(temperature_c), maximo = max(temperature_c), Q1 = quantile(temperature_c, 0.25), Q3 = quantile(temperature_c, 0.75), IQR = IQR(temperature_c)) %&gt;% mutate(variable = &quot;temperature_c&quot;) -&gt; var_num_temp df %&gt;% summarise( n = length(fuel_price_usd_per_l), media = mean(fuel_price_usd_per_l), ds = sd(fuel_price_usd_per_l), mediana = median(fuel_price_usd_per_l), minimo = min(fuel_price_usd_per_l), maximo = max(fuel_price_usd_per_l), Q1 = quantile(fuel_price_usd_per_l, 0.25), Q3 = quantile(fuel_price_usd_per_l, 0.75), IQR = IQR(fuel_price_usd_per_l)) %&gt;% mutate(variable = &quot;fuel_price_usd_per_l&quot;) -&gt; var_num_fuel df %&gt;% summarise( n = length(unemployment), media = mean(unemployment), ds = sd(unemployment), mediana = median(unemployment), minimo = min(unemployment), maximo = max(unemployment), Q1 = quantile(unemployment, 0.25), Q3 = quantile(unemployment, 0.75), IQR = IQR(unemployment)) %&gt;% mutate(variable = &quot;unemployment&quot;)-&gt; var_num_unemploy bind_rows(var_num_temp, var_num_fuel, var_num_unemploy) %&gt;% select(variable, everything()) ## variable n media ds mediana minimo maximo Q1 Q3 IQR ## 1 temperature_c 10774 15.7319782 9.92244608 16.9666667 -8.3666667 33.827778 7.5833333 24.1666667 16.58333333 ## 2 fuel_price_usd_per_l 10774 0.7497458 0.05949359 0.7433805 0.6641289 1.107674 0.7082456 0.7814213 0.07317569 ## 3 unemployment 10774 8.0820086 0.62435501 8.0990000 3.8790000 9.765000 7.7950000 8.3600000 0.56500000 Code library(patchwork) # Boxplot de temperature_c p1 &lt;- df %&gt;% ggplot(aes(x=&quot;&quot;, y = temperature_c)) + geom_boxplot(fill = &quot;#1f77b4&quot;, alpha = 0.7) + labs( title = &quot;Distribucion de temperatura&quot;, y = &quot;Temperatura (°C)&quot;, x = &quot;&quot; ) + theme_bw() # Boxplot de fuel_price_usd_per_l p2 &lt;- df %&gt;% ggplot(aes(x=&quot;&quot;,y = fuel_price_usd_per_l)) + geom_boxplot(fill = &quot;#2ca02c&quot;, alpha = 0.7) + labs( title = &quot;Distribucion del precio del combustible&quot;, y = &quot;Precio (USD/litro)&quot;, x = &quot;&quot; ) + theme_bw() # Boxplot de unemployment p3 &lt;- df %&gt;% ggplot(aes(x=&quot;&quot;,y = unemployment)) + geom_boxplot(fill = &quot;#d62728&quot;, alpha = 0.7) + labs( title = &quot;Distribucion del desempleo&quot;, y = &quot;Tasa de desempleo (%)&quot;, x = &quot;&quot; ) + theme_bw() # Unir los tres gráficos en una sola visualización p1 + p2 + p3 Ejercicio La interpretación de la tabla y el gráfico quedan como ejercicio Ahora analizaremos las variables categóricas Code # Tabla de frecuencias para type tabla_type &lt;- df %&gt;% count(type, name = &quot;Frecuencia&quot;) %&gt;% mutate( Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;type&quot;, Categoria = as.character(type) ) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Tabla de frecuencias para department tabla_department &lt;- df %&gt;% count(department, name = &quot;Frecuencia&quot;) %&gt;% mutate( Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;department&quot;, Categoria = as.character(department) ) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Tabla de frecuencias para is_holiday tabla_holiday &lt;- df %&gt;% count(is_holiday, name = &quot;Frecuencia&quot;) %&gt;% mutate( Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;is_holiday&quot;, Categoria = as.character(is_holiday) ) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Unir todas las tablas bind_rows(tabla_type, tabla_department, tabla_holiday) ## Variable Categoria Frecuencia Porcentaje ## 1 type A 9872 91.63 ## 2 type B 902 8.37 ## 3 department 1 144 1.34 ## 4 department 2 144 1.34 ## 5 department 3 144 1.34 ## 6 department 4 144 1.34 ## 7 department 5 144 1.34 ## 8 department 6 144 1.34 ## 9 department 7 144 1.34 ## 10 department 8 144 1.34 ## 11 department 9 144 1.34 ## 12 department 10 144 1.34 ## 13 department 11 144 1.34 ## 14 department 12 144 1.34 ## 15 department 13 144 1.34 ## 16 department 14 144 1.34 ## 17 department 16 144 1.34 ## 18 department 17 144 1.34 ## 19 department 18 144 1.34 ## 20 department 19 140 1.30 ## 21 department 20 144 1.34 ## 22 department 21 144 1.34 ## 23 department 22 144 1.34 ## 24 department 23 144 1.34 ## 25 department 24 144 1.34 ## 26 department 25 144 1.34 ## 27 department 26 144 1.34 ## 28 department 27 144 1.34 ## 29 department 28 144 1.34 ## 30 department 29 144 1.34 ## 31 department 30 144 1.34 ## 32 department 31 144 1.34 ## 33 department 32 144 1.34 ## 34 department 33 144 1.34 ## 35 department 34 144 1.34 ## 36 department 35 144 1.34 ## 37 department 36 144 1.34 ## 38 department 37 120 1.11 ## 39 department 38 144 1.34 ## 40 department 39 7 0.06 ## 41 department 40 144 1.34 ## 42 department 41 144 1.34 ## 43 department 42 144 1.34 ## 44 department 43 2 0.02 ## 45 department 44 144 1.34 ## 46 department 45 126 1.17 ## 47 department 46 144 1.34 ## 48 department 47 114 1.06 ## 49 department 48 90 0.84 ## 50 department 49 144 1.34 ## 51 department 50 72 0.67 ## 52 department 51 99 0.92 ## 53 department 52 144 1.34 ## 54 department 54 144 1.34 ## 55 department 55 144 1.34 ## 56 department 56 144 1.34 ## 57 department 58 144 1.34 ## 58 department 59 144 1.34 ## 59 department 60 144 1.34 ## 60 department 67 144 1.34 ## 61 department 71 144 1.34 ## 62 department 72 144 1.34 ## 63 department 74 144 1.34 ## 64 department 77 39 0.36 ## 65 department 78 56 0.52 ## 66 department 79 144 1.34 ## 67 department 80 144 1.34 ## 68 department 81 144 1.34 ## 69 department 82 144 1.34 ## 70 department 83 144 1.34 ## 71 department 85 144 1.34 ## 72 department 87 144 1.34 ## 73 department 90 144 1.34 ## 74 department 91 144 1.34 ## 75 department 92 144 1.34 ## 76 department 93 144 1.34 ## 77 department 94 144 1.34 ## 78 department 95 144 1.34 ## 79 department 96 138 1.28 ## 80 department 97 144 1.34 ## 81 department 98 144 1.34 ## 82 department 99 123 1.14 ## 83 is_holiday False 10732 99.61 ## 84 is_holiday True 42 0.39 El \\(91.63\\%\\) de los registros pertenecen al tipo A, lo que indica que la gran mayoría de las observaciones están asociadas a este tipo de tienda, donde el tipo B representa solo el \\(8.37\\%\\) de los casos, lo cual puede sugerir que es menos común, más específico o de menor cobertura. Existen más de \\(80\\) categorías distintas. La mayoría de los departamentos tienen la misma frecuencia de \\(144\\) observaciones (\\(1.34\\%\\)), lo que sugiere una distribución uniforme entre muchas de las categorías. Tambien nos inidica que hay una alta dispersión en las frecuencias de los departamentos, lo que indica que no todos los departamentos tienen el mismo nivel de actividad. Algunos departamentos deben ser tratados con precaución en los análisis inferenciales por su baja frecuencia. Solo el \\(0.39\\%\\) de los registros corresponden a días festivos. Esto muestra una clara desproporción: los días feriados son poco frecuentes en los datos. Aunque son escasos, los registros en días festivos pueden tener un comportamiento distinto (por ejemplo, picos de ventas o cambios de demanda), por lo que deben analizarse por separado o con métodos robustos que no se vean afectados por el desbalance. Code tabla_type_b &lt;- df %&gt;% count(type, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 1), Etiqueta = paste0(Frecuencia, &quot; (&quot;, Porcentaje, &quot;%)&quot;)) ggplot(tabla_type_b, aes(x = type, y = Frecuencia)) + geom_col(fill = &quot;#008B8B&quot;, width = 0.6) + geom_text(aes(label = Etiqueta), vjust = -0.5, size = 5) + facet_wrap(~ &quot;Distribución de la variable type&quot;) + scale_y_continuous(expand = expansion(mult = c(0, 0.15))) + labs(x = &quot;Tipo de tienda&quot;, y = &quot;Frecuencia (Porcentaje)&quot;) + theme_bw(base_size = 14) + theme( plot.title = element_blank(), strip.background = element_rect(fill = &quot;gray80&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;), panel.grid.major.x = element_blank() ) La gran mayoría de las observaciones (\\(91.6\\%\\)) corresponden al tipo A, con \\(9872\\) registros. Solo el \\(8.4\\%\\) restante corresponde al tipo B, con \\(902\\) registros. Code # Crear tabla de frecuencias para department tabla_department_b &lt;- df %&gt;% count(department, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 1), Etiqueta = paste0(Frecuencia, &quot; (&quot;, Porcentaje, &quot;%)&quot;)) # Gráfico ggplot(tabla_department_b, aes(x = factor(department), y = Frecuencia)) + geom_col(fill = &quot;#008B8B&quot;, width = 0.6) + geom_text(aes(label = Etiqueta), hjust = -0.1, size = 3) + facet_wrap(~ &quot;Distribución de la variable department&quot;) + scale_y_continuous(expand = expansion(mult = c(0, 0.10))) + labs(x = &quot;Departamento&quot;, y = &quot;Frecuencia (Porcentaje)&quot;) + coord_flip() + theme_bw(base_size = 14) + theme( plot.title = element_blank(), strip.background = element_rect(fill = &quot;gray80&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;), panel.grid.major.y = element_blank(), axis.text.y = element_text(size = 8) ) Ejercicio Interpreta el gráfico de la variable department y muestra únicamente las 10 categorías con mayor porcentaje. Realiza el gráfico para la variable is_holiday e interprétalo. 3.4.5 Análisis exploratorio bivariado Realizaremos la comparacion de la variable weekly_sales con respecto a las variables numéricas independientes. Code # Diagrama de dispersión: weekly_sales vs temperature_c df %&gt;% ggplot(aes(x = temperature_c, y = weekly_sales)) + geom_point(alpha = 0.4, color = &quot;#1E90FF&quot;) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + labs(x = &quot;Temperatura (°C)&quot;, y = &quot;Ventas semanales (uds)&quot; ) + theme_bw()+ facet_grid(.~ &quot;Dispersión entre Weekly Sales y Temperature (°C)&quot;) Los puntos están muy dispersos a lo largo de todo el eje de temperatura (desde aproximadamente \\(-10 °C\\) hasta \\(35 °C\\)), pero no muestran una tendencia clara ascendente ni descendente. La mayoría de las tiendas se agrupan entre temperaturas de \\(0 °C\\) y \\(30 °C\\), lo que es esperado para regiones con climas templados. A lo largo del eje de temperatura, las ventas semanales presentan alta variabilidad, es decir, hay tiendas con ventas altas y bajas a cualquier temperatura. Existen varios puntos con ventas muy elevadas (por encima de 200.000), que podrían representar eventos especiales, promociones, festividades, etc. También se observan múltiples valores cercanos a cero, lo cual podría indicar tiendas cerradas, errores en la base de datos, o semanas sin ventas. No se evidencia una relación lineal clara entre la temperatura y las ventas semanales. La temperatura parece tener un efecto mínimo o nulo sobre las ventas, lo cual sugiere que otras variables podrían tener mayor peso explicativo en el comportamiento de weekly_sales. Ejercicio Realiza el diagrama de dispersión para la variable weekly_sales con respecto a las variables fuel_price_usd_per_l y unemployment e interprétalos. Realizaremos la comparacion de la variable weekly_sales con respecto a las variables categóricas independientes. Code # Agrupación por &#39;type&#39; type_week &lt;- df %&gt;% group_by(type) %&gt;% summarise(n = length(weekly_sales), media = mean(weekly_sales), ds = sd(weekly_sales), mediana = median(weekly_sales), minimo = min(weekly_sales), maximo = max(weekly_sales), Q1 = quantile(weekly_sales, 0.25), Q3 = quantile(weekly_sales, 0.75), IQR = IQR(weekly_sales)) %&gt;% mutate(variable = &quot;type&quot;, niveles = as.character(type)) %&gt;% select(variable, niveles, everything(), -type) # Agrupación por &#39;store&#39; store_week &lt;- df %&gt;% group_by(store) %&gt;% summarise(n = length(weekly_sales), media = mean(weekly_sales), ds = sd(weekly_sales), mediana = median(weekly_sales), minimo = min(weekly_sales), maximo = max(weekly_sales), Q1 = quantile(weekly_sales, 0.25), Q3 = quantile(weekly_sales, 0.75), IQR = IQR(weekly_sales)) %&gt;% mutate(variable = &quot;store&quot;, niveles = as.character(store)) %&gt;% select(variable, niveles, everything(), -store) # Agrupación por &#39;department&#39; dep_week &lt;- df %&gt;% group_by(department) %&gt;% summarise(n = length(weekly_sales), media = mean(weekly_sales), ds = sd(weekly_sales), mediana = median(weekly_sales), minimo = min(weekly_sales), maximo = max(weekly_sales), Q1 = quantile(weekly_sales, 0.25), Q3 = quantile(weekly_sales, 0.75), IQR = IQR(weekly_sales)) %&gt;% mutate(variable = &quot;department&quot;, niveles = as.character(department)) %&gt;% select(variable, niveles, everything(), -department) # Agrupación por &#39;is_holiday&#39; holi_week &lt;- df %&gt;% group_by(is_holiday) %&gt;% summarise(n = length(weekly_sales), media = mean(weekly_sales), ds = sd(weekly_sales), mediana = median(weekly_sales), minimo = min(weekly_sales), maximo = max(weekly_sales), Q1 = quantile(weekly_sales, 0.25), Q3 = quantile(weekly_sales, 0.75), IQR = IQR(weekly_sales)) %&gt;% mutate(variable = &quot;is_holiday&quot;, niveles = as.character(is_holiday)) %&gt;% select(variable, niveles, everything(), -is_holiday) # Unión de todas las tablas bind_rows(type_week, store_week, dep_week, holi_week) ## # A tibble: 96 × 11 ## variable niveles n media ds mediana minimo maximo Q1 Q3 IQR ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 type A 9872 23675. 30129. 11944. -1098 293966. 3862. 31982. 28120. ## 2 type B 902 25697. 31156. 13336. -798 232559. 3998. 38195. 34197. ## 3 store 1 901 20897. 26994. 9775. -698 140504. 3199 30186. 26987. ## 4 store 2 897 26517. 32682. 13765. -1098 178983. 4892. 34612. 29720. ## 5 store 4 901 26127. 31202. 13064. -88 165766. 4571. 35552. 30982. ## 6 store 6 894 21561. 23658. 13201. -698 119812. 4284. 30962. 26678. ## 7 store 10 902 25697. 31156. 13336. -798 232559. 3998. 38195. 34197. ## 8 store 13 913 25664. 31499. 13050. -98 166872. 4396. 34686. 30290. ## 9 store 14 885 30384. 40467. 14793. -498 293966. 4386 37384. 32998. ## 10 store 19 906 19931. 24662. 11092. -449 147449. 3695. 24861. 21166. ## # ℹ 86 more rows La variable type clasifica las tiendas en dos categorías: A y B, donde el \\(91.6\\%\\) de los registros corresponde a tiendas tipo A (\\(n = 9872\\)) y solo el \\(8.4\\%\\) a tipo B (\\(n = 902\\)). A pesar de su menor frecuencia, las tiendas tipo B presentan una media de ventas semanales más alta de \\(25696.68\\) usd (DS = \\(31155.87\\) usd) comparada con las de tipo A de \\(23674.67\\) usd (DS = \\(30129.41\\) usd). El \\(50\\%\\) de las tiendas tipo B tiene ventas semanales por debajo de 13336.08 usd, mientras que en las tipo A este valor es de \\(11943.920\\) usd. Además, las tiendas tipo B presentan un rango intercuartílico más amplio (IQR = \\(34196.80\\) usd) en comparación con las tipo A (IQR = \\(28119.99\\) usd), lo que indica una mayor dispersión en la distribución de sus ventas semanales. Estos datos sugieren que, aunque las tiendas tipo B son menos frecuentes, tienden a mostrar un mejor desempeño en ventas, acompañado de una mayor variabilidad en sus resultados. La variable is_holiday identifica si una semana corresponde a un periodo festivo (True) o no (False). La mayoría de los registros (\\(99.6\\%\\)) corresponden a semanas no festivas (\\(n = 10732\\)), mientras que solo el \\(0.4\\%\\) pertenece a semanas festivas (\\(n = 42\\)). Las semanas no festivas presentan una media de ventas semanales de \\(23934.91 usd\\) (DS = \\(30,244.33\\) usd), significativamente superior a la de las semanas festivas, que registran en promedio apenas \\(600.55\\) usd (DS = \\(1054.73\\) usd). El \\(50\\%\\) de las semanas no festivas tiene ventas por debajo de \\(12135.16\\) usd, mientras que en las semanas festivas ese valor cae a tan solo \\(37.50\\) usd. El rango intercuartílico (IQR) también es mucho más amplio en las semanas no festivas (\\(28555.85\\) usd) que en las festivas (\\(928\\) usd), lo que indica no solo un volumen mayor de ventas en semanas regulares, sino también una mayor variabilidad. Estos resultados reflejan que las semanas festivas se asocian con un desempeño comercial considerablemente menor en comparación con las semanas ordinarias. Ejercicio Realiza la interpretación para la variable weekly_sales con respecto a las variables department y store. Code # boxplot: weekly_sales vs tipo df %&gt;% ggplot(aes(x = type, y = weekly_sales)) + geom_boxplot(fill = &quot;#87CEFA&quot;, outlier.colour = &quot;red&quot;, outlier.shape = 16) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 3, color = &quot;darkblue&quot;) + labs(x = &quot;Tipo de tienda&quot;, y = &quot;Ventas semanales (usd)&quot; ) + theme_bw()+ facet_grid(.~&quot;Distribución de Weekly Sales por tipo de tienda&quot;) El gráfico compara las ventas semanales de las tiendas tipo A y B. Se observa que, aunque hay muchas más tiendas tipo A, las tiendas tipo B presentan un promedio de ventas semanales ligeramente más alto, lo cual se refleja en el rombo azul más elevado en su caja. Además, las tiendas tipo B muestran mayor variación en sus ventas, es decir, algunas venden mucho y otras menos, mientras que las tipo A tienen un comportamiento un poco más concentrado. Los puntos rojos sobre las cajas indican semanas con ventas excepcionalmente altas, siendo más frecuentes en las tiendas tipo A. En conjunto, esto sugiere que las tiendas tipo B, aunque menos numerosas, tienden a tener un mejor desempeño promedio, pero con mayor variabilidad; mientras que las tiendas tipo A tienen más casos extremos de ventas muy altas. Ejercicio Realiza la gráfica para la variable weekly_sales con respecto a las variables department, is_holiday y store. Interprétala. Realiza el análisis bivariado con respecto a las variables independientes. Paquete GGally El paquete GGally es una extensión de ggplot2 que facilita la creación de gráficos multivariados para análisis exploratorio de datos. Una de sus funciones más utilizadas es ggpairs(), que permite visualizar simultáneamente relaciones entre varias variables, tanto numéricas como categóricas, mediante: Diagramas de dispersión (scatterplots), Histogramas o densidades en la diagonal, Correlaciones entre pares de variables numéricas. Esto es especialmente útil para detectar patrones, asociaciones o valores atípicos en conjuntos de datos con múltiples variables. Su instalación y carga del paquete Code install.packages(&quot;GGally&quot;) library(GGally) Modo de uso del paquete GGally: Code library(GGally) df %&gt;% ggpairs() Ejercicio Menciona los errores que puedes encontrar en el gráfico 3.5 Análisis explotario con variable respuesta categórica 3.5.1 Contexto del conjunto de datos de marketing de un banco Este conjunto de datos corresponde a campañas de marketing directo realizadas por una institución bancaria portuguesa, cuyo objetivo era promover la contratación de depósitos a plazo fijo por parte de sus clientes. Las campañas se llevaron a cabo principalmente mediante llamadas telefónicas, y los datos recolectados reflejan tanto características socioeconómicas del cliente como información específica de la interacción comercial. El conjunto contiene 4521 observaciones y 17 variables, entre las cuales se incluyen datos demográficos, financieros y relacionados con el historial de contacto de cada cliente. A continuación se describen las variables: Estas son las variables age: Edad del cliente. job: Profesión del cliente (por ejemplo: desempleado, servicios, gestión). marital: Estado civil del cliente (soltero, casado, divorciado). education: Nivel educativo (primaria, secundaria, terciaria). default: Indica si el cliente tiene crédito en incumplimiento (sí/no). balance: Saldo promedio anual de la cuenta bancaria. housing: Indica si posee préstamo hipotecario (sí/no). loan: Indica si tiene préstamo personal (sí/no). contact: Tipo de comunicación utilizada (celular o fijo). day: Día del mes en que se realizó el último contacto. month: Mes del último contacto. duration: Duración de la última llamada (en segundos). campaign: Número de contactos realizados durante la campaña actual. pdays: Número de días transcurridos desde el último contacto previo (valor -1 indica que no hubo contacto anterior). previous: Número de contactos previos al actual. poutcome: Resultado de una campaña anterior (éxito, fracaso, desconocido). y: Variable objetivo que indica si el cliente aceptó (yes) o no aceptó (no) contratar un depósito a plazo fijo. El objetivo principal es predecir si un cliente aceptará o no un depósito a plazo fijo (y), a partir de sus características personales, financieras y del historial de contactos anteriores. Este análisis permitirá a la entidad financiera optimizar sus estrategias de marketing, enfocándose en los perfiles con mayor probabilidad de conversión, reduciendo costos y mejorando la eficiencia de las campañas. Inicialmente, haremos un análisis exploratorio de los datos. 3.5.2 Extracción, transformación y carga (ETL) Carguemos el conjunto de datos: Code url &lt;- &quot;https://raw.githubusercontent.com/cdeoroaguado/Datos/refs/heads/main/datamanip/bank.csv&quot; datos &lt;- read.csv(url, sep = &quot;;&quot;, stringsAsFactors = TRUE) Verifiquemos que leímos bien los datos viendo el encabezado y la cola de los datos: Code head(datos,n=5) ## age job marital education default balance housing loan contact day month duration campaign pdays ## 1 30 unemployed married primary no 1787 no no cellular 19 oct 79 1 -1 ## 2 33 services married secondary no 4789 yes yes cellular 11 may 220 1 339 ## 3 35 management single tertiary no 1350 yes no cellular 16 apr 185 1 330 ## 4 30 management married tertiary no 1476 yes yes unknown 3 jun 199 4 -1 ## 5 59 blue-collar married secondary no 0 yes no unknown 5 may 226 1 -1 ## previous poutcome y ## 1 0 unknown no ## 2 4 failure no ## 3 1 failure no ## 4 0 unknown no ## 5 0 unknown no Las dimensiones, los nombres de las columnas y la estructura de la base de datos se obtienen con los códigos: Code dim(datos) ## [1] 4521 17 Code colnames(datos) ## [1] &quot;age&quot; &quot;job&quot; &quot;marital&quot; &quot;education&quot; &quot;default&quot; &quot;balance&quot; &quot;housing&quot; &quot;loan&quot; &quot;contact&quot; ## [10] &quot;day&quot; &quot;month&quot; &quot;duration&quot; &quot;campaign&quot; &quot;pdays&quot; &quot;previous&quot; &quot;poutcome&quot; &quot;y&quot; Code str(datos) ## &#39;data.frame&#39;: 4521 obs. of 17 variables: ## $ age : int 30 33 35 30 59 35 36 39 41 43 ... ## $ job : Factor w/ 12 levels &quot;admin.&quot;,&quot;blue-collar&quot;,..: 11 8 5 5 2 5 7 10 3 8 ... ## $ marital : Factor w/ 3 levels &quot;divorced&quot;,&quot;married&quot;,..: 2 2 3 2 2 3 2 2 2 2 ... ## $ education: Factor w/ 4 levels &quot;primary&quot;,&quot;secondary&quot;,..: 1 2 3 3 2 3 3 2 3 1 ... ## $ default : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ balance : int 1787 4789 1350 1476 0 747 307 147 221 -88 ... ## $ housing : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 2 2 1 2 2 2 2 ... ## $ loan : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 1 2 1 1 1 1 1 2 ... ## $ contact : Factor w/ 3 levels &quot;cellular&quot;,&quot;telephone&quot;,..: 1 1 1 3 3 1 1 1 3 1 ... ## $ day : int 19 11 16 3 5 23 14 6 14 17 ... ## $ month : Factor w/ 12 levels &quot;apr&quot;,&quot;aug&quot;,&quot;dec&quot;,..: 11 9 1 7 9 4 9 9 9 1 ... ## $ duration : int 79 220 185 199 226 141 341 151 57 313 ... ## $ campaign : int 1 1 1 4 1 2 1 2 2 1 ... ## $ pdays : int -1 339 330 -1 -1 176 330 -1 -1 147 ... ## $ previous : int 0 4 1 0 0 3 2 0 0 2 ... ## $ poutcome : Factor w/ 4 levels &quot;failure&quot;,&quot;other&quot;,..: 4 1 1 4 4 1 2 4 4 1 ... ## $ y : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... Para facilitar el análisis exploratorio y evitar redundancias o ruido en el modelo, se eliminarán las variables day, month y previous. Code library(tidyverse) datos %&gt;% select(-day, -month, -previous) -&gt; df Code head(df, n=5) ## age job marital education default balance housing loan contact duration campaign pdays poutcome y ## 1 30 unemployed married primary no 1787 no no cellular 79 1 -1 unknown no ## 2 33 services married secondary no 4789 yes yes cellular 220 1 339 failure no ## 3 35 management single tertiary no 1350 yes no cellular 185 1 330 failure no ## 4 30 management married tertiary no 1476 yes yes unknown 199 4 -1 unknown no ## 5 59 blue-collar married secondary no 0 yes no unknown 226 1 -1 unknown no Identifiquemos los valores NA por columna: Code df %&gt;% summarise(across(everything(), ~ sum(is.na(.)), .names = &quot;NA_{.col}&quot;)) ## NA_age NA_job NA_marital NA_education NA_default NA_balance NA_housing NA_loan NA_contact NA_duration NA_campaign ## 1 0 0 0 0 0 0 0 0 0 0 0 ## NA_pdays NA_poutcome NA_y ## 1 0 0 0 Otra forma de hacerlo es: Code library(Amelia) missmap(df) 3.5.3 Análisis de la variable y (Target) Consideremos el resumen de la y: Code df %&gt;% count(y) %&gt;% mutate(porcentaje = (n/sum(n))*100) ## y n porcentaje ## 1 no 4000 88.476 ## 2 yes 521 11.524 La variable y representa el resultado de la campaña de marketing, indicando si el cliente aceptó (yes) o no aceptó (no) contratar un depósito a plazo fijo. De un total de \\(4521\\) observaciones, el \\(88.48\\%\\) de los clientes (\\(n = 4000\\)) no aceptaron la oferta, mientras que solo el \\(11.52\\%\\) (\\(n = 521\\)) sí lo hicieron. Esta distribución refleja un fuerte desequilibrio en los clientes, lo que sugiere que la mayoría de los clientes no están interesados en este producto financiero, y dicho desequilibrio debe considerarse cuidadosamente al construir modelos de predicción. Veamos un diagrama de barras Code # Crear tabla de frecuencias para department tabla_y &lt;- df %&gt;% count(y, name = &quot;Clientes&quot;) %&gt;% mutate(Porcentaje = round(Clientes / sum(Clientes) * 100, 1), Etiqueta = paste0(Clientes, &quot; (&quot;, Porcentaje, &quot;%)&quot;)) # Gráfico tabla_y %&gt;% ggplot(aes(x = factor(y), y = Clientes)) + geom_col(fill = &quot;#008B8B&quot;, width = 0.6) + geom_text(aes(label = Etiqueta), vjust = -0.5, size = 3) + facet_grid(~ &quot;Distribución de la campaña de marketing (no/yes)&quot;) + scale_y_continuous(expand = expansion(mult = c(0, 0.15))) + labs(x = &quot;Campaña de marketing&quot;, y = &quot;número de clientes (Porcentaje)&quot;) + #coord_flip() + theme_bw(base_size = 14) + theme( plot.title = element_blank(), strip.background = element_rect(fill = &quot;gray80&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;), panel.grid.major.y = element_blank(), axis.text.y = element_text(size = 8) ) El gráfico de barras muestra claramente la distribución de respuestas en la campaña de marketing, evidenciando que la gran mayoría de los clientes no aceptaron la oferta del depósito a plazo fijo. Específicamente, el \\(88.5\\%\\) de los clientes (\\(4000\\) personas) rechazaron la campaña (no), mientras que solo el \\(11.5&#39;%\\) (\\(521\\) personas) decidieron aceptarla (yes). Esta marcada diferencia indica un fuerte desbalance en las clases de la variable objetivo, lo cual es relevante al momento de construir modelos predictivos, ya que se requiere considerar técnicas que manejen adecuadamente este desequilibrio para no sesgar el modelo hacia la clase mayoritaria. 3.5.4 Análisis de las variables características (independientes) Analizaremos el iniciamente las variables numéricas Code df %&gt;% summarise( n = length(age), media = mean(age), ds = sd(age), mediana = median(age), minimo = min(age), maximo = max(age), Q1 = quantile(age, 0.25), Q3 = quantile(age, 0.75), IQR = IQR(age)) %&gt;% mutate(variable = &quot;age&quot;) -&gt; var_num_age df %&gt;% summarise( n = length(balance), media = mean(balance), ds = sd(balance), mediana = median(balance), minimo = min(balance), maximo = max(balance), Q1 = quantile(balance, 0.25), Q3 = quantile(balance, 0.75), IQR = IQR(balance)) %&gt;% mutate(variable = &quot;balance&quot;) -&gt; var_num_bal df %&gt;% summarise( n = length(duration), media = mean(duration), ds = sd(duration), mediana = median(duration), minimo = min(duration), maximo = max(duration), Q1 = quantile(duration, 0.25), Q3 = quantile(duration, 0.75), IQR = IQR(duration)) %&gt;% mutate(variable = &quot;duration&quot;)-&gt; var_num_dur df %&gt;% summarise( n = length(campaign), media = mean(campaign), ds = sd(campaign), mediana = median(campaign), minimo = min(campaign), maximo = max(campaign), Q1 = quantile(campaign, 0.25), Q3 = quantile(campaign, 0.75), IQR = IQR(campaign)) %&gt;% mutate(variable = &quot;campaign&quot;)-&gt; var_num_camp df %&gt;% summarise( n = length(pdays), media = mean(pdays), ds = sd(pdays), mediana = median(pdays), minimo = min(pdays), maximo = max(pdays), Q1 = quantile(pdays, 0.25), Q3 = quantile(pdays, 0.75), IQR = IQR(pdays)) %&gt;% mutate(variable = &quot;pdays&quot;)-&gt; var_num_pdays bind_rows(var_num_age, var_num_bal, var_num_dur,var_num_camp,var_num_pdays) %&gt;% select(variable, everything()) ## variable n media ds mediana minimo maximo Q1 Q3 IQR ## 1 age 4521 41.17010 10.576211 39 19 87 33 49 16 ## 2 balance 4521 1422.65782 3009.638142 444 -3313 71188 69 1480 1411 ## 3 duration 4521 263.96129 259.856633 185 4 3025 104 329 225 ## 4 campaign 4521 2.79363 3.109807 2 1 50 1 3 2 ## 5 pdays 4521 39.76664 100.121124 -1 -1 871 -1 -1 0 El análisis estadístico de las variables numéricas revela características clave de los clientes contactados durante la campaña de marketing. La edad promedio es de \\(41.17\\) años (DS = \\(10.58\\) años), con un rango de \\(19\\) a \\(87\\) años, lo que indica una población adulta diversa. El saldo promedio anual en la cuenta bancaria (balance) es de \\(1422.66\\) USD (DS = \\(3009.64\\) USD), evidenciando una alta dispersión y la presencia de valores extremos, desde \\(-3313\\) hasta \\(71188\\) USD. La duración de las llamadas telefónicas presenta un promedio de \\(263.96\\) segundos (DS = \\(259.86\\) segundos), con el \\(50\\%\\) de las observaciones por debajo de \\(185\\) segundos y un rango que va desde \\(4\\) hasta \\(3025\\) segundos, lo que sugiere una distribución muy dispersa y posiblemente sesgada. En cuanto al número de contactos durante la campaña (campaign), se observa un promedio de \\(2.79\\) contactos (DS = \\(3.11\\)), con el \\(50\\%\\) de los registros por debajo de \\(2\\) y valores extremos que alcanzan hasta \\(50\\), aunque la mayoría de los clientes fue contactado pocas veces (rango intercuartílico de \\(P75% = 3\\) y \\(P25% = 1\\)). Por último, la variable pdays, que indica los días transcurridos desde el último contacto en campañas anteriores, tiene una media de \\(39.77\\) días (DS = \\(100.12\\) días), pero con el \\(50\\%\\) de los registros y los percentiles \\(25\\%\\) y \\(75\\%\\) en \\(-1\\), lo que indica que la mayoría de los clientes no había sido contactada previamente, a pesar de que algunos registros superan los \\(800\\) días. Estos datos permiten identificar comportamientos heterogéneos y posibles segmentos dentro de la base de clientes. Code library(patchwork) library(gridExtra) # Boxplot de age p1 &lt;- df %&gt;% ggplot(aes(x = &quot;&quot;, y = age)) + geom_boxplot(fill = &quot;#1f77b4&quot;, alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;black&quot;) + labs( title = &quot;Distribución de la edad&quot;, y = &quot;Años&quot;, x = &quot;&quot; ) + theme_bw() # Boxplot de balance p2 &lt;- df %&gt;% ggplot(aes(x = &quot;&quot;, y = balance)) + geom_boxplot(fill = &quot;#2f43b1&quot;, alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;black&quot;) + labs( title = &quot;Distribución del balance de la cuenta&quot;, y = &quot;Usd&quot;, x = &quot;&quot; ) + theme_bw() # Diagrama de boxplot para duration p3 &lt;- df %&gt;% ggplot(aes(x = &quot;&quot;, y = duration)) + geom_boxplot(fill = &quot;#2ca02c&quot;, alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;black&quot;) + labs( title = &quot;Distribución de la duración de llamada (duration)&quot;, y = &quot;Segundos&quot;, x = &quot;&quot; ) + theme_bw() # Diagrama de boxplot para campaign p4 &lt;-df %&gt;% ggplot(aes(x = &quot;&quot;, y = campaign)) + geom_boxplot(fill = &quot;#ff7f0e&quot;, alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;black&quot;) + labs( title = &quot;Distribución del número de contactos (campaign)&quot;, y = &quot;Número de contactos&quot;, x = &quot;&quot; ) + theme_bw() # Diagrama de boxplot para pdays p5 &lt;-df %&gt;% ggplot(aes(x = &quot;&quot;, y = pdays)) + geom_boxplot(fill = &quot;#9467bd&quot;, alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;black&quot;) + labs( title = &quot;Días desde último contacto en campaña anterior (pdays)&quot;, y = &quot;Días&quot;, x = &quot;&quot; ) + theme_bw() grid.arrange(p1, p2,p3,p4,p5, ncol = 2,newpage = FALSE) Ejercicio La interpretación de los gráficos quedan como ejercicio Ahora analizaremos las variables categóricas Code # Para job tabla_job &lt;- df %&gt;% count(job, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;job&quot;, Categoria = job) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para marital tabla_marital &lt;- df %&gt;% count(marital, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;marital&quot;, Categoria = marital) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para education tabla_education &lt;- df %&gt;% count(education, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;education&quot;, Categoria = education) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para default tabla_default &lt;- df %&gt;% count(default, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;default&quot;, Categoria = default) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para housing tabla_housing &lt;- df %&gt;% count(housing, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;housing&quot;, Categoria = housing) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para loan tabla_loan &lt;- df %&gt;% count(loan, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;loan&quot;, Categoria = loan) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para contact tabla_contact &lt;- df %&gt;% count(contact, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;contact&quot;, Categoria = contact) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Para poutcome tabla_poutcome &lt;- df %&gt;% count(poutcome, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 2), Variable = &quot;poutcome&quot;, Categoria = poutcome) %&gt;% select(Variable, Categoria, Frecuencia, Porcentaje) # Unir todas las tablas bind_rows(tabla_job, tabla_marital, tabla_education,tabla_default, tabla_housing, tabla_loan,tabla_contact, tabla_poutcome) ## Variable Categoria Frecuencia Porcentaje ## 1 job admin. 478 10.57 ## 2 job blue-collar 946 20.92 ## 3 job entrepreneur 168 3.72 ## 4 job housemaid 112 2.48 ## 5 job management 969 21.43 ## 6 job retired 230 5.09 ## 7 job self-employed 183 4.05 ## 8 job services 417 9.22 ## 9 job student 84 1.86 ## 10 job technician 768 16.99 ## 11 job unemployed 128 2.83 ## 12 job unknown 38 0.84 ## 13 marital divorced 528 11.68 ## 14 marital married 2797 61.87 ## 15 marital single 1196 26.45 ## 16 education primary 678 15.00 ## 17 education secondary 2306 51.01 ## 18 education tertiary 1350 29.86 ## 19 education unknown 187 4.14 ## 20 default no 4445 98.32 ## 21 default yes 76 1.68 ## 22 housing no 1962 43.40 ## 23 housing yes 2559 56.60 ## 24 loan no 3830 84.72 ## 25 loan yes 691 15.28 ## 26 contact cellular 2896 64.06 ## 27 contact telephone 301 6.66 ## 28 contact unknown 1324 29.29 ## 29 poutcome failure 490 10.84 ## 30 poutcome other 197 4.36 ## 31 poutcome success 129 2.85 ## 32 poutcome unknown 3705 81.95 En cuanto al estado civil (marital), la mayoría de los clientes están casados (\\(61.87\\%\\)), seguidos por solteros (\\(26.45\\%\\)) y divorciados (\\(11.68\\%\\)), lo que sugiere una base de datos predominantemente compuesta por personas con vínculos conyugales estables. Respecto al nivel educativo (education), más de la mitad de los clientes tiene educación secundaria (\\(51.01\\%\\)), mientras que el \\(29.86\\%\\) alcanzó estudios terciarios y el \\(15.00\\%\\) solo nivel primario. Un pequeño porcentaje (\\(4.14\\%\\)) no reporta su nivel educativo. Estos datos permiten identificar patrones demográficos que podrían ser relevantes para el diseño de estrategias de marketing más efectivas según el perfil del cliente. Ejercicio Continua interpretando las demas variables. Code tabla_default_b &lt;- df %&gt;% count(default, name = &quot;Frecuencia&quot;) %&gt;% mutate(Porcentaje = round(Frecuencia / sum(Frecuencia) * 100, 1), Etiqueta = paste0(Frecuencia, &quot; (&quot;, Porcentaje, &quot;%)&quot;)) tabla_default_b %&gt;% ggplot(aes(x = default, y = Frecuencia)) + geom_col(fill = &quot;#008B8B&quot;, width = 0.6) + geom_text(aes(label = Etiqueta), vjust = -0.5, size = 5) + facet_wrap(~ &quot;Distribución de la variable default&quot;) + scale_y_continuous(expand = expansion(mult = c(0, 0.15))) + labs(x = &quot;¿Tiene crédito en incumplimiento?&quot;, y = &quot;Frecuencia (Porcentaje)&quot;) + theme_bw(base_size = 14) + theme( plot.title = element_blank(), strip.background = element_rect(fill = &quot;gray80&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;), panel.grid.major.x = element_blank() ) La gráfica muestra la distribución de la variable default, que indica si un cliente tiene o no un crédito en incumplimiento de pago. Se observa que el \\(98.3\\%\\) de los clientes (\\(4445\\) personas) no tienen incumplimientos, mientras que solo el \\(1.7\\%\\) (\\(76\\) personas) sí presentan incumplimientos. Esta marcada desproporción sugiere que la mayoría de los clientes se encuentran al día con sus compromisos financieros, y que el incumplimiento es un evento poco frecuente en la base de datos. Ejercicio Realiza los gráficos de las variables independientes categóricas faltantes e interprelas. 3.5.5 Análisis exploratorio bivariado Realizaremos la comparacion de la variable y con respecto a las variables numéricas independientes. Code # Diagrama boxplot: y vs age df %&gt;% ggplot(aes(x = y, y = age)) + geom_boxplot(fill = &quot;#1E90FF&quot;, alpha = 0.6, color = &quot;black&quot;) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 20, size = 3, color = &quot;red&quot;) + labs( title = &quot;Distribución de la edad según la respuesta del cliente&quot;, x = &quot;¿Aceptó el depósito a plazo fijo? (y)&quot;, y = &quot;Edad del cliente&quot; ) + theme_bw() + theme( plot.title = element_text(hjust = 0.5), strip.background = element_rect(fill = &quot;gray90&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;) ) El grafico muestra la distribución de la edad de los clientes según su respuesta a la campaña de marketing, diferenciando entre quienes no aceptaron (no) y quienes sí aceptaron (yes) contratar un depósito a plazo fijo. Ambas gráficos muestran un comportamiento similar, donde el \\(50\\%\\) de los clientes alrededor de los 40 años o menos en ambos casos. Tambien, se observa que los clientes que aceptaron la oferta tienden a tener un promedio ligeramente superior en comparación con quienes no aceptaron. Esto podría indicar que los clientes de mayor edad muestran una leve mayor disposición a aceptar el producto. En resumen, aunque hay una leve diferencia por edad, esta variable por sí sola no parece explicar completamente la decisión de aceptar el producto. Ejercicio Realiza el gráfico boxplot para las variables balance, duration, campaing, pdays segun la respuesta de campaña de marketing del banco (y) e interprétalos. Realizaremos la comparacion de las variables numéricas independientes con respecto a la variable y Code # Agrupación por &#39;y&#39; age_y &lt;- df %&gt;% group_by(y) %&gt;% summarise(n = length(age), media = mean(age), ds = sd(age), mediana = median(age), minimo = min(age), maximo = max(age), Q1 = quantile(age, 0.25), Q3 = quantile(age, 0.75), IQR = IQR(age)) %&gt;% mutate(variable = &quot;age&quot;, niveles = as.character(y)) %&gt;% select(variable, niveles, everything(), -y) balance_y &lt;- df %&gt;% group_by(y) %&gt;% summarise(n = length(balance), media = mean(balance), ds = sd(balance), mediana = median(balance), minimo = min(balance), maximo = max(balance), Q1 = quantile(balance, 0.25), Q3 = quantile(balance, 0.75), IQR = IQR(balance)) %&gt;% mutate(variable = &quot;balance&quot;, niveles = as.character(y)) %&gt;% select(variable, niveles, everything(), -y) duration_y &lt;- df %&gt;% group_by(y) %&gt;% summarise(n = length(duration), media = mean(duration), ds = sd(duration), mediana = median(duration), minimo = min(duration), maximo = max(duration), Q1 = quantile(duration, 0.25), Q3 = quantile(duration, 0.75), IQR = IQR(duration)) %&gt;% mutate(variable = &quot;duration&quot;, niveles = as.character(y)) %&gt;% select(variable, niveles, everything(), -y) campaign_y &lt;- df %&gt;% group_by(y) %&gt;% summarise(n = length(campaign), media = mean(campaign), ds = sd(campaign), mediana = median(campaign), minimo = min(campaign), maximo = max(campaign), Q1 = quantile(campaign, 0.25), Q3 = quantile(campaign, 0.75), IQR = IQR(campaign)) %&gt;% mutate(variable = &quot;campaign&quot;, niveles = as.character(y)) %&gt;% select(variable, niveles, everything(), -y) pdays_y &lt;- df %&gt;% group_by(y) %&gt;% summarise(n = length(pdays), media = mean(pdays), ds = sd(pdays), mediana = median(pdays), minimo = min(pdays), maximo = max(pdays), Q1 = quantile(pdays, 0.25), Q3 = quantile(pdays, 0.75), IQR = IQR(pdays)) %&gt;% mutate(variable = &quot;pdays&quot;, niveles = as.character(y)) %&gt;% select(variable, niveles, everything(), -y) # Unión de todas las tablas bind_rows(age_y,balance_y,duration_y,campaign_y,pdays_y) ## # A tibble: 10 × 11 ## variable niveles n media ds mediana minimo maximo Q1 Q3 IQR ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age no 4000 41.0 10.2 39 19 86 33 48 15 ## 2 age yes 521 42.5 13.1 40 19 87 32 50 18 ## 3 balance no 4000 1403. 3075. 420. -3313 71188 61 1407 1346 ## 4 balance yes 521 1572. 2444. 710 -1206 26965 171 2160 1989 ## 5 duration no 4000 226. 210. 167 4 3025 96 283 187 ## 6 duration yes 521 553. 390. 442 30 2769 260 755 495 ## 7 campaign no 4000 2.86 3.21 2 1 50 1 3 2 ## 8 campaign yes 521 2.27 2.09 2 1 24 1 3 2 ## 9 pdays no 4000 36.0 96.3 -1 -1 871 -1 -1 0 ## 10 pdays yes 521 68.6 122. -1 -1 804 -1 98 99 Ejercicio Realiza la interpretación para las variables independientes según y. Realizaremos la comparacion de las variables categóricas independientes con respecto a la variable y Code df %&gt;% group_by(y) %&gt;% count(marital, name = &quot;n&quot;) %&gt;% mutate(categoria = marital, variable = &quot;marital&quot;, porcentaje = (n / sum(n)) * 100) %&gt;% select(y, variable, categoria, n, porcentaje) ## # A tibble: 6 × 5 ## # Groups: y [2] ## y variable categoria n porcentaje ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 no marital divorced 451 11.3 ## 2 no marital married 2520 63 ## 3 no marital single 1029 25.7 ## 4 yes marital divorced 77 14.8 ## 5 yes marital married 277 53.2 ## 6 yes marital single 167 32.1 La distribución del estado civil según la respuesta del marketing muestra diferencias relevantes en los patrones de respuesta. Entre quienes respondieron afirmativamente (yes), el \\(53.17\\%\\) están casados, el \\(32.05\\%\\) solteros y el \\(14.78\\%\\) divorciados. En contraste, en el grupo que respondió negativamente (no), la mayoría (\\(63\\%\\)) también está casada, pero la proporción de solteros (\\(25.73%\\)) y divorciados (\\(11.28\\%\\)) es menor. Esta diferencia sugiere que las personas solteras y divorciadas presentan una mayor disposición o probabilidad de responder afirmativamente frente a la acción evaluada por la variable y, mientras que los casados tienden más a rechazarla, lo que podría ser clave para estrategias de segmentación o análisis de comportamiento. Ejercicio Realiza las tablas de las variables independientes segun variable respuesta y. Interpreta las tablas. Code df_plot &lt;- df %&gt;% group_by(y) %&gt;% count(default, name = &quot;n&quot;) %&gt;% mutate(categoria = default, variable = &quot;default&quot;, porcentaje = (n / sum(n)) * 100) %&gt;% select(y, categoria, n, porcentaje) # Gráfica de barras agrupadas con etiquetas df_plot %&gt;% ggplot(aes(x = categoria, y = n, fill = y)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge(width = 0.9)) + geom_text(aes(label = paste0(n, &quot; (&quot;, round(porcentaje, 1), &quot;%)&quot;)), position = position_dodge(width = 0.9), vjust = -0.3, size = 3) + labs(title = &quot;Historial de impago (default)&quot;, x = &quot;Historial de impago (default)&quot;, y = &quot;Cantidad de clientes&quot; ) + theme_bw() + scale_fill_brewer(palette = &quot;Set1&quot;) Ejercicio Interpreta la gráfica. Realiza las demas gráficas con sus respectivas interpretaciones 3.6 Análisis de estadística paramétrica y no paramétrica El análisis de estadística paramétrica y no paramétrica comprende un conjunto de técnicas inferenciales utilizadas para contrastar hipótesis y evaluar relaciones entre variables. La estadística paramétrica se basa en supuestos específicos sobre la distribución de los datos, como la normalidad y la homogeneidad de varianzas, lo que permite aplicar pruebas como la t de Student, ANOVA y pruebas de proporciones. Por su parte, la estadística no paramétrica ofrece métodos alternativos que no requieren dichos supuestos estrictos, siendo útiles cuando los datos no cumplen condiciones de normalidad o se trata de escalas ordinales. Este enfoque incluye pruebas como la de Wilcoxon, Mann-Whitney, Kruskal-Wallis y Chi-cuadrado. Ambos enfoques permiten tomar decisiones estadísticas robustas y fundamentadas, adaptándose a la naturaleza de los datos analizados. 3.6.1 Comparación de medias entre dos grupos independientes con estadística paramétrica Una de las estrategias más empleadas al comparar una variable cuantitativa entre dos grupos independientes es el contraste de medias. No obstante, observar diferencias en los promedios muestrales no implica automáticamente que exista una diferencia estadísticamente significativa a nivel poblacional. Esto se debe a que cada grupo presenta su propia variabilidad (varianza intrínseca), lo cual puede generar diferencias aparentes por el simple azar muestral. Para evaluar si la diferencia observada es estadísticamente significativa, se recurre a pruebas paramétricas como el test \\(Z\\) (cuando se conoce la desviación estándar poblacional y los tamaños muestrales son grandes) o, más comúnmente, al t-test de Student, que es aplicable cuando las desviaciones estándar poblacionales son desconocidas. Este tipo de pruebas permite: Realizar pruebas de hipótesis, bajo la formulación: \\[ H_0: \\mu_1 = \\mu_2 \\quad \\text{vs} \\quad H_a: \\mu_1 \\ne \\mu_2 \\] Construir intervalos de confianza para estimar la diferencia real entre las medias poblacionales con un nivel de confianza específico (por ejemplo, 95%). 3.6.1.1 Condiciones del t-test para muestras independientes Para que los resultados del t-test sean válidos, deben cumplirse las siguientes condiciones: Independencia: Las observaciones dentro y entre los grupos deben ser independientes. Esto se garantiza si el muestreo es aleatorio y si el tamaño muestral no supera el 10% de la población (cuando no se hace con reemplazo). Normalidad: Las poblaciones deben seguir una distribución normal. Si bien esta condición se formula sobre las poblaciones, en la práctica se evalúa con las muestras. La prueba es robusta frente a desviaciones moderadas de normalidad si cada grupo tiene al menos 30 observaciones, gracias al Teorema del Límite Central. Homogeneidad de varianzas (homocedasticidad): Se asume que ambas poblaciones tienen varianzas iguales. Esta condición puede ser verificada con pruebas como Levene o F de Fisher o Bartless. Si no se cumple, se utiliza una versión alternativa del t-test: el Welch Two Sample t-test, que ajusta los grados de libertad para corregir esta desigualdad, aunque con una ligera pérdida de precisión. 3.6.1.2 Grados de libertad En el t-test clásico (varianzas iguales): \\[df = n_1 + n_2 - 2\\] En el t-test de Welch (varianzas desiguales), los grados de libertad se aproximan con: \\[df \\approx \\frac{ \\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2 }{ \\frac{ \\left( \\frac{s_1^2}{n_1} \\right)^2 }{n_1 - 1} + \\frac{ \\left( \\frac{s_2^2}{n_2} \\right)^2 }{n_2 - 1} }\\] 3.6.1.3 Error estándar de la diferencia de medias El error estándar (SE) para comparar dos medias es: \\[SE = \\sqrt{ \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} }\\] Este valor se utiliza tanto para construir intervalos de confianza como para calcular el estadístico de prueba: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE}\\] Cuando se cumplen las condiciones mencionadas, se puede considerar que la diferencia de medias muestrales sigue una distribución t de Student con los grados de libertad apropiados. En consecuencia, los t-scores reemplazan a los z-scores en los cálculos, ajustando así los valores críticos a la incertidumbre inherente al tamaño muestral. Este enfoque proporciona una herramienta poderosa para evaluar efectos de tratamientos, diferencias entre grupos y pruebas de efectividad, en una amplia gama de contextos experimentales y observacionales. 3.6.1.4 Tamaño del Efecto (Effect Size) en comparaciones de medias El tamaño del efecto es una medida que cuantifica la magnitud de la diferencia observada entre grupos, sin depender del tamaño muestral ni de la inferencia estadística. A diferencia de los p-values, que únicamente indican si la diferencia es estadísticamente significativa bajo una hipótesis nula, el tamaño del efecto ofrece una medida de relevancia práctica o importancia clínica de los resultados. En contextos de comparación de medias mediante t-tests para muestras independientes, dos de las métricas más utilizadas para estimar el tamaño del efecto son: La d de Cohen representa la diferencia estandarizada entre medias, es decir, cuántas desviaciones estándar separan en promedio los grupos comparados: \\[d = \\frac{|\\bar{X}_1 - \\bar{X}_2|}{s_p},\\] donde \\(s_p\\) es la desviación estándar combinada. Existen dos formas comunes de calcularla: La primera es desviación estándar ponderada: \\[s_p = \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2} }\\] y se usa cuando asumes que las varianzas de los dos grupos son iguales. La segunda es el promedio cuadrático simple: \\[s_p = \\sqrt{ \\frac{s_1^2 + s_2^2}{2} }\\] y se usa cuando asumes que las varianzas de los dos grupos son iguales. Además, la interpretacion de valores de \\(d\\) es: \\(d \\leq 0.2\\): tamaño del efecto pequeño \\(d \\approx 0.5\\): tamaño del efecto mediano \\(d \\geq 0.8\\): tamaño del efecto grande \\(r\\) de Pearson (para t-test) Este coeficiente mide la fuerza de asociación entre una variable categórica binaria (grupo) y una variable continua: \\[r = \\sqrt{ \\frac{t^2}{t^2 + gl} }\\] donde: \\(t\\) es el estadístico del t-test. \\(gl\\) son los grados de libertad. La interpretación de valores de \\(r\\): \\(r \\leq 0.1\\): tamaño del efecto pequeño \\(r \\geq 0.3\\): tamaño del efecto mediano \\(r \\geq 0.5\\): tamaño del efecto grande Ejemplo de presión arterial Se ha recolectado una muestra aleatoria de pacientes para evaluar sus niveles de presión arterial. En el archivo datos_medicos.xlsx se encuentran registradas dos variables continuas: PresionSistolica y PresionDiastolica, correspondientes a la presión sistólica y diastólica de cada individuo, respectivamente. Con base en esta información, verifique si existe o no una diferencia significativa entre la presión sistólica y la diastólica en la muestra analizada, y si esta diferencia es relevante desde el punto de vista práctico. Solución Carguemos los datos Code library(readxl) # Lectura de los datos df &lt;- read_excel(&quot;data/datos_medicos.xlsx&quot;) # Ver las 5 primeras obeservaciones head(df) ## # A tibble: 6 × 2 ## Presion Valores ## &lt;chr&gt; &lt;dbl&gt; ## 1 PresionSistolica 114. ## 2 PresionSistolica 118. ## 3 PresionSistolica 136. ## 4 PresionSistolica 121. ## 5 PresionSistolica 121. ## 6 PresionSistolica 137. Ejercicio Realiza el análisis exploratorio del conjunto de datos Verifiquemos si los datos tiene comportamiento normal Code # dataframe de presion sistolica dfps &lt;- df %&gt;% filter(Presion == &quot;PresionSistolica&quot;) # dataframe de presion diastolica dfpd &lt;- df %&gt;% filter(Presion == &quot;PresionDiastolica&quot;) Code # kolmogorov-smirnov ks.test(scale(dfps$Valores),pnorm) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: scale(dfps$Valores) ## D = 0.058097, p-value = 0.8884 ## alternative hypothesis: two-sided Code ks.test(scale(dfpd$Valores),pnorm) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: scale(dfpd$Valores) ## D = 0.057927, p-value = 0.8905 ## alternative hypothesis: two-sided Con una confianza del \\(95\\%\\), vemos que las distribuciones de la presión sistolica (\\(D = 0.058097, p-valor = 0.8884\\)) y presión diastolica (\\(D = 0.057927, p-valor = 0.8905\\)) proviene de una distribución normal Verifiquemos igualdad de varianzas (homocedasticidad): Code var.test(dfps$Valores, dfpd$Valores) ## ## F test to compare two variances ## ## data: dfps$Valores and dfpd$Valores ## F = 0.92784, num df = 99, denom df = 99, p-value = 0.7102 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.6242897 1.3789878 ## sample estimates: ## ratio of variances ## 0.9278405 Con una confianza del \\(95\\%\\), vemos que no diferencias estadísticamente significativa en las varianzas de la presión sistolica y diastolica ($F_{(99,99)}=0.92784, p-valor=0.7102). Teorema de razón de varianzas Si \\(s_1^2\\) y \\(s_2^2\\) son las varianzas de muestras aleatorias independientes de tamaño \\(n_1\\) y \\(n_2\\), tomadas de poblaciones normales con varianzas \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\), respectivamente, entonces, una prueba de hipótesis con nivel de significancia \\(\\alpha\\) para la razón de varianzas \\(\\sigma_1^2 / \\sigma_2^2\\) se basa en el siguiente estadístico de prueba: \\[ F = \\frac{s_1^2}{s_2^2} \\] donde \\(F\\) se distribuye como una variable aleatoria con distribución \\(F\\) de Fisher-Snedecor con \\(v_1 = n_1 - 1\\) y \\(v_2 = n_2 - 1\\) grados de libertad. Además, un intervalo de confianza bilateral al nivel \\(1 - \\alpha\\) para la razón de varianzas \\(\\sigma_1^2 / \\sigma_2^2\\) está dado por: \\[ \\left( \\frac{s_1^2}{s_2^2} \\cdot \\frac{1}{F_{1 - \\alpha/2}(v_1, v_2)},\\ \\frac{s_1^2}{s_2^2} \\cdot \\frac{1}{F_{\\alpha/2}(v_1, v_2)} \\right) \\] donde \\(F_{\\alpha/2}(v_1, v_2)\\) y \\(F_{1 - \\alpha/2}(v_1, v_2)\\) son los cuantiles de la distribución F que dejan un área de \\(\\alpha/2\\) en las colas derecha e izquierda, respectivamente. Ejercicio Usando el teorema de razón de varianzas verifica la salida de la función var.test con los datos de presión arterial. Veamos si hay diferencia significativas o no en el promedio de la presión sistolica y diastolica. Code t.test(dfps$Valores, dfpd$Valores, paired=FALSE, # muestras independientes var.equal = TRUE # homocedasticidad ) ## ## Two Sample t-test ## ## data: dfps$Valores and dfpd$Valores ## t = 31.888, df = 198, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 39.36328 44.55275 ## sample estimates: ## mean of x mean of y ## 120.90406 78.94604 La prueba \\(t\\) para muestras independientes nos indica que con una confianza del 95%, se concluye que existe una diferencia estadísticamente significativa entre las medias de la presión sistólica y la presión diastólica (\\(t_{198} = 31.888, p-valor &lt; 0.001\\)). Veamos el calculo del tamaño del efecto Code library(effsize) cohen.d(dfps$Valores, dfpd$Valores, paired = FALSE) ## ## Cohen&#39;s d ## ## d estimate: 4.509702 (large) ## 95 percent confidence interval: ## lower upper ## 3.984821 5.034583 Con base en el resultado del índice de Cohen’s d, se obtiene un valor estimado de \\(4.51\\) (\\(IC_{95\\%}=(3.98,5.03)\\)), lo que indica un tamaño del efecto extremadamente grande. Esto indica que dicha diferencia no solo es estadísticamente significativa, sino también altamente relevante desde el punto de vista práctico o clínico. Teorema de la diferencia de medias (muestras independientes con varianzas iguales) Sea \\(\\bar{X}_1\\) y \\(\\bar{X}_2\\) las medias muestrales de dos poblaciones independientes, con tamaños \\(n_1\\) y \\(n_2\\), y varianzas poblacionales iguales \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\). Si ambas poblaciones siguen una distribución normal, o si los tamaños muestrales son suficientemente grandes (por el teorema central del límite), entonces la estadística de prueba para contrastar la hipótesis nula: \\[ H_0 : \\mu_1 = \\mu_2 \\quad \\text{vs} \\quad H_1 : \\mu_1 \\neq \\mu_2 \\] está dada por: \\[ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{S_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}} \\] donde \\(S_p^2\\) es la varianza combinada (pooled variance), definida como: \\[ S_p^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2} \\] y \\(S_1^2\\) y \\(S_2^2\\) son las varianzas muestrales. Esta estadística sigue una distribución t de Student con \\(n_1 + n_2 - 2\\) grados de libertad. Además, un intervalo de confianza al nivel \\(1 - \\alpha\\) para la diferencia de medias \\(\\mu_1 - \\mu_2\\) está dado por: \\[ \\left( (\\bar{X}_1 - \\bar{X}_2) \\pm t_{(1 - \\alpha/2,\\; n_1 + n_2 - 2)} \\cdot \\sqrt{S_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} \\right) \\] Ejercicio Usando el teorema de la diferencia de medias verifica la salida de la función t.test con los datos de presión arterial. Aplica el teorema de diferencia de medias con la distribución normal. Ejemplo de comparación de precios por tipo de cuenta Se ha recolectado una muestra aleatoria de cuentas clasificadas en dos tipos: Inversión y Ganancia, con el fin de evaluar si existe una diferencia significativa en el valor promedio de los precios registrados. En la variable Precio se encuentra el valor económico asociado a cada cuenta. En el archivo datos_economia.xlsx Con base en esta información, se propone realizar una prueba estadística para determinar si hay diferencia significativa entre los promedios de precio según el tipo de cuenta, y evaluar si dicha diferencia es relevante desde el punto de vista práctico. Este análisis permitirá tomar decisiones fundamentadas sobre las políticas financieras relacionadas con los tipos de cuenta manejados por la institución. Solución Carguemos los datos Code library(readxl) # Lectura de los datos df &lt;- read_excel(&quot;data/datos_economia.xlsx&quot;) # Ver las 5 primeras observaciones head(df) ## # A tibble: 6 × 2 ## `Tipo de cuenta` Precio ## &lt;chr&gt; &lt;dbl&gt; ## 1 Inversion 4439524. ## 2 Inversion 4769823. ## 3 Inversion 6558708. ## 4 Inversion 5070508. ## 5 Inversion 5129288. ## 6 Inversion 6715065. Ejercicio Realiza el análisis exploratorio del conjunto de datos Verifiquemos si los datos tiene comportamiento normal Code # dataframe de inversion inversion &lt;- df %&gt;% filter(`Tipo de cuenta` == &quot;Inversion&quot;) %&gt;% pull(Precio) # dataframe de ganancia ganancia &lt;- df %&gt;% filter(`Tipo de cuenta` == &quot;Ganancia&quot;)%&gt;% pull(Precio) Code # kolmogorov-smirnov ks.test(scale(inversion),pnorm) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: scale(inversion) ## D = 0.058097, p-value = 0.8884 ## alternative hypothesis: two-sided Code ks.test(scale(ganancia),pnorm) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: scale(ganancia) ## D = 0.057927, p-value = 0.8905 ## alternative hypothesis: two-sided Con una confianza del \\(95\\%\\), vemos que las distribuciones de la inversión (\\(D = 0.058097, p-valor = 0.8884\\)) y la ganancia (\\(D = 0.057927, p-valor = 0.8905\\)) proviene de una distribución normal Verifiquemos igualdad de varianzas (homocedasticidad): Code var.test(Precio ~ `Tipo de cuenta`,data = df) ## ## F test to compare two variances ## ## data: Precio by Tipo de cuenta ## F = 0.044888, num df = 99, denom df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.03020281 0.06671472 ## sample estimates: ## ratio of variances ## 0.04488844 Con una confianza del \\(95\\%\\), vemos que hay diferencias estadísticamente significativa en las varianzas (\\(F_{(99,99)}=0.044, p-valor &lt; 0.001\\)). Ejercicio Aplica las pruebas de Bartlett, Levene y Fligner-Killeen, explica detalladamente cada prueba y cual distribución usa y despues interpreta el resultado. Veamos si hay diferencia significativas o no en el promedio de la presión sistolica y diastolica. Code t.test(inversion, ganancia, var.equal = FALSE, paired=FALSE) ## ## Welch Two Sample t-test ## ## data: inversion and ganancia ## t = 46.212, df = 107.87, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 4126960 4496870 ## sample estimates: ## mean of x mean of y ## 5090405.9 778490.6 La prueba de Welch para muestras independientes nos indica que con una confianza del 95%, se concluye que existe una diferencia estadísticamente significativa entre las medias de la presión sistólica y la presión diastólica (\\(t_{107.87} = 46.212, p-valor &lt; 0.001\\)). Veamos el calculo del tamaño del efecto Code library(effsize) cohen.d(inversion, ganancia, paired = FALSE) ## ## Cohen&#39;s d ## ## d estimate: 6.535323 (large) ## 95 percent confidence interval: ## lower upper ## 5.833174 7.237472 Con base en el resultado del índice de Cohen’s d, se obtiene un valor estimado de \\(6.53\\) (\\(IC_{95\\%}=(5.83,7.23)\\)), lo que indica un tamaño del efecto extremadamente grande. Esto indica que dicha diferencia no solo es estadísticamente significativa, sino también altamente relevante en el tipo de cuenta. 3.6.2 Comparación de medias entre dos grupos pareados con estadística paramétrica Dos medias se consideran dependientes o pareadas cuando provienen de muestras relacionadas, es decir, cuando existe una correspondencia directa entre las observaciones de ambos grupos. Este tipo de diseño es común cuando las mediciones se realizan sobre los mismos individuos bajo dos condiciones diferentes. 3.6.2.1 Ejemplos comunes Comparar el rendimiento de estudiantes en dos pruebas distintas (por ejemplo, lectura y escritura). Evaluar el efecto de un tratamiento médico comparando una variable antes y después del tratamiento en los mismos pacientes. En este contexto, para determinar si hay una diferencia significativa entre las condiciones \\(X\\) e \\(Y\\), se calcula para cada individuo la diferencia: \\[d_i = x_i - y_i\\] Aunque la hipótesis nula plantee que no existe diferencia (es decir, que \\(\\mu_X = \\mu_Y\\)), debido a la variabilidad natural entre observaciones, las diferencias individuales \\(d_i\\) no serán exactamente cero. No obstante, si no hay efecto sistemático, el promedio de estas diferencias tenderá a cero por compensación aleatoria: \\[\\bar{d} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - y_i)\\] El análisis se centra en evaluar si este promedio de diferencias es significativamente distinto de cero, utilizando una prueba t de muestras pareadas. 3.6.2.2 Supuestos del t-test para muestras dependientes Normalidad: Se asume que las diferencias \\(d_i\\) provienen de una distribución normal. Este supuesto puede evaluarse a partir de la muestra si no se tiene información poblacional. Igualdad de varianzas: No es necesario que las varianzas de los grupos originales sean iguales (no se requiere homocedasticidad). Si los supuestos se cumplen, se puede considerar que: \\[ d_i \\sim \\mathcal{N}(\\mu_d, \\sigma_d^2) \\] Como en la mayoría de situaciones de inferencia estadística, los parámetros poblacionales son desconocidos, por lo que se estiman a partir de la muestra. Bajo estos supuestos, se tiene: \\[ \\bar{d} \\sim \\mathcal{N}(\\mu_d, \\hat{\\sigma}_d^2) \\] donde \\(\\bar{d}\\) es el promedio muestral de las diferencias y \\(\\hat{\\sigma}_d^2\\) es la varianza muestral de dichas diferencias. El test \\(t\\) pareado se utiliza para verificar la hipótesis: \\[ H_0: \\mu_d = 0 \\quad \\text{vs} \\quad H_1: \\mu_d \\neq 0 \\] La estadística de prueba está dada por: \\[ t = \\frac{\\bar{d}}{s_d / \\sqrt{n}} \\sim t_{n-1} \\] donde \\(\\bar{d}\\): media de las diferencias \\(s_d\\): desviación estándar de las diferencias \\(n\\): número de pares Ejemplo: Evaluación del desempeño en una intervención educativa Una institución educativa implementa una nueva estrategia de enseñanza con el objetivo de mejorar el desempeño de los estudiantes en una prueba estandarizada. Para evaluar su efectividad, se selecciona aleatoriamente una muestra de 10 estudiantes y se registra su tiempo (en segundos) para completar una tarea cognitiva específica al inicio del periodo académico. Al finalizar el año, se repite la misma medición con los mismos estudiantes. Este diseño corresponde a un esquema de medidas pareadas, ya que las observaciones antes y después de la intervención se realizan sobre los mismos individuos. Por tanto, el análisis estadístico adecuado consiste en aplicar una prueba t para muestras dependientes, que permita evaluar si la estrategia implementada produjo un cambio significativo en el desempeño de los estudiantes. Code # Datos de tiempos antes y después de la intervención educativa datos &lt;- data.frame( estudiante = c(1:10), antes = c(12.9, 13.5, 12.8, 15.6, 17.2, 19.2, 12.6, 15.3, 14.4, 11.3), despues = c(12.7, 13.6, 12.0, 15.2, 16.8, 20.0, 12.0, 15.9, 16.0, 11.1) ) Con base en estas mediciones, se procederá a aplicar la prueba estadística para determinar si existe evidencia significativa de mejora en los tiempos registrados tras la implementación de la nueva metodología. Solución Ejercicio Realiza el análisis exploratorio del conjunto de datos Veamos si los datos tiene comportamiento normal: Code # prueba de normalidad shapiro.test(datos$antes) ## ## Shapiro-Wilk normality test ## ## data: datos$antes ## W = 0.94444, p-value = 0.6033 Code shapiro.test(datos$despues) ## ## Shapiro-Wilk normality test ## ## data: datos$despues ## W = 0.93638, p-value = 0.5135 La prueba de Shapiro_Will, nos muestra que la intervención de antes (\\(W = 0.9444, p-valor = 0.6033\\)) y despues (\\(W = 0.93638, p-valor = 0.5135\\)) tienen comportamiento normal. Veamos si hay diferencia significativas en los promedios de las intervenciones de antes y despues Code t.test(x = datos$despues, y = datos$antes, paired = TRUE) ## ## Paired t-test ## ## data: datos$despues and datos$antes ## t = 0.21331, df = 9, p-value = 0.8358 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -0.4802549 0.5802549 ## sample estimates: ## mean difference ## 0.05 La prueba \\(t\\) nos indica que no hay diferencias estadísticamente significativa en los promedios de las intervenciones de antes y despues (\\(t_{9}=0.2133, p-valor = 0.8358\\)) Ejercicio Usando el teorema de la diferencia de medias para datos pareados verifica la salida de la función t.test con los datos de la intervención educativa Calculemos el tamaño del efecto Code cohen.d(d = datos$antes, f = datos$despues, paired = TRUE) ## ## Cohen&#39;s d ## ## d estimate: -0.0169815 (negligible) ## 95 percent confidence interval: ## lower upper ## -0.1842481 0.1502851 El resultado del tamaño del efecto calculado mediante Cohen’s d para muestras pareadas fue de \\(-0.017\\) (\\(IC_{95\\%}=(-0.184,0.150)\\)), lo cual es prácticamente inexistente desde el punto de vista práctico. El intervalo de confianza nos confirma que no hay evidencia de un cambio significativo ni relevante en los tiempos registrados antes y después de la intervención. 3.6.3 Comparación de dos grupos independientes con estadística no paramétrica El test de Mann–Whitney–Wilcoxon (WMW), también conocido como Wilcoxon rank-sum test o U-test de Mann–Whitney, es una prueba estadística no paramétrica utilizada para comparar dos muestras independientes. Su objetivo es determinar si ambas proceden de poblaciones con distribuciones similares, sin asumir normalidad ni trabajar directamente con las medias, como ocurre en los test t. 3.6.3.1 Fundamento conceptual La idea central del test es la siguiente: si dos muestras provienen de poblaciones con la misma distribución, al combinar todas las observaciones y ordenarlas de menor a mayor, se esperaría que los valores de ambas muestras estén aleatoriamente intercalados. En cambio, si una de las muestras tiende a tener valores sistemáticamente mayores o menores, sus observaciones tenderán a agruparse hacia un extremo del ordenamiento. Desde una perspectiva probabilística, el test contrasta si la probabilidad de que una observación de una población sea mayor que una de la otra es igual a 0.5: \\[ H_0: P(X &gt; Y) = 0.5 \\quad \\text{(no hay diferencia entre grupos)} \\] \\[ H_1: P(X &gt; Y) \\neq 0.5 \\quad \\text{(hay diferencia entre grupos)} \\] Este planteamiento no presupone normalidad ni igualdad de medias, sino que se basa en la equidistribución de las poblaciones. 3.6.3.2 Interpretación y alcance Aunque es frecuente leer que el test de Mann–Whitney–Wilcoxon compara medianas, esta interpretación solo es válida si ambas poblaciones tienen la misma forma de distribución (es decir, misma asimetría y varianza). En general, lo que el test evalúa es una diferencia en tendencias centrales sin especificar la medida exacta (media o mediana). Nota técnica: ¿El test WMW compara medianas? Si bien se suele afirmar que el test de Mann–Whitney–Wilcoxon compara medianas, esto es estrictamente cierto solo cuando ambas poblaciones tienen la misma forma de distribución. Es decir, se requiere que presenten igual dispersión, simetría y curtosis. En ese caso, una diferencia en la ubicación central puede interpretarse como una diferencia de medianas. Sin embargo, si las distribuciones difieren en forma, el test detecta diferencias en la distribución global, y no exclusivamente en la mediana. Por tanto, el test WMW evalúa diferencias en localización general, y su interpretación como comparación de medianas debe hacerse con cautela, y solo bajo condiciones que aseguren homogeneidad de forma entre grupos. 3.6.3.3 Comparación con el test t El test WMW suele ser menos potente que el t-test cuando los supuestos del análisis paramétrico se cumplen (la pérdida de potencia se estima en torno al 5%). Esto se debe a que el WMW trabaja con rangos y no con valores reales, lo que reduce su sensibilidad frente a diferencias pequeñas. Sin embargo, esta misma característica le confiere mayor robustez ante valores atípicos y violaciones de normalidad, convirtiéndolo en una alternativa preferible en contextos donde los datos no cumplen los supuestos clásicos. 3.6.3.4 Supuestos y condiciones de aplicación Para aplicar correctamente el test de Mann–Whitney–Wilcoxon se deben cumplir las siguientes condiciones: Independencia de las observaciones entre los dos grupos. Los datos deben ser al menos ordinales, es decir, deben poder ordenarse de menor a mayor. No se requiere normalidad ni homocedasticidad estricta. Para interpretar el test como una comparación de medianas, se requiere que ambas poblaciones tengan distribuciones con forma similar (igual dispersión y simetría). Es preferible que las muestras tengan tamaños comparables, aunque no es obligatorio. El test de Mann–Whitney–Wilcoxon es una herramienta flexible y robusta para comparar dos muestras independientes cuando no se puede asumir normalidad o cuando existen valores extremos. A cambio de una ligera pérdida de potencia frente al test t, ofrece mayor fiabilidad en contextos no paramétricos y mantiene una base sólida para contrastar diferencias de ubicación entre grupos. Ejemplo de peso al nacer y hábito de fumar Se ha recolectado una muestra representativa de nacimientos contenida en el conjunto de datos births, disponible en el paquete openintro de R. Dentro de este conjunto, se encuentran registradas variables como el peso al nacer (en onzas) y si la madre fumó o no durante el embarazo (variable categórica smoke: “yes” o “no”). Con base en esta información, verifique si existe o no una diferencia significativa en el peso al nacer entre los recién nacidos cuyas madres fumaron durante el embarazo y aquellos cuyas madres no lo hicieron. Además, evalúe si esta diferencia es relevante desde el punto de vista práctico. Solución Carguemos los datos Code library(openintro) # Cargar los datos data(&quot;births&quot;) # Asignar a un data frame df &lt;- births # Ver las 5 primeras observaciones head(df) ## # A tibble: 6 × 9 ## f_age m_age weeks premature visits gained weight sex_baby smoke ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker Ejercicio Realiza el análisis exploratorio del conjunto de datos Veamos si tienen comportamiento normal Code library(nortest) df %&gt;% group_by(smoke) %&gt;% summarise(n = length(weight), est_ks = ks.test(scale(weight),&#39;pnorm&#39;)$statistic, p_ks = ks.test(scale(weight),&#39;pnorm&#39;)$p.value, estsw = shapiro.test(weight)$statistic, p_sw = shapiro.test(weight)$p.value, est_lt = lillie.test(weight)$statistic, p_lt = lillie.test(weight)$p.value) ## Warning: There were 4 warnings in `summarise()`. ## The first warning was: ## ℹ In argument: `est_ks = ks.test(scale(weight), &quot;pnorm&quot;)$statistic`. ## ℹ In group 1: `smoke = nonsmoker`. ## Caused by warning in `ks.test.default()`: ## ! ties should not be present for the one-sample Kolmogorov-Smirnov test ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 3 remaining warnings. ## # A tibble: 2 × 8 ## smoke n est_ks p_ks estsw p_sw est_lt p_lt ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 100 0.132 0.0604 0.924 0.0000223 0.132 0.000184 ## 2 smoker 50 0.125 0.413 0.895 0.000328 0.125 0.0484 El test de Lilliefors evidencia que el peso de los recién nacidos no presenta un comportamiento normal en ninguno de los grupos analizados. En el caso de los hijos de madres que no fuman(\\(Lilliefors = 0.132, p-valor &lt; 0.001\\)); mientras que para los hijos de madres que fuman (\\(Lilliefors = 0.125, p-valor = 0.048\\)). Dado que no se cumple con el supuesto de normalidad en los grupos analizados, la comparación de varianzas debe realizarse mediante pruebas robustas a esta condición. En este contexto, se recomienda emplear el test de Levene o el test no paramétrico de Fligner-Killeen, ambos diseñados para evaluar la homogeneidad de varianzas y que utilizan la mediana como medida de tendencia central, lo que los hace adecuados en presencia de datos no normales. Veamos la igualdad de varianza Code library(car) leveneTest(weight ~ smoke, data = df, center = &quot;median&quot;) ## Levene&#39;s Test for Homogeneity of Variance (center = &quot;median&quot;) ## Df F value Pr(&gt;F) ## group 1 0.4442 0.5062 ## 148 La prueba de Levene nos indica no se encuentran diferencias significativas en la varianza del peso entre los recién nacidos de madres fumadoras y no fumadoras (\\(F_{1}=0.4442, p-valor=0.5062\\)). Veamos si hay diferencia en los grupos de madres: Code library(rstatix) df %&gt;% wilcox_test(weight ~ smoke, paired = FALSE) ## # A tibble: 1 × 7 ## .y. group1 group2 n1 n2 statistic p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 weight nonsmoker smoker 100 50 2879 0.131 La prueba de U de Mann-Whitney, nos muestra que no hay evidencia estadísticamente significativa de una diferencia en el peso de los recien nacidos entre madres fumadoras y no fumadoras (\\(U = 2879, p-valor = 0.131\\)). Code library(rstatix) df %&gt;% wilcox_effsize(weight ~ smoke, paired = FALSE) ## # A tibble: 1 × 7 ## .y. group1 group2 effsize n1 n2 magnitude ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt; ## 1 weight nonsmoker smoker 0.123 100 50 small Es claro que si no hay diferencia estadísticamente en los grupos, el efecto es pequeño o no existe. Ejercicio ¿Puedes usar una prueba paramétrica?. En caso afirmativo, ejecutela 3.6.4 Comparación de dos grupos pareados con estadística no paramétrica La prueba de los rangos con signo de Wilcoxon (Wilcoxon signed-rank test) es una herramienta estadística no paramétrica utilizada para comparar dos muestras relacionadas. Su uso es especialmente recomendable cuando los datos no cumplen con los supuestos de normalidad requeridos por pruebas paramétricas como el t-test para muestras pareadas. Esta prueba resulta útil cuando la distribución de las diferencias entre pares de observaciones muestra asimetría, colas pesadas o cuando el tamaño muestral es demasiado reducido como para validar la normalidad mediante métodos gráficos o contrastes de hipótesis. 3.6.4.1 Condiciones para aplicar la prueba de Wilcoxon signed-rank Antes de aplicar esta prueba, deben cumplirse las siguientes condiciones: Dependencia entre las muestras: los datos deben provenir de observaciones emparejadas o relacionadas (por ejemplo, mediciones antes y después sobre los mismos individuos). Datos ordenables: los valores deben ser ordinales o continuos, de manera que puedan ordenarse de menor a mayor (o viceversa). No se requiere normalidad, pero sí simetría en las diferencias: no es necesario que las diferencias entre pares sigan una distribución normal, pero sí se espera que tengan una distribución simétrica alrededor de un valor central (típicamente cero). Opera sobre medianas: a diferencia del t-test, que se basa en la media, el Wilcoxon signed-rank test evalúa la simetría de las diferencias, lo cual suele interpretarse como un contraste sobre la mediana de las diferencias. Es preferible al t-test cuando: Hay valores atípicos en los datos No se cumple el criterio de normalidad El tamaño de muestra es pequeño 3.6.4.2 ¿Cuándo se recomienda utilizar esta prueba? Existen dos situaciones clave en las que es más apropiado usar el Wilcoxon signed-rank test en lugar del t-test pareado: Cuando se detecta no normalidad: Si los datos permiten identificar que las diferencias no se distribuyen normalmente (mediante gráficos o contrastes), el t-test deja de ser adecuado. En estos casos, el test de Wilcoxon constituye una alternativa robusta. Otras opciones también válidas son el bootstrapping, la regresión cuantílica o los tests de permutación. Cuando no se puede verificar la distribución: Si el tamaño de muestra es tan pequeño que no permite evaluar con certeza la forma de la distribución, y no se dispone de evidencia previa sobre su comportamiento, es más prudente aplicar el Wilcoxon signed-rank test, ya que no requiere asumir normalidad. Características principales Evalúa si las diferencias entre pares siguen una distribución simétrica alrededor de cero. Se basa en los rangos de las diferencias absolutas, por lo que no utiliza los valores originales sino su orden. Es aplicable a variables que se puedan ordenar (ordinales o continuas). Posee menor poder estadístico que el t-test en datos normales, ya que ignora la magnitud exacta de los valores extremos. Sin embargo, esto lo hace más robusto frente a valores atípicos y distribuciones no normales. Consideraciones finales El Wilcoxon signed-rank test es una opción sólida para el análisis de muestras emparejadas cuando los supuestos del t-test no se cumplen. No obstante, debe tenerse presente que un tamaño muestral pequeño limita la inferencia estadística, independientemente de la prueba empleada. Por ello, el análisis debe acompañarse de juicio experto y de otras fuentes de información cuando estén disponibles. Ejemplo Un grupo de participantes fue evaluado antes y después de una intervención formativa mediante un cuestionario estructurado. El ítem Q1 del cuestionario mide una dimensión específica del desempeño o conocimiento relacionado con la temática abordada en la intervención. A cada participante se le asignó una puntuación en dos momentos temporales: durante la fase de Pretest (antes de la intervención) y durante la fase de Posttest (después de la intervención). El propósito de este análisis es determinar si la intervención generó un cambio significativo en las respuestas al ítem Q1. Para ello, se aplicará una prueba estadística que permita comparar las puntuaciones obtenidas en el pretest y el posttest para cada participante, evaluando si dichas diferencias son estadísticamente significativas. Solución Carguemos el conjunto de datos Code url_dat &lt;- &quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vQaVafuOSuEnOIiJJoB_OLF6GHib4EGqtAPnFBkNXFj29iB8yex4wYXYAAyIW16eA/pub?gid=1616716040&amp;single=true&amp;output=tsv&quot; datos &lt;- read.delim(url_dat) datos &lt;- datos %&gt;% mutate(across(c(Experimento, Genero,Nivel.educativo), as.factor)) Ejercicio Realiza el análisis exploratorio del conjunto de datos Veamos si tienen comportamiento normal Code datos %&gt;% group_by(Experimento) %&gt;% summarise(n = length(Q1), est_ks = ks.test(scale(Q1),&#39;pnorm&#39;)$statistic, p_ks = ks.test(scale(Q1),&#39;pnorm&#39;)$p.value, estsw = shapiro.test(Q1)$statistic, p_sw = shapiro.test(Q1)$p.value, est_lt = lillie.test(Q1)$statistic, p_lt = lillie.test(Q1)$p.value) ## Warning: There were 4 warnings in `summarise()`. ## The first warning was: ## ℹ In argument: `est_ks = ks.test(scale(Q1), &quot;pnorm&quot;)$statistic`. ## ℹ In group 1: `Experimento = Post-test`. ## Caused by warning in `ks.test.default()`: ## ! ties should not be present for the one-sample Kolmogorov-Smirnov test ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 3 remaining warnings. ## # A tibble: 2 × 8 ## Experimento n est_ks p_ks estsw p_sw est_lt p_lt ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Post-test 49 0.410 0.000000143 0.645 0.00000000123 0.410 2.43e-23 ## 2 Pretest 49 0.324 0.0000688 0.791 0.000000652 0.324 3.28e-14 La prueba de Shapiro-Will nos muestra que los experimentos del pretest (\\(W = 0.7905, p-valor &lt; 0.001\\)) y postest (\\(W = .6447, p-valor &lt; 0.001\\)) no tienen comportamiento normal. Veamos si hay diferencia de la pregunta 1 segun el experimento: Code datos %&gt;% rstatix::wilcox_test(Q1 ~ Experimento, paired = TRUE) ## # A tibble: 1 × 7 ## .y. group1 group2 n1 n2 statistic p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Q1 Post-test Pretest 49 49 1128 0.00000000145 La prueba de los rangos con signo de Wilcoxon para muestras pareadas nos indica que la intervención produjo un cambio estadísticamente significativo en las respuestas al ítem Q1 (\\(W=1128, p-valor &lt; 0.001\\)). La dirección de este cambio sugiere una mejora en el desempeño de los participantes, dado que las puntuaciones en el post-test son consistentemente mayores que en el pretest. Code datos %&gt;% rstatix::wilcox_effsize(Q1 ~ Experimento, paired = TRUE) ## # A tibble: 1 × 7 ## .y. group1 group2 effsize n1 n2 magnitude ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt; ## 1 Q1 Post-test Pretest 0.879 49 49 large Además del resultado estadísticamente significativo, el tamaño del efecto obtenido para la comparación entre las puntuaciones de Q1 en el pretest y el post-test fue de \\(r = 0.8794\\), lo cual representa un efecto grande. Esto indica que el cambio observado no solo es significativo, sino que también tiene relevancia práctica considerable sobre el desempeño de los participantes tras la intervención. 3.7 ANOVA (análisis de varianza para comparar múltiples medias) La técnica de análisis de varianza (ANOVA), también conocida como análisis factorial, fue desarrollada por Ronald Fisher en la década de 1930 y constituye una herramienta estadística fundamental para evaluar el efecto de uno o más factores (cada uno con dos o más niveles) sobre la media de una variable continua. ANOVA se utiliza cuando se desea comparar simultáneamente las medias de dos o más grupos, evaluando si las diferencias observadas son estadísticamente significativas. Además, esta técnica puede extenderse para analizar no solo los efectos principales de los factores, sino también sus posibles interacciones y, en ciertos casos, el efecto de los factores sobre la variabilidad (varianza) de la variable dependiente. 3.7.1 Análisis de varianza de un factor 3.7.1.1 Supuestos y formulación de las hipótesis Se considera que existen \\(k\\) poblaciones distintas, denominadas tratamientos o categorías, cada una de las cuales está normalmente distribuida con medias \\(\\mu_1, \\mu_2, \\dots, \\mu_k\\), y comparten una misma varianza común \\(\\sigma^2\\). Para representarlas, se extraen muestras aleatorias independientes de tamaños \\(n_1, n_2, \\dots, n_k\\), respectivamente. A lo largo del análisis se utiliza la notación \\(y_{ij}\\) para denotar la i-ésima observación dentro del j-ésimo grupo o tratamiento, facilitando así el estudio comparativo entre medias categorizadas. Bajo este esquema, los datos pueden organizarse como se muestra en la Tabla 3.3, lo cual permite aplicar la técnica de análisis de varianza de un factor para contrastar la hipótesis de igualdad entre medias poblacionales (Solano 2019). Tabla 3.3: Observaciones muestrales de muestras aleatorias independientes de \\(k\\) poblaciones Población 1 Población 2 Población 3 … Población \\(k\\) \\(y_{11}\\) \\(y_{12}\\) \\(y_{13}\\) … \\(y_{1k}\\) \\(y_{21}\\) \\(y_{22}\\) \\(y_{23}\\) … \\(y_{2k}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) … \\(\\vdots\\) \\(y_{n_{1},1}\\) \\(y_{n_{2},2}\\) \\(y_{n_{3},3}\\) … \\(y_{n_{k},k}\\) Entonces, el procedimiento para contrastar la hipótesis de igualdad de medias en este contexto se denomina análisis de varianza de un factor, una terminología que se hará más clara cuando tratemos otros modelos de análisis de varianza. Definición (Hipótesis en el análisis de varianza de un factor) Supongamos que tenemos muestras aleatorias independientes con tamaños \\(n_1, n_2, \\ldots, n_k\\). Si representamos las medias poblacionales por \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\), el análisis de varianza de un factor está diseñado para contrastar la hipótesis nula de que todas las medias poblacionales son iguales, es decir: \\[ H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\quad \\text{vs} \\quad H_1: \\text{Al menos dos medias son diferentes}. \\] A partir de la definición anterior, en este apartado desarrollaremos el contraste de la hipótesis nula de igualdad de medias entre \\(k\\) poblaciones, considerando que se dispone de muestras aleatorias independientes para cada una de ellas. Con el fin de facilitar los cálculos y el análisis posterior, los datos serán organizados en una tabla estructurada por categorías, como se muestra a continuación. Tabla 3.4: Observaciones muestrales de muestras aleatorias independientes de \\(k\\) poblaciones Población 1 Población 2 Población 3 … Población \\(k\\) Total Muestras \\(y_{11}\\) \\(y_{12}\\) \\(y_{13}\\) \\(\\cdots\\) \\(y_{1k}\\) \\(y_{21}\\) \\(y_{22}\\) \\(y_{23}\\) \\(\\cdots\\) \\(y_{2k}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\cdots\\) \\(\\vdots\\) \\(y_{n_{1},1}\\) \\(y_{n_{2},2}\\) \\(y_{n_{3},3}\\) \\(\\cdots\\) \\(y_{n_{k},k}\\) Tamaño \\(n_1\\) \\(n_2\\) \\(n_3\\) \\(\\cdots\\) \\(n_k\\) \\(N\\) Sumas \\(T_1\\) \\(T_2\\) \\(T_3\\) \\(\\cdots\\) \\(T_k\\) \\(T\\) Medias \\(\\bar{y}_1\\) \\(\\bar{y}_2\\) \\(\\bar{y}_3\\) \\(\\cdots\\) \\(\\bar{y}_k\\) \\(\\bar{y}\\) donde \\(y_{ij}\\) es la i-ésima observación del tratamiento \\(j\\). \\(n_j\\) es el tamaño de la j-ésima muestra. \\(N = n_1 + n_2 + \\cdots + n_k\\) es la suma de todos los tamaños de muestra. \\(T_j = y_{1j} + y_{2j} + \\cdots + y_{n_j,j}\\) es la suma de las observaciones de la muestra j-ésima. \\(T = T_1 + T_2 + \\cdots + T_k\\) es la suma de todas las observaciones. \\(\\bar{y}_j = \\frac{T_j}{n_j}\\) es la media de las observaciones de la muestra j-ésima. \\(\\bar{y} = \\frac{T}{N}\\) es la media total de todas las observaciones. 3.7.1.2 Sumas de cuadrados y teorema de descomposición El contraste de igualdad de medias se fundamenta en la comparación entre dos fuentes de variabilidad en los datos muestrales: La primera es la variabilidad en torno a las medias muestrales individuales de los \\(k\\) grupos de observaciones. Esta se denomina variabilidad dentro de los grupos. La segunda es la variabilidad entre las medias de los \\(k\\) grupos. Esta se denomina variabilidad entre grupos. A continuación, definiremos medidas cuantitativas para ambos tipos de variabilidad. Definición. Variabilidad dentro de los grupos Es la variación de las observaciones individuales respecto a su propia media muestral dentro de cada grupo. Para el grupo \\(j = 1, \\ldots, k\\) , se define como: \\[ (SE)_j = \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 \\] Definición. Suma de cuadrados del error (SSE) Es la suma total de la variabilidad dentro de todos los grupos. También se conoce como suma de cuadrados dentro de los grupos y se define como: \\[ SSE = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 \\] Definición. Suma de cuadrados entre grupos (SSA) En relación con la variabilidad entre grupos, que simbolizaremos como SSA, una medida natural consiste en calcular las diferencias entre las medias muestrales de cada grupo y la media muestral global. Estas diferencias se elevan al cuadrado para representar la desviación de cada grupo respecto a la media global. Como cada grupo \\(j\\) tiene un tamaño \\(n_j\\), se pondera cada diferencia por dicho tamaño. Así, la suma de cuadrados entre grupos (también llamada suma de cuadrados de tratamientos) se define como: \\[ SSA = \\sum_{j=1}^{k} n_j (\\bar{y}_j - \\bar{y})^2 \\] donde \\(\\bar{y}_j\\) es la media muestral del grupo \\(j\\), \\(\\bar{y}\\) es la media muestral global, \\(n_j\\) es el tamaño de la muestra del grupo \\(j\\), \\(k\\) es el número total de grupos. Definición. Suma de cuadrados total (SST) Es la variabilidad total de todas las observaciones con respecto a la media global. Se define como: \\[ SST = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y})^2 \\] Teorema. Descomposición de la suma de cuadrados Supongamos que tenemos muestras aleatorias independientes de tamaños \\(n_1, n_2, \\ldots, n_k\\), correspondientes a \\(k\\) poblaciones. Sean \\(y_{ij}\\) la i-ésima observación muestral en el grupo \\(j\\), \\(\\bar{y}_1, \\bar{y}_2, \\ldots, \\bar{y}_k\\) las medias muestrales por grupo, y \\(\\bar{y}\\) la media muestral global. Definimos las siguientes sumas de cuadrados: Suma de cuadrados total (SST): \\[ SST = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y})^2 = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} y_{ij}^2 - \\frac{T^2}{N} \\] Suma de cuadrados entre grupos (SSA): \\[ SSA = \\sum_{j=1}^{k} (\\bar{y}_j - \\bar{y})^2 n_j = \\sum_{j=1}^{k} \\frac{T_j^2}{n_j} - \\frac{T^2}{N} \\] Suma de cuadrados dentro de los grupos (SSE): \\[ SSE = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 \\] Entonces, se cumple que: \\[ SST = SSA + SSE \\] Demostración del teorema Recordemos que la suma total de cuadrados se define como: \\[ SST = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y})^2 \\] Descomponemos la diferencia \\(y_{ij} - \\bar{y}\\): \\[ y_{ij} - \\bar{y} = (y_{ij} - \\bar{y}_j) + (\\bar{y}_j - \\bar{y}) \\] Elevamos al cuadrado y aplicamos la identidad \\[ (y_{ij} - \\bar{y})^2 = (y_{ij} - \\bar{y}_j)^2 + 2(y_{ij} - \\bar{y}_j)(\\bar{y}_j - \\bar{y}) + (\\bar{y}_j - \\bar{y})^2 \\] Sumamos sobre \\(i\\) y \\(j\\): \\[ SST = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 + 2 \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)(\\bar{y}_j - \\bar{y}) + \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (\\bar{y}_j - \\bar{y})^2 \\] Notemos que \\(\\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j) = 0\\), por lo tanto la segunda suma es cero. En la tercera suma, \\((\\bar{y}_j - \\bar{y})^2\\) no depende de \\(i\\), así que: \\[ \\sum_{i=1}^{n_j} (\\bar{y}_j - \\bar{y})^2 = n_j (\\bar{y}_j - \\bar{y})^2 \\] Entonces: \\[ SST = \\underbrace{\\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2}_{SSE} + \\underbrace{\\sum_{j=1}^{k} n_j (\\bar{y}_j - \\bar{y})^2}_{SSA} \\] Por tanto, se cumple la relación de descomposición: \\[ SST = SSE + SSA \\] ¿Por qué \\(\\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j) = 0\\)? Vamos a analizar el término interno: \\[\\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)\\] Este representa la suma de las desviaciones de cada valor respecto a su propia media grupal. Es decir, por definición tenemos que \\[\\bar{y}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij} \\quad \\Rightarrow \\quad \\sum_{i=1}^{n_j} y_{ij} = n_j \\bar{y}_j.\\] Entonces \\[\\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j) = \\sum_{i=1}^{n_j} y_{ij} - \\sum_{i=1}^{n_j} \\bar{y}_j = \\sum_{i=1}^{n_j} y_{ij} - n_j \\bar{y}_j = n_j \\bar{y}_j - n_j \\bar{y}_j = 0\\] Esto se cumple para cada grupo \\(j\\). Luego \\[\\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)(\\bar{y}_j - \\bar{y}) = (\\bar{y}_j - \\bar{y}) \\cdot \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j) = (\\bar{y}_j - \\bar{y}) \\cdot 0 = 0.\\] 3.7.1.3 Estimaciones insesgadas de la varianza poblacional El contraste de igualdad de medias en el análisis de varianza se fundamenta en el supuesto de que las \\(k\\) poblaciones involucradas comparten una varianza poblacional común. Si la hipótesis nula de igualdad de medias es verdadera, entonces tanto la suma de cuadrados entre grupos (SSA) como la suma de cuadrados dentro de los grupos (SSE) pueden considerarse como bases válidas para estimar dicha varianza común. No obstante, para obtener estimaciones adecuadas, es necesario dividir cada suma de cuadrados entre sus respectivos grados de libertad, conforme se establece en el siguiente teorema. Teorema 4.1.3 Supongamos que tenemos muestras aleatorias independientes de tamaños \\(n_1, n_2, \\ldots, n_k\\), correspondientes a \\(k\\) poblaciones con varianzas iguales (\\(\\sigma^2\\)). Sea \\(N\\) el tamaño muestral total, de manera que \\(N = n_1 + n_2 + \\cdots + n_k\\). Sean \\(SSA\\) y \\(SSE\\) como en el teorema 4.1.2. Entonces, dos estimaciones insesgadas de \\(\\sigma^2\\) son las siguientes: Cuadrado medio entre los grupos (o del tratamiento): \\[ MSA = \\frac{SSA}{k - 1} \\] Cuadrado medio dentro de los grupos (o del error): \\[ MSE = \\frac{SSE}{N - k} \\] Demostración del teorema Primero probemos que \\(\\mathbb{E}[MSE] = \\sigma^2\\). Sabemos que: \\[SSE = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2 = \\sum_{j=1}^k (n_j - 1) s_j^2, \\quad \\left( s=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}{n-1}\\right)\\] Cada \\(s_j^2\\) es un estimador insesgado de la varianza poblacional por lo que \\[\\mathbb{E}[s_j^2] = \\sigma^2 \\Rightarrow \\mathbb{E}[(n_j - 1) s_j^2] = (n_j - 1) \\sigma^2\\] luego \\[\\mathbb{E}[SSE] = \\sum_{j=1}^{k} \\mathbb{E}[(n_j - 1) s_j^2] = \\sum_{j=1}^k (n_j - 1) \\sigma^2 = (N - k) \\sigma^2.\\] En consecuencia \\[\\mathbb{E}[MSE] = \\mathbb{E} \\left[ \\frac{SSE}{N - k} \\right] = \\frac{\\mathbb{E}[SSE]}{N - k} = \\frac{(N - k)\\sigma^2}{N - k} = \\sigma^2\\] Esto preuba que MSE es un estimador insesgado de \\(\\sigma^2\\). De otro lado, probemos que \\(\\mathbb{E}[MSA] = \\sigma^2\\). En efecto, supongamos que se tienen \\(k\\) grupos independientes, cada uno con \\(n_j\\) observaciones: \\[y_{ij} = \\mu + \\tau_j + \\epsilon_{ij} \\quad \\text{con } \\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2), \\text{ independientes}\\] donde: \\(y_{ij}\\): i-ésima observación del grupo j \\(\\mu\\): media global general \\(\\tau_j\\): efecto del tratamiento j (con \\(\\sum_{j=1}^k \\tau_j = 0\\)) \\(\\epsilon_{ij}\\): error aleatorio con varianza común \\(\\sigma^2\\) y supongamos que \\[H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_k = 0\\] Sabemos que la suma de cuadrados entre tratamientos (SSA) se define como: \\[SSA = \\sum_{j=1}^{k} n_j (\\bar{y}_j - \\bar{y})^2\\] donde \\(\\bar{y}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}\\): media del grupo \\(j\\) \\(\\bar{y} = \\frac{1}{N} \\sum_{j=1}^k \\sum_{i=1}^{n_j} y_{ij}\\): media global, con \\(N = \\sum_{j=1}^k n_j.\\) Entonces \\(\\mathbb{E}[\\bar{y}_j] = \\mu + \\tau_j\\), \\(\\mathbb{E}[\\bar{y}] = \\mu \\quad \\text{(ya que } \\sum \\tau_j = 0 \\text{)}\\) por lo que la expresión en términos de variables aleatorias \\[\\mathbb{E}[SSA] = \\mathbb{E}\\left[ \\sum_{j=1}^k n_j (\\bar{y}_j - \\bar{y})^2 \\right],\\] expandiendo el cuadrado \\[ (\\bar{y}_j - \\bar{y})^2 = \\bar{y}_j^2 - 2\\bar{y}_j\\bar{y} + \\bar{y}^2 \\] luego \\[ SSA = \\sum_{j=1}^k n_j \\left(\\bar{y}_j^2 - 2\\bar{y}_j\\bar{y} + \\bar{y}^2\\right) = \\sum_{j=1}^k n_j \\bar{y}_j^2 - 2\\bar{y} \\sum_{j=1}^k n_j \\bar{y}_j + N \\bar{y}^2 \\] Sabemos que \\[ \\sum_{j=1}^k n_j \\bar{y}_j = N \\bar{y} \\] y por lo tanto \\[ SSA = \\sum_{j=1}^k n_j \\bar{y}_j^2 - N \\bar{y}^2 \\] Ahora, la esperanza de SSA \\[ \\mathbb{E}[SSA] = \\sum_{j=1}^k n_j \\mathbb{E}[\\bar{y}_j^2] - N \\mathbb{E}[\\bar{y}]^2 \\] usando \\[ \\text{Var}(\\bar{y}_j) = \\frac{\\sigma^2}{n_j}, \\quad \\mathbb{E}[\\bar{y}_j^2] = (\\mu + \\tau_j)^2 + \\frac{\\sigma^2}{n_j} \\] \\[ \\mathbb{E}[\\bar{y}]^2 = \\mu^2 + \\text{Var}(\\bar{y}) = \\mu^2 + \\frac{\\sigma^2}{N} \\] por lo que \\[ \\mathbb{E}[SSA] = \\sum_{j=1}^k n_j \\left[(\\mu + \\tau_j)^2 + \\frac{\\sigma^2}{n_j}\\right] - N \\left(\\mu^2 + \\frac{\\sigma^2}{N}\\right) \\] distribuyendo \\[ \\sum_{j=1}^k n_j (\\mu + \\tau_j)^2 + \\sum_{j=1}^k \\sigma^2 - N\\mu^2 - \\sigma^2. \\] Observamos que \\[ \\sum_{j=1}^k n_j (\\mu + \\tau_j)^2 = N\\mu^2 + \\sum_{j=1}^k n_j \\tau_j^2 \\quad \\text{(ya que } \\sum n_j \\tau_j = 0 \\text{)} \\] y finalmente \\[ \\mathbb{E}[SSA] = N\\mu^2 + \\sum n_j \\tau_j^2 + k\\sigma^2 - N\\mu^2 - \\sigma^2 = \\sum n_j \\tau_j^2 + (k - 1)\\sigma^2 \\] Bajo la hipótesis nula H₀: todos los \\(\\tau_j = 0\\), entonces: \\[ \\mathbb{E}[SSA] = (k - 1)\\sigma^2 \\] En conclusión, \\[ \\mathbb{E}\\left[\\frac{SSA}{k - 1}\\right] = \\sigma^2 \\Rightarrow MSA \\text{ es insesgado.} \\] Referencias Solano, Humberto Llinás. 2019. Estadística Inferencial. Área metropolitana de Barranquilla, Colombia: Universidad del Norte. "],["applications.html", "Capítulo 4 Applications 4.1 Example one 4.2 Example two", " Capítulo 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],["final-words.html", "Capítulo 5 Final Words", " Capítulo 5 Final Words We have finished a nice book. "],["referencias-1.html", "Referencias", " Referencias Solano, Humberto Llinás. 2019. Estadística Inferencial. Área metropolitana de Barranquilla, Colombia: Universidad del Norte. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ———. 2025. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
