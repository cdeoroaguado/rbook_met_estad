[["rls.html", "Capítulo 7 Regresión lineal simple 7.1 El modelo de regresión lineal simple (RLS) 7.2 Estimación de los parámetros por minimos cuadrados de \\(\\beta_0, \\beta_1\\) 7.3 Estimación de \\(\\sigma^2\\)", " Capítulo 7 Regresión lineal simple En muchos procesos productivos se controlan las condiciones de operación de un sistema. Es fundamental determinar cómo una variable respuesta cambia dependiendo de dichas condiciones. Por ejemplo, en un proceso de embutido de cárnicos, la cantidad de producto empacado (en gramos) puede depender de la velocidad y la temperatura de operación de una máquina. En la practica, por supuesto es importante Entender la relación entre la cantidad de producto empacado, la velocidad y la temperatura de operación. Garantizar el cumplimiento de las especificaciones en la cantidad de producto empacado. Optmizar las condiciones de operación para reducir costos y minimizar desperdicios. El Modelo de Regresión Lineal se aplica ampliamente en ingeniería cuando se busca dar respuesta a los tres puntos anteriores. En general, los modelos de regresión lineal permiten estudiar la relación entre: Una variable respuesta \\(Y\\). Un conjunto de variables controlables del proceso, denotadas como \\(X_1, X_2, \\dots, X_k\\). Una vez entendida esta relación, es posible predecir valores futuros de la variable respuesta a partir de valores específicos y conocidos de las variables controlables. Nota: variable controlable Variable controlable: Son aquellas variables de proceso que pueden medirse fácilmente y mantenerse fijas durante la operación. Dependiendo del número de variables controlables, distinguimos: Modelo de Regresión Lineal Simple (RLS): Cuando se tiene una sola variable controlable, digamos \\(x\\). Modelo de Regresión Lineal Múltiple (RLM): Cuando existen más de una variable controlable, digamos \\(x_1, x_2, \\dots, x_k\\). Ejemplo en procesos de embutidos: \\[ x_1 = \\text{velocidad}, \\quad x_2 = \\text{temperatura} \\] 7.1 El modelo de regresión lineal simple (RLS) Consideremos un proceso donde se tienen registros de la Resistencia, en psi, y la Edad, en semanas, de varias soldaduras. Los datos pueden leerse en R como se muestra a continuación: Code ## Lectura de datos desde archivo local file &lt;- &quot;data/data_rls.txt&quot; df &lt;- read.table(file, header = TRUE) ## Primeras 6 filas head(df) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Con miras explorar si existe una relación entre la Resistencia y la Edad de la soldadura, existen dos estrategias. observación En RLS, es posible establecer si existe una posible relación entre \\(X\\) y \\(Y\\) es Construir un gráfico de dispersión Calculando el coeficiente de correlación lineal muestral Para construir un gráfico de dispersión procedemos de la siguiente manera: Code library(tidyverse) df %&gt;% ggplot(aes(x = Edad, y = Resistencia)) + geom_point() + theme_bw() Observe que, aparentemente, cuanto mayor sea la Edad de la soldadura menor será su Resistencia. Por lo tanto, podemos decir que la relación entre Edad y Resistencia es inversamente proporcional. Ahora, miremos la correlación, suponiendo que los datos tienen comportamiento Code ## coeficiente de correlación with(df, cor(Edad, Resistencia)) ## [1] -0.8827718 Por lo tanto, concluimos que la relación entre la Edad de la soldadura y su Resistencia es inversamente proporcional. 7.1.1 Modelo determinista y modelo probabilístico Para el modelo determinista: \\[ y_i = \\beta_0 + \\beta_1 x_i, \\] el valor observado de la Resistencia \\(Y_i\\) es una función lineal de la Edad \\(x_i\\). La generalización apropiada de esto a un modelo probabilístico supone que el valor esperado de la Resistencia \\(Y_i\\) es una función lineal de la Edad \\(x_i\\). Si denotamos por \\(E(Y_i \\,|\\, X = x_i)\\) a la esperanza de la variable aleatoria \\(Y_i\\), entonces bajo el supuesto de linealidad podemos escribir: \\[ E(Y_i \\,|\\, X = x_i) \\;=\\; \\beta_0 + \\beta_1 x_i. \\tag{7.1} \\] En la práctica, el valor observado de \\(Y_i\\) (Resistencia medida) se desviará inevitablemente de su valor esperado debido a la variabilidad y errores de medición. Si esta diferencia se representa mediante la variable aleatoria \\(\\varepsilon_i\\), con media cero por (7.1), se puede establecer: \\[ \\varepsilon_i \\;=\\; Y_i - E(Y_i \\,|\\, X = x_i) \\;=\\; Y_i - (\\beta_0 + \\beta_1 x_i), \\] o de manera equivalente: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i. \\tag{7.2} \\] La ecuación (7.2) es la llamada recta verdadera (o poblacional) de regresión, que describe la relación entre la Resistencia (psi) y la Edad (semanas) de las soldaduras, considerando tanto la tendencia promedio como la variabilidad aleatoria. Definición Supongamos que estamos interesados en conocer la relación entre la variable \\(Y\\) y una independente \\(X\\). Si la variable \\(X\\) toma los valores \\(x_i\\), entonces la ecuación de la recta poblacional de regresión expresa los correspondientes valores de \\(Y_i\\) como: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\] donde \\(\\beta_0\\) y \\(\\beta_1\\) son constantes, y \\(\\varepsilon_i\\), llamado término de error, es una variable aleatoria con media cero. 7.1.2 Supuestos básicos para el modelo de regresión lineal. Los supuestos para el modelo de regresión lineal son Supuestos para el modelo de regresión lineal simple Denotemos la recta verdadera de regresión por \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) y asumamos que se dispone de \\(n\\) pares de observaciones. Suelen realizarse, al respecto, los siguientes supuestos: Cada \\(x_i\\) es un número fijo (asignado, por ejemplo, por un investigador) o es la realización de una variable aleatoria \\(X_i\\) independiente del término de error \\(\\varepsilon_i\\). En el último caso, la inferencia se realiza condicionando al valor observado \\(x_i\\). Los términos de error \\(\\varepsilon_i\\) son variables aleatorias con media 0, es decir, \\(E(\\varepsilon_i) = 0\\), para todo \\(i = 1, \\ldots, n\\). Las variables aleatorias \\(\\varepsilon_i\\) tienen todas la misma varianza \\(\\sigma^2\\), es decir, \\(V(\\varepsilon_i) = \\sigma^2\\), para todo \\(i = 1, \\ldots, n\\). Las variables aleatorias \\(\\varepsilon_i\\) no se hallan correlacionadas, es decir, son independientes, luego \\(E(\\varepsilon_i \\varepsilon_j) = 0\\), para todo \\(i, j = 1, \\ldots, n\\), con \\(i \\neq j\\). Los errores \\(\\varepsilon_i\\) estan distribuidos normalmente. Teorema Denotando la recta verdadera de regresión por \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\), si se cumplen los supuestos del modelo de regresión, entonces, para un valor fijo \\(x_i\\) de \\(X\\), la esperanza y varianza condicionales de \\(Y_i\\), dado que \\(X = x_i\\), vienen dadas, respectivamente, por: \\(E(Y_i \\,|\\, X = x_i) = \\beta_0 + \\beta_1 x_i \\quad \\text{y} \\quad V(Y_i \\,|\\, X = x_i) = \\sigma^2.\\) Demostración del teorema Esperanza condicional. Usando linealidad de la esperanza y que \\(\\beta_0\\) y \\(\\beta_1\\, x_i\\) son constantes dado \\(X=x_i\\): \\[ E(Y_i \\mid X=x_i) = E(\\beta_0 + \\beta_1 x_i + \\varepsilon_i \\mid X=x_i) = \\beta_0 + \\beta_1 x_i + E(\\varepsilon_i \\mid X=x_i). \\] Por independencia (o, en su defecto, por el supuesto \\(E(\\varepsilon_i\\mid X)=0\\)): \\[ E(\\varepsilon_i \\mid X=x_i)=0. \\] Luego, \\[ \\boxed{\\,E(Y_i \\mid X=x_i)=\\beta_0+\\beta_1 x_i\\,}. \\] Varianza condicional. La varianza de una constante es cero y \\(\\beta_0+\\beta_1 x_i\\) es constante dado \\(X=x_i\\): \\[\\begin{align} \\operatorname{Var}(Y_i \\mid X=x_i) &amp;= \\operatorname{Var}(\\beta_0 + \\beta_1 x_i + \\varepsilon_i \\mid X=x_i) \\\\[6pt] &amp;= \\operatorname{Var}(\\beta_0 + \\beta_1 x_i \\mid X=x_i) + \\operatorname{Var}(\\varepsilon_i \\mid X=x_i) \\\\[6pt] &amp;= 0 + \\operatorname{Var}(\\varepsilon_i \\mid X=x_i). \\end{align}\\] Por independencia (o por homocedasticidad condicional): \\[ \\operatorname{Var}(\\varepsilon_i \\mid X=x_i) = \\operatorname{Var}(\\varepsilon_i)=\\sigma^2. \\] Por tanto, \\[ \\boxed{\\,\\operatorname{Var}(Y_i \\mid X=x_i)=\\sigma^2\\,}. \\] En general, el modelo de RLS surge ante la necesidad de predecir una variable respuesta \\(Y\\), generalmente continua, como función de una variable controlable \\(X\\), donde matematicamente el modelo puede expresarse como: \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\] \\[ \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2), \\] \\[ \\sigma^2 = \\text{constante}. \\] donde \\((\\beta_0, \\beta_1, \\sigma^2)\\) corresponden a los parámetros del modelo y \\(\\varepsilon_i\\) es el error aleatorio para la observación \\(i\\). Los términos \\((\\beta_0, \\beta_1)\\) se conocen como los coeficientes del modelo. \\(\\sigma^2\\) corresponde a la varianza de los errores. En la práctica, se tiene o se toma una muestra aleatoria de tamaño \\(n\\) proveniente de una población o de un proceso de producción/servicios. A partir de dicha muestra, se busca estimar los valores de \\((\\beta_0, \\beta_1, \\sigma^2)\\). 7.2 Estimación de los parámetros por minimos cuadrados de \\(\\beta_0, \\beta_1\\) Para estimar \\(\\beta_0\\) y \\(\\beta_1\\) se usa el método de mínimos cuadrados. Esto es, se estiman \\(\\beta_0\\) y \\(\\beta_1\\) tales que la suma de los cuadrados de las diferencias entre las observaciones \\(y_i\\) y la línea recta sea mínima. El modelo es: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i=1,2,\\ldots,n \\] El criterio de mínimos cuadrados es: \\[ S(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\varepsilon_i^2=\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Para obtener los estimadores, derivamos respecto a \\(\\beta_0\\) y \\(\\beta_1\\): \\[\\begin{align} \\frac{\\partial S}{\\partial \\beta_0}\\Big|_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;= -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[6pt] \\frac{\\partial S}{\\partial \\beta_1}\\Big|_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;= -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)x_i = 0 \\end{align}\\] Estas se simplifican a las llamadas ecuaciones normales: \\[\\begin{align} n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^n x_i &amp;= \\sum_{i=1}^n y_i \\\\[6pt] \\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 &amp;= \\sum_{i=1}^n y_i x_i \\end{align}\\] La solución de este sistema es: \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\] \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n y_i x_i - \\frac{1}{n}\\left(\\sum_{i=1}^n y_i\\right)\\left(\\sum_{i=1}^n x_i\\right)} {\\sum_{i=1}^n x_i^2 - \\frac{1}{n}\\left(\\sum_{i=1}^n x_i\\right)^2} \\] donde: \\[ \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i, \\qquad \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i \\] El modelo ajustado es: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] De manera más sencilla, definimos: \\[ S_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2 \\qquad \\text{y} \\qquad S_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\] Entonces, una forma conveniente de escribir la pendiente es: \\[ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} \\] Ejercicio Queda de ejercicio verificar que en dicho punto crítico se alcanza un mínimo local. Sugerencia, realizar la segundas derivadas parciales. Ejercicio Demuestre que \\[\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) = \\sum_{i=1}^n x_i y_i - n \\, \\overline{x} \\, \\overline{y}\\] Demuestre que \\[\\sum_{i=1}^n (x_i - \\overline{x})^2 = \\sum_{i=1}^n x_i^2 - n \\overline{x}^2\\] Observacion En la práctica, \\(\\hat{\\beta}_1\\) se interpreta como el cambio unitario en el valor esperado de \\(Y\\), es decir, en \\(E[Y\\mid X]\\), por cada cambio unitario del factor controlable \\(X\\). Continuando con el ejemplo de los datos de soldadura, estimemos paso a paso \\(\\beta_0, \\beta_1\\): Code # 1. Lectura de datos df &lt;- read.table(file, header = TRUE) # 2. Variables Y &lt;- df$Resistencia X &lt;- df$Edad n &lt;- length(X) # 3. Medias x_bar &lt;- mean(X) y_bar &lt;- mean(Y) # 4. Cálculo de Sxx y Sxy Sxx &lt;- sum((X - x_bar)^2) Sxy &lt;- sum((X - x_bar) * (Y - y_bar)) # 5. Estimadores de mínimos cuadrados beta1 &lt;- Sxy / Sxx beta1 ## [1] -0.2437636 Code beta0 &lt;- y_bar - beta1 * x_bar beta0 ## [1] 15.74275 En R la función clave paara ajustar modelos de RLS es lm. Para mayor información, puede consultar la ayuda de la función escribiendo ?lm en la consola, o en la página oficial. En general, la sintaxis es Code # ajuste del modelo de RLS modelo &lt;- lm(Resistencia ~ Edad, data = df) modelo ## ## Call: ## lm(formula = Resistencia ~ Edad, data = df) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Los resultados indican que \\(\\hat{\\beta}_0 = 15.74\\) y \\(\\hat{\\beta}_1 = -0.244\\). Por lo tanto, el modelo ajustado puede escribirse como: \\[ \\widehat{\\text{Resistencia}}_i = 15.74 \\;-\\; 0.244 \\,\\text{Edad}_i, \\] \\[ \\varepsilon_i \\sim N(0, \\sigma^2), \\qquad \\sigma^2 = \\text{constante}. \\] Interpretación de \\(\\,\\hat{\\beta}_0\\,\\) y \\(\\,\\hat{\\beta}_1\\) Se espera que, por cada semana que transcurre, la resistencia de la soldadura disminuya 0.244 psi. Es decir, \\(\\hat{\\beta}_1 = -0.244\\). Una soldadura nueva \\((\\text{Edad} = 0)\\) tiene una resistencia promedio de 15.74 psi. Es decir, \\(\\hat{\\beta}_0 = 15.74\\). Ejercicio 1 Las cantidades de un compuesto químico “y”, que se disuelven en 100 gramos de agua a varias temperaturas “x”, se registran como sigue: x (°C) y (gramos) 0 8 0 6 0 8 15 12 15 10 15 14 30 25 30 21 30 24 45 31 45 33 45 28 60 44 60 39 60 42 75 48 75 51 75 44 Grafique un diagrama de dispersión de los datos. ¿Parecerá plausible un modelo de regresión lineal simple? Encuentre las estimaciones de mínimos cuadrados de la pendiente y la ordenada al origen del modelo de regresión lineal simple. Estime la cantidad de compuesto químico que se disolverá en 100 gramos de agua a 50°C. 7.3 Estimación de \\(\\sigma^2\\) Además de estimar \\(\\beta_0\\) y \\(\\beta_1\\), se requiere un estimado de \\(\\sigma^2\\) para probar hipótesis y formar intervalos de confianza pertinentes al modelo de regresión. Ahora, se llama residuos (errores) a las desviaciones verticales \\[\\varepsilon_i = y_i - \\hat{y}_i, \\quad i = 1, 2, \\ldots, n\\] de la recta estimada. Cuando no se puede usar información previa, el estimado de \\(\\sigma^2\\) se obtiene de la suma de cuadrados de los errores (SSE): \\[ SSE = S = \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\tag{7.3} \\] Si sustituimos \\(\\hat{y}_i = \\hat{\\beta_0}+\\hat{\\beta_1}x_i\\) en la ecuación (7.3) tenemos que \\[\\begin{align} SSE &amp;= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left( y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i \\right)^2 \\\\[6pt] &amp;= \\sum_{i=1}^n \\left[ (y_i - \\bar{y}) - \\hat{\\beta}_1 (x_i - \\bar{x}) \\right]^2 \\\\[6pt] &amp;= \\sum_{i=1}^n (y_i - \\bar{y})^2 - 2\\hat{\\beta}_1 \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) + \\hat{\\beta}_1^2 \\sum_{i=1}^n (x_i - \\bar{x})^2 \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1^2 S_{xx} \\end{align}\\] Como \\(\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}}\\) se sigue que \\[\\begin{align} SSE &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1^2 S_{xx} \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1 \\left(\\frac{S_{xy}}{S_{xx}}\\right) S_{xx} \\\\[6pt] &amp;= SST - 2\\hat{\\beta}_1 S_{xy} + \\hat{\\beta}_1 S_{xy} \\\\[6pt] &amp;= SST - \\hat{\\beta}_1 S_{xy}. \\end{align}\\] La suma de cuadrados de residuales tiene \\(n - 2\\) grados de libertad, porque dos grados de libertad se asocian con los estimados \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que se usan para obtener \\(\\hat{y}_i\\). Por el teorema de MSA y MSE insesgado de \\(\\sigma^2\\), sabemos que el valor esperado de \\(SSE\\) es: \\[ E(SSE) = (n-2)\\sigma^2, \\] por lo que un estimador insesgado de \\(\\sigma^2\\) es: \\[ \\hat{\\sigma}^2 = \\frac{SSE}{n-2} = MSE \\] La cantidad \\(MSE\\) se llama cuadrado medio residual. La raíz cuadrada de \\(\\hat{\\sigma}^2\\) se llama, a veces, el error estándar de la regresión (RSE), y tiene las mismas unidades que la variable respuesta \\(y\\). Ya que \\(\\hat{\\sigma}^2\\) depende de la suma de cuadrados de residuales, cualquier violación de las hipótesis sobre los errores del modelo, o cualquier especificación equivocada de la forma del modelo, pueden dañar la utilidad de \\(\\hat{\\sigma}^2\\) como estimador de \\(\\sigma^2\\). Como \\(\\sigma^2\\) se calcula con los residuales del modelo de regresión, se dice que es un estimador de \\(\\sigma^2\\) dependiente del modelo. Continuando con el ejemplo de soldaduras. Estimemos el \\(\\sigma\\). Haciendolo paso a paso Code # 1. Cargar datos datos &lt;- read.table(&quot;data/data_rls.txt&quot;, header = TRUE) x &lt;- datos$Edad y &lt;- datos$Resistencia # 2. Ajustar el modelo de regresión lineal simple modelo &lt;- lm(Resistencia ~ Edad, data = datos) # 3. Los coeficientes son b0 &lt;- modelo$coefficients[1] b1 &lt;- modelo$coefficients[2] # 4. Ajustados y residuos yhat &lt;- b0 + b1 * x e &lt;- y - yhat # 5. SSE por definición SSE &lt;- sum(e^2) # Otra forma de btener los residuos residuos &lt;- resid(modelo) SSE_otro &lt;- sum(residuos^2) # 6. Calcular MSE n &lt;- nrow(datos) # número de observaciones gl &lt;- n - 2 # grados de libertad MSE &lt;- SSE / gl MSE ## [1] 0.3491253 Code # 7. Calcular RSE RSE &lt;- sqrt(MSE) RSE ## [1] 0.5908683 Cuando ajustamos el modelo con la función modelo &lt;- lm(Resistencia ~ Edad, data = datos) para obtener el RSE de la siguiente forma Code ## estimación de sigma summary(modelo)$sigma ## [1] 0.5908683 A partir de este resultado podemos concluir que \\(\\hat{\\sigma}=0.591\\). Por lo tanto, \\(MSE=0.591^2=0.349\\). Finalmente, el modelo ajustado puede expresarse como \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2) \\\\[6pt] \\hat{\\mu}_i &amp;= 15.743 - 0.244 \\, \\text{Edad}_i \\\\[6pt] \\hat{\\sigma}^2 &amp;= 0.591^2 = 0.349 \\end{align}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
