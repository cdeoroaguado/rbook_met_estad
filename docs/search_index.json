[["rlm.html", "Capítulo 8 Regresión lineal multiple 8.1 Formulación matemática del modelo de RLM 8.2 Estimación de los coeficientes del modelo de RLM 8.3 Propiedades de los estimadores de \\(\\beta\\)", " Capítulo 8 Regresión lineal multiple Como se mencionó en el Capítulo de rls, los modelos de regresión lineal permiten predecir valores futuros de una variable respuesta continua a partir de valores específicos de las variables controlables del proceso. En la práctica, pueden existir múltiples variables controlables en un proceso de producción o de servicios. Por ejemplo, en un proceso de pintura electrostática, puede interesar determinar el espesor de la capa de pintura (variable respuesta \\(y\\), en micrones) con la que se recubre una lámina de área determinada, a partir de valores conocidos de: la presión del aire (\\(x_1\\), en psi), y la velocidad de la banda transportadora (\\(x_2\\), en m/s). En este contexto, el objetivo es: Determinar la magnitud de la influencia de las variables \\(x_1\\) y \\(x_2\\) sobre el espesor esperado de la capa. Construir una función de predicción \\(f(x_1, x_2)\\) que permita estimar el espesor esperado. Elaborar intervalos de confianza y predicción para dicho valor. Si la variable respuesta \\(y\\) es continua y aproximadamente simétrica, los objetivos (1), (2) y (3) pueden abordarse mediante la estimación de un modelo de regresión lineal. Dado que el número de variables controlables es \\(k &gt; 1\\), una alternativa apropiada es utilizar el modelo de Regresión Lineal Múltiple (RLM) Nota El modelo de RLM es una extensión del modelo de RLS cuando se tiene más de una variable controlable. Las notas de clase son tomadas del texto (Montgomery, Peck, and Vining 2012) 8.1 Formulación matemática del modelo de RLM El modelo RLM puede relacionar la respuesta \\(y\\) con \\(k\\) regresores o variables predictoras. El modelo se expresa como: \\[ \\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\varepsilon \\tag{8.1} \\end{align} \\] Este se denomina modelo de regresión lineal múltiple con \\(k\\) regresores. También, el modelo de RLM puede expresarse como: para \\(i=1,\\dots,n,\\) \\[ \\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + \\varepsilon_i, \\\\ \\varepsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante.} \\end{align} \\] donde: \\(\\beta_0\\) es la ordenada al origen, \\(\\beta_i\\) son los coeficientes de regresión que miden la influencia de cada variable \\(x_i\\), y \\(\\varepsilon_i\\) representa el término de error aleatorio. 8.2 Estimación de los coeficientes del modelo de RLM Los coeficientes del modelo de RLM pueden estimarse mediante el método de mínimos cuadrados ordinarios (MCO), que busca minimizar la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por el modelo. \\[ \\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + \\varepsilon_i \\\\ &amp;= \\beta_0 + \\sum_{j=1}^{k} \\beta_j x_{ij} + \\varepsilon_i, \\quad i = 1, 2, \\ldots, n \\tag{8.2} \\end{align} \\] La función de mínimos cuadrados es: \\[ \\begin{align} S(\\beta_0, \\beta_1, \\ldots, \\beta_k) &amp;= \\sum_{i=1}^{n} \\varepsilon_i^2 \\\\ &amp;= \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j x_{ij} \\right)^2 \\tag{8.3} \\end{align} \\] Se debe minimizar la función \\(S\\) respecto a \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\). Los estimadores de \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) por mínimos cuadrados deben satisfacer: \\[ \\begin{align} \\left. \\frac{\\partial S}{\\partial \\beta_0} \\right|_{\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k} &amp;= -2 \\sum_{i=1}^{n} \\left( y_i - \\hat{\\beta}_0 - \\sum_{j=1}^{k} \\hat{\\beta}_j x_{ij} \\right) = 0 \\tag{8.4a} \\\\ \\left. \\frac{\\partial S}{\\partial \\beta_j} \\right|_{\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k} &amp;= -2 \\sum_{i=1}^{n} \\left( y_i - \\hat{\\beta}_0 - \\sum_{j=1}^{k} \\hat{\\beta}_j x_{ij} \\right) x_{ij} = 0, \\quad j = 1, 2, \\ldots, k \\tag{8.4b} \\end{align} \\] Al simplificar las ecuaciones (8.4), se obtienen las ecuaciones normales de mínimos cuadrados: \\[ \\begin{align} n\\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^{n} x_{i1} + \\hat{\\beta}_2 \\sum_{i=1}^{n} x_{i2} + \\cdots + \\hat{\\beta}_k \\sum_{i=1}^{n} x_{ik} &amp;= \\sum_{i=1}^{n} y_i \\\\ \\hat{\\beta}_0 \\sum_{i=1}^{n} x_{i1} + \\hat{\\beta}_1 \\sum_{i=1}^{n} x_{i1}^2 + \\hat{\\beta}_2 \\sum_{i=1}^{n} x_{i1}x_{i2} + \\cdots + \\hat{\\beta}_k \\sum_{i=1}^{n} x_{i1}x_{ik} &amp;= \\sum_{i=1}^{n} x_{i1}y_i \\\\ \\vdots \\nonumber \\\\ \\hat{\\beta}_0 \\sum_{i=1}^{n} x_{ik} + \\hat{\\beta}_1 \\sum_{i=1}^{n} x_{ik}x_{i1} + \\hat{\\beta}_2 \\sum_{i=1}^{n} x_{ik}x_{i2} + \\cdots + \\hat{\\beta}_k \\sum_{i=1}^{n} x_{ik}^2 &amp;= \\sum_{i=1}^{n} x_{ik}y_i \\tag{8.5} \\end{align} \\] Nótese que hay \\(p = k + 1\\) ecuaciones normales, una para cada uno de los coeficientes desconocidos del modelo de regresión. La solución de estas ecuaciones normales proporciona los estimadores por mínimos cuadrados \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k\\). Es más cómodo manejar los modelos de regresión múltiple en notación matricial, ya que permite representar de manera compacta el modelo, los datos y los resultados. En notación matricial, el modelo expresado por la ecuación (8.2) es: \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] en donde: \\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\[3pt] y_2 \\\\[3pt] \\vdots \\\\[3pt] y_n \\end{bmatrix}, \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nk} \\end{bmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\[3pt] \\beta_1 \\\\[3pt] \\vdots \\\\[3pt] \\beta_k \\end{bmatrix}, \\quad \\boldsymbol{\\varepsilon} = \\begin{bmatrix} \\varepsilon_1 \\\\[3pt] \\varepsilon_2 \\\\[3pt] \\vdots \\\\[3pt] \\varepsilon_n \\end{bmatrix} \\] donde: \\(\\mathbf{y}\\): vector \\((n \\times 1)\\) de observaciones. \\(\\mathbf{X}\\): matriz \\((n \\times p)\\) de niveles de las variables independientes. \\(\\boldsymbol{\\beta}\\): vector \\((p \\times 1)\\) de coeficientes de regresión. \\(\\boldsymbol{\\varepsilon}\\): vector \\((n \\times 1)\\) de términos de error aleatorio. Se desea determinar el vector \\(\\hat{\\boldsymbol{\\beta}}\\) de estimadores de mínimos cuadrados que minimice: \\[ S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\varepsilon_i^2 = \\boldsymbol{\\varepsilon}&#39;\\boldsymbol{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\] Nótese que \\(S(\\boldsymbol{\\beta})\\) puede expresarse como: \\[ \\begin{align} S(\\boldsymbol{\\beta}) &amp;= \\mathbf{y}&#39;\\mathbf{y} - \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y} - \\mathbf{y}&#39;\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} \\\\ &amp;= \\mathbf{y}&#39;\\mathbf{y} - 2\\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y} + \\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} \\end{align} \\] ya que \\(\\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y}\\) es un escalar (matriz \\(1 \\times 1\\)) y su traspuesta \\((\\boldsymbol{\\beta}&#39;\\mathbf{X}&#39;\\mathbf{y})&#39; = \\mathbf{y}&#39;\\mathbf{X}\\boldsymbol{\\beta}\\) es el mismo escalar. Los estimadores de mínimos cuadrados deben satisfacer: \\[ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}&#39;\\mathbf{y} + 2\\mathbf{X}&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\] lo que se simplifica a: \\[ \\mathbf{X}&#39;\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}&#39;\\mathbf{y} \\tag{8.6} \\] Para resolver las ecuaciones normales se multiplican ambos lados de (8.6) por la inversa de \\(\\mathbf{X}&#39;\\mathbf{X}\\). Así, el estimador de \\(\\boldsymbol{\\beta}\\) por mínimos cuadrados es: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} \\tag{8.7} \\] Este estimador existe siempre que \\((\\mathbf{X}&#39;\\mathbf{X})^{-1}\\) exista, lo cual ocurre si los regresores son linealmente independientes, es decir, si ninguna columna de la matriz \\(\\mathbf{X}\\) es una combinación lineal de las demás. Es fácil ver que la forma matricial de las ecuaciones normales (8.6) es idéntica a la forma escalar (8.5). Al escribir (8.6) con detalle se obtiene: \\[ \\begin{bmatrix} n &amp; \\displaystyle\\sum_{i=1}^{n} x_{i1} &amp; \\displaystyle\\sum_{i=1}^{n} x_{i2} &amp; \\cdots &amp; \\displaystyle\\sum_{i=1}^{n} x_{ik} \\\\[8pt] \\displaystyle\\sum_{i=1}^{n} x_{i1} &amp; \\displaystyle\\sum_{i=1}^{n} x_{i1}^2 &amp; \\displaystyle\\sum_{i=1}^{n} x_{i1}x_{i2} &amp; \\cdots &amp; \\displaystyle\\sum_{i=1}^{n} x_{i1}x_{ik} \\\\[8pt] \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\[8pt] \\displaystyle\\sum_{i=1}^{n} x_{ik} &amp; \\displaystyle\\sum_{i=1}^{n} x_{ik}x_{i1} &amp; \\displaystyle\\sum_{i=1}^{n} x_{ik}x_{i2} &amp; \\cdots &amp; \\displaystyle\\sum_{i=1}^{n} x_{ik}^2 \\end{bmatrix} \\begin{bmatrix} \\hat{\\beta}_0 \\\\[3pt] \\hat{\\beta}_1 \\\\[3pt] \\vdots \\\\[3pt] \\hat{\\beta}_k \\end{bmatrix} = \\begin{bmatrix} \\displaystyle\\sum_{i=1}^{n} y_i \\\\[8pt] \\displaystyle\\sum_{i=1}^{n} x_{i1}y_i \\\\[8pt] \\vdots \\\\[8pt] \\displaystyle\\sum_{i=1}^{n} x_{ik}y_i \\end{bmatrix} \\] Si se realiza la multiplicación matricial indicada, se obtiene la forma escalar de las ecuaciones normales (8.5). En esta forma se observa que \\(\\mathbf{X}&#39;\\mathbf{X}\\) es una matriz simétrica de dimensión \\(p \\times p\\), y que \\(\\mathbf{X}&#39;\\mathbf{y}\\) es un vector columna de dimensión \\(p \\times 1\\). Los elementos diagonales de \\(\\mathbf{X}&#39;\\mathbf{X}\\) son las sumas de los cuadrados de los elementos de las columnas de \\(\\mathbf{X}\\), mientras que los elementos fuera de la diagonal son las sumas de los productos cruzados entre columnas de \\(\\mathbf{X}\\). Los elementos de \\(\\mathbf{X}&#39;\\mathbf{y}\\) son las sumas de los productos cruzados de las columnas de \\(\\mathbf{X}\\) con las observaciones \\(y_i\\). 8.2.1 Modelo ajustado de regresión El modelo ajustado correspondiente a los niveles de las variables regresoras \\(\\mathbf{x}&#39; = [1, x_1, x_2, \\ldots, x_k]\\) es: \\[ \\hat{y} = \\mathbf{x}&#39;\\hat{\\boldsymbol{\\beta}} = \\hat{\\beta}_0 + \\sum_{j=1}^{k} \\hat{\\beta}_j x_j \\] El vector de valores ajustados \\(\\hat{\\mathbf{y}}\\), que corresponde a los valores observados \\(\\mathbf{y}\\), se expresa como: \\[ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} = \\mathbf{H}\\mathbf{y} \\tag{8.8} \\] donde \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\) es una matriz \\(n \\times n\\), conocida como matriz de sombrero (hat matrix). Esta matriz transforma los valores observados en los valores ajustados y juega un papel central en el análisis de regresión. 8.2.2 Vector de residuales La diferencia entre el valor observado \\(y_i\\) y el valor ajustado \\(\\hat{y}_i\\) se denomina residual: \\[ \\varepsilon_i = y_i - \\hat{y}_i \\] Por lo tanto, el vector de residuales se puede escribir en notación matricial como: \\[ \\boldsymbol{\\varepsilon} = \\mathbf{y} - \\hat{\\mathbf{y}} \\tag{8.9a} \\] De manera equivalente: \\[ \\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{y} - \\mathbf{H}\\mathbf{y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y} \\tag{8.9b} \\] Como ejemplo, consideremos los siguientes datos provenientes de un proceso de pintura electrostática: datos de pintura Code ## lectura de datos path &lt;- &#39;data/data_pintura.txt&#39; df &lt;- read.table(path, header = TRUE) head(df) ## y x1 x2 ## 1 91.88308 18.722091 6 ## 2 94.37544 19.056131 3 ## 3 74.63026 9.292093 3 ## 4 93.66348 17.456714 4 ## 5 76.84981 14.626183 9 ## 6 76.05198 12.786439 9 En este caso, la variable respuesta \\(y\\) representa el espesor de la capa de pintura (en micrones), mientras que \\(x_1\\) corresponde a la presión del aire (en psi) y \\(x_2\\) a la velocidad de la banda transportadora (en m/s). Una forma inicial de explorar si existe una relación lineal entre la variable respuesta \\(y\\) y las variables predictoras \\(x_1\\) y \\(x_2\\) es mediante una red de diagramas de dispersión (scatterplot matrix), como se muestra a continuación. Code library(GGally) library(tidyverse) # Etiquetas para ejes/paneles labs_cols &lt;- c(&quot;x1&quot; = &quot;Presión del aire (psi)&quot;, &quot;x2&quot; = &quot;Velocidad de banda (m/s)&quot;, &quot;y&quot; = &quot;Espesor (micrones)&quot;) # Red de gráficos: df %&gt;% ggpairs( columns = c(&quot;x1&quot;, &quot;x2&quot;, &quot;y&quot;), columnLabels = labs_cols[c(&quot;x1&quot;, &quot;x2&quot;, &quot;y&quot;)], diag = list(continuous = &quot;densityDiag&quot;), upper = list(continuous = GGally::wrap(&quot;cor&quot;, method = &quot;pearson&quot;, stars = TRUE, size = 4)), lower = list(continuous = GGally::wrap(&quot;smooth&quot;, method = &quot;lm&quot;, se = FALSE)) ) + theme_bw() + labs(title = &quot;Relaciones entre y (espesor), x1 (presión) y x2 (velocidad)&quot;, subtitle = &quot;Diagonal: densidades • Superior: correlación (★ p-valor) • Inferior: ajuste lineal&quot;) La Figura anterior muestra la red de diagramas de dispersión entre la variable respuesta \\(y\\) (espesor de la capa de pintura) y las variables predictoras \\(x_1\\) (presión del aire) y \\(x_2\\) (velocidad de la banda transportadora). A partir del gráfico se pueden obtener las siguientes conclusiones: Relación entre \\(y\\) y \\(x_1\\) (presión del aire): Se observa una fuerte correlación positiva (\\(r = 0.890\\), altamente significativa). A medida que la presión del aire aumenta, también lo hace el espesor de la capa de pintura. Esto indica que \\(x_1\\) tiene una influencia directa e importante sobre la variable respuesta. Relación entre \\(y\\) y \\(x_2\\) (velocidad de la banda): Se presenta una correlación negativa moderada (\\(r = -0.317\\)), estadísticamente significativa. A mayor velocidad de la banda, el espesor tiende a disminuir, lo cual es consistente con el comportamiento físico del proceso:una mayor velocidad reduce el tiempo de exposición, y por tanto, la cantidad de pintura depositada. Relación entre \\(x_1\\) y \\(x_2\\):La correlación entre las variables explicativas es muy baja (\\(r = -0.041\\)), indicando independencia práctica entre ambas. Esto es favorable, ya que sugiere ausencia de multicolinealidad, lo que garantiza estimadores más estables en el modelo. Un gráfico en 3D permite explorar visualmente la relación conjunta entre las tres variables del proceso de pintura electrostática: el espesor de la capa (\\(y\\)), la presión del aire (\\(x_1\\)) y la velocidad de la banda transportadora (\\(x_2\\)). Este diagrama puede generarse con el paquete scatterplot3d, como se muestra a continuación: Code library(scatterplot3d) # Gráfico 3D scatterplot3d( x = df$x1, y = df$x2, z = df$y, pch = 16, color = &quot;steelblue&quot;, type = &quot;h&quot;, # Líneas verticales desde el plano XY highlight.3d = TRUE, # Resalta profundidad con color angle = 55, xlab = &quot;Presión del aire (psi)&quot;, ylab = &quot;Velocidad de la banda (m/s)&quot;, zlab = &quot;Espesor de la capa (micrones)&quot;, cex.lab = 1.1 ) El gráfico 3D muestra que al aumentar la presión del aire, el espesor de la capa también aumenta, mientras que al incrementar la velocidad de la banda, el espesor disminuye ligeramente. El mismo gráfico de dispersión 3D puede construirse de manera interactiva con el paquete plotly. Esto permite rotar, acercar o explorar los puntos desde diferentes ángulos, lo cual facilita la visualización del comportamiento del espesor frente a la presión y la velocidad. Code library(plotly) # Gráfico 3D interactivo plot_ly( data = df, x = ~x1, y = ~x2, z = ~y, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, marker = list(size = 5, color = ~y, colorscale = &quot;Reds&quot;, showscale = TRUE) ) %&gt;% layout( scene = list( xaxis = list(title = &quot;Presión del aire (psi)&quot;), yaxis = list(title = &quot;Velocidad de la banda (m/s)&quot;), zaxis = list(title = &quot;Espesor de la capa (micrones)&quot;) ), title = &quot;Gráfico 3D interactivo: relación entre presión, velocidad y espesor&quot; ) El gráfico 3D interactivo muestra que el espesor de la capa aumenta con la presión del aire y disminuye ligeramente al aumentar la velocidad de la banda, confirmando una tendencia lineal coherente con el modelo de regresión múltiple. Para ajustar (entrenar) el modelo de regresión multiple, encontremos los coeficientes, lo haremos paso a paso: Code # 1) Construir matriz de diseño X (con intercepto) y vector y X &lt;- cbind(Intercept = 1, x1 = df$x1, x2 = df$x2) y &lt;- as.matrix(df$y) #2 ) Calcular X&#39;X, su inversa y X&#39;y XtX &lt;- t(X) %*% X print(round(XtX, 6)) ## Intercept x1 x2 ## Intercept 100.000 1286.718 503.000 ## x1 1286.718 18587.290 6435.177 ## x2 503.000 6435.177 2941.000 Code XtX_inv &lt;- solve(XtX) # (X&#39;X)^{-1} print(round(XtX_inv, 6)) ## Intercept x1 x2 ## Intercept 0.159084 -0.006570 -0.012833 ## x1 -0.006570 0.000493 0.000044 ## x2 -0.012833 0.000044 0.002438 Code Xty &lt;- t(X) %*% y print(round(Xty, 6)) ## [,1] ## Intercept 8005.60 ## x1 106059.83 ## x2 39779.69 Code # 3) Estimadores del RLM: beta_hat = (X&#39;X)^{-1} X&#39;y beta_hat &lt;- XtX_inv %*% Xty rownames(beta_hat) &lt;- c(&quot;(Intercepto)&quot;,&quot;x1&quot;,&quot;x2&quot;) colnames(beta_hat) &lt;- &quot;Estimadores&quot; print(round(beta_hat, 6)) ## Estimadores ## (Intercepto) 66.284831 ## x1 1.482753 ## x2 -1.055206 Para estimar el modelo de RLM, utilizamos la función lm como se muestra a continuación: Code ## ajuste del modelo de RLM modelo &lt;- lm(y ~ x1 + x2, data = df) summary(modelo) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0571 -1.6610 -0.1362 1.5409 8.7008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.28483 1.10350 60.068 &lt; 2e-16 *** ## x1 1.48275 0.06144 24.132 &lt; 2e-16 *** ## x2 -1.05521 0.13660 -7.725 1.03e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.767 on 97 degrees of freedom ## Multiple R-squared: 0.8716, Adjusted R-squared: 0.8689 ## F-statistic: 329.1 on 2 and 97 DF, p-value: &lt; 2.2e-16 El modelo estimado con los datos experimentales es: \\[ \\hat{y}_i = 66.285 + 1.483\\,x_{1i} - 1.055\\,x_{2i}\\qquad \\varepsilon_i \\sim N(0, \\sigma^2), \\quad \\sigma^2 = \\text{constante.} \\] y la interpretación seria: \\(\\beta_1 = 1.483\\): al aumentar la presión del aire en una unidad (1 psi), el espesor promedio de la capa aumenta aproximadamente 1.483 micrones, manteniendo constante la velocidad de la banda. \\(\\beta_2 = -1.055\\): al incrementar la velocidad de la banda en una unidad (1 m/s), el espesor promedio disminuye en aproximadamente 1.055 micrones, manteniendo constante la presión del aire. \\(\\beta_0 = 66.285\\): representa el espesor promedio estimado cuando tanto la presión como la velocidad son cero (valor teórico de referencia). Ahora, obtengamos los residuos paso a paso: Code # Valores ajustados y residuales (opcional para comprobar) y_hat &lt;- X %*% beta_hat e &lt;- y-y_hat head(cbind(y_observado=y[,1],y_ajustado=y_hat[,1],residuo=e[,1])) ## y_observado y_ajustado residuo ## [1,] 91.88308 87.71384 4.1692406 ## [2,] 94.37544 91.37476 3.0006861 ## [3,] 74.63026 76.89710 -2.2668374 ## [4,] 93.66348 87.94801 5.7154730 ## [5,] 76.84981 78.47500 -1.6251960 ## [6,] 76.05198 75.74712 0.3048659 Usando la función resid, obtienes el residual Code residuos &lt;- resid(modelo) head(residuos) ## 1 2 3 4 5 6 ## 4.1692406 3.0006861 -2.2668374 5.7154730 -1.6251960 0.3048659 8.3 Propiedades de los estimadores de \\(\\beta\\) 8.3.1 Esperanza del estimador de mínimos cuadrados Las propiedades estadísticas del estimador de mínimos cuadrados \\(\\hat{\\boldsymbol{\\beta}}\\) se pueden demostrar fácilmente. Comenzaremos examinando su sesgo. Sabemos que: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} \\] Dado que \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), se tiene: \\[ \\begin{align} E(\\hat{\\boldsymbol{\\beta}}) &amp;= E\\!\\left[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y}\\right] \\\\[6pt] &amp;= E\\!\\left[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})\\right] \\\\[6pt] &amp;= E\\!\\left[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol{\\varepsilon}\\right] \\\\[6pt] &amp;= E\\!\\left[\\boldsymbol{\\beta} + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol{\\varepsilon}\\right] \\\\[6pt] &amp;= \\boldsymbol{\\beta} + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;E(\\boldsymbol{\\varepsilon}) \\end{align} \\] Como \\(E(\\boldsymbol{\\varepsilon}) = 0\\), se obtiene: \\[ \\boxed{E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}} \\] Por lo tanto, \\(\\hat{\\boldsymbol{\\beta}}\\) es un estimador insesgado de \\(\\boldsymbol{\\beta}\\). 8.3.2 Varianza del estimador de mínimos cuadrados Probemos iniciamente que \\(Var(\\mathbf{A}\\mathbf{z}) = \\mathbf{A}\\,Var(\\mathbf{z})\\,\\mathbf{A}&#39;\\). En efecto, sea \\(\\mathbf{z}\\) un vector aleatorio con \\(E(\\mathbf{z})=\\boldsymbol{\\mu}\\) y varianza \\[ Var(\\mathbf{z}) = E\\!\\left[(\\mathbf{z}-\\boldsymbol{\\mu})(\\mathbf{z}-\\boldsymbol{\\mu})&#39;\\right]. \\] Sea \\(\\mathbf{A}\\) una matriz constante de dimensión compatible y definamos \\(\\mathbf{y}=\\mathbf{A}\\mathbf{z}\\). Entonces \\(E(\\mathbf{y})=E(\\mathbf{A}\\mathbf{z})=\\mathbf{A}E(\\mathbf{z})=\\mathbf{A}\\boldsymbol{\\mu}\\) por lo que \\[ \\begin{align} Var(\\mathbf{y}) &amp;= E\\!\\left[(\\mathbf{y}-E(\\mathbf{y}))(\\mathbf{y}-E(\\mathbf{y}))&#39;\\right] \\\\[4pt] &amp;= E\\!\\left[(\\mathbf{A}\\mathbf{z}-\\mathbf{A}\\boldsymbol{\\mu})\\,(\\mathbf{A}\\mathbf{z}-\\mathbf{A}\\boldsymbol{\\mu})&#39;\\right] \\\\[4pt] &amp;= E\\!\\left[\\mathbf{A}(\\mathbf{z}-\\boldsymbol{\\mu})\\,(\\mathbf{z}-\\boldsymbol{\\mu})&#39;\\mathbf{A}&#39;\\right] \\quad (\\text{factorizando } \\mathbf{A} \\text{ y } \\mathbf{A}&#39; \\text{ al ser constantes}) \\\\[4pt] &amp;= \\mathbf{A}\\,E\\!\\left[(\\mathbf{z}-\\boldsymbol{\\mu})(\\mathbf{z}-\\boldsymbol{\\mu})&#39;\\right]\\mathbf{A}&#39; \\\\[4pt] &amp;= \\mathbf{A}\\,Var(\\mathbf{z})\\,\\mathbf{A}&#39;. \\end{align} \\] En consecuencia \\[ \\boxed{\\,Var(\\mathbf{A}\\mathbf{z}) = \\mathbf{A}\\,Var(\\mathbf{z})\\,\\mathbf{A}&#39;\\,} \\] Nota: si la transformación es afín \\(\\mathbf{y}=\\mathbf{A}\\mathbf{z}+\\mathbf{b}\\) con \\(\\mathbf{b}\\) constante, el término \\(\\mathbf{b}\\) no afecta a la varianza y el resultado se mantiene. Por lo otro, recordemos que el estimador de mínimos cuadrados se define como: \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{y} \\] y que el modelo lineal se expresa como: \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad E(\\boldsymbol{\\varepsilon}) = 0, \\quad Var(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}. \\] Sustituyendo \\(\\mathbf{y}\\) en la expresión de \\(\\hat{\\boldsymbol{\\beta}}\\): \\[ \\begin{align} \\hat{\\boldsymbol{\\beta}} &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) \\\\[6pt] &amp;= \\boldsymbol{\\beta} + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol{\\varepsilon} \\end{align} \\] Ahora calculamos la varianza de \\(\\hat{\\boldsymbol{\\beta}}\\): \\[ \\begin{align} Var(\\hat{\\boldsymbol{\\beta}}) &amp;= Var\\!\\left[\\boldsymbol{\\beta} + (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol{\\varepsilon}\\right] \\\\[6pt] &amp;= Var\\!\\left[(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\boldsymbol{\\varepsilon}\\right] \\end{align} \\] Nota: El término \\(\\boldsymbol{\\beta}\\) es constante, por lo tanto no contribuye a la varianza. Aplicando la propiedad de la varianza de una transformación lineal \\(Var(\\mathbf{A}\\mathbf{z}) = \\mathbf{A} \\, Var(\\mathbf{z}) \\, \\mathbf{A}&#39;\\): \\[ \\begin{align} Var(\\hat{\\boldsymbol{\\beta}}) &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39; \\, Var(\\boldsymbol{\\varepsilon}) \\, \\mathbf{X} (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\end{align} \\] Como por hipótesis del modelo \\(Var(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}\\), sustituimos: \\[ \\begin{align} Var(\\hat{\\boldsymbol{\\beta}}) &amp;= (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;(\\sigma^2 \\mathbf{I})\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\[6pt] &amp;= \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\[6pt] &amp;= \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\end{align} \\] Por lo tanto, la varianza del estimador de mínimos cuadrados es: \\[ \\boxed{Var(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1}} \\] Referencias Montgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2012. Introducción Al análisis de Regresión Lineal. 5ta ed. Hoboken, New Jersey: Wiley. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
